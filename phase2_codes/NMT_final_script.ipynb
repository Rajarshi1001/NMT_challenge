{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcqEg7sbrYG6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Transformer for machine translation\n",
        "### The __torchtext.data__ may through an error stating `no module found named \"Field\"` which probably arises due to deprecation of this module in the newer version of torch. Execute the cell below to install the `torchtext version 0.6.0` to run the notebook. This is because the _Field_ and _TabularDataset_ makes the vocabulary and dataloader creation much simpler.\n",
        "```python\n",
        "pip install torchtext==0.6.0\n",
        "print(torchtext.__version__)\n",
        "```\n",
        "You also may need to restart the runtime"
      ],
      "metadata": {
        "id": "_yfb-ygVtIN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.6.0\n",
        "# print(torchtext.__version__)"
      ],
      "metadata": {
        "id": "evQERu-gFOaq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import vocab\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "from torch import Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerDecoder,TransformerEncoderLayer, TransformerDecoderLayer\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import csv, json\n",
        "import pickle\n",
        "import random\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import re, string\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from string import digits\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "BQ1nfwc6tHuT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace the name of th test data after -O\n",
        "!wget 'https://drive.google.com/uc?export=download&id=1q56e_kFF0XmBDa8CVzt7f-CFVrcffzSL' -O test_data.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qSp21pPYElZ",
        "outputId": "96a86324-42ee-43ee-f26b-fe2321718532"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-27 23:14:43--  https://drive.google.com/uc?export=download&id=1q56e_kFF0XmBDa8CVzt7f-CFVrcffzSL\n",
            "Resolving drive.google.com (drive.google.com)... 172.253.117.102, 172.253.117.100, 172.253.117.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.253.117.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-8c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qqsppf6k5oksfa70gq536f6m6jte2940/1698448425000/01575642757053754597/*/1q56e_kFF0XmBDa8CVzt7f-CFVrcffzSL?e=download&uuid=93998401-f800-4011-8c8e-69da82b190a9 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-10-27 23:14:47--  https://doc-10-8c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qqsppf6k5oksfa70gq536f6m6jte2940/1698448425000/01575642757053754597/*/1q56e_kFF0XmBDa8CVzt7f-CFVrcffzSL?e=download&uuid=93998401-f800-4011-8c8e-69da82b190a9\n",
            "Resolving doc-10-8c-docs.googleusercontent.com (doc-10-8c-docs.googleusercontent.com)... 74.125.135.132, 2607:f8b0:400e:c01::84\n",
            "Connecting to doc-10-8c-docs.googleusercontent.com (doc-10-8c-docs.googleusercontent.com)|74.125.135.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36171289 (34M) [application/json]\n",
            "Saving to: ‘test_data.json’\n",
            "\n",
            "test_data.json      100%[===================>]  34.50M   122MB/s    in 0.3s    \n",
            "\n",
            "2023-10-27 23:14:47 (122 MB/s) - ‘test_data.json’ saved [36171289/36171289]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_FILE = \"test_data.json\"\n",
        "\n",
        "val_ids = []\n",
        "\n",
        "def fetchtestData():\n",
        "\n",
        "    DATA_DIR = \"testData\"\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.makedirs(DATA_DIR)\n",
        "\n",
        "    with open(TEST_FILE, \"r\") as file:\n",
        "        val_data = json.load(file)\n",
        "        for lang_type, lang_data in val_data.items():\n",
        "            print(lang_type)\n",
        "            for data_type, data_entries in lang_data.items():\n",
        "                ids, en_sns = [], []\n",
        "                for ent_id, ent_data in data_entries.items():\n",
        "                    val_ids.append(ent_id)\n",
        "                    ids.append(ent_id)\n",
        "                    en_sns.append(ent_data[\"source\"])\n",
        "\n",
        "                valdf = pd.DataFrame({\"id\" : ids, \"english\" : en_sns})\n",
        "                valdf.to_csv(os.path.join(DATA_DIR,\"test{}.csv\".format(lang_type)))\n",
        "    print(len(val_ids))\n",
        "\n",
        "def merge():\n",
        "    DATA_DIR = \"models/Translations\"\n",
        "    RESULTS = \"answer.csv\"\n",
        "\n",
        "    if os.path.exists(os.path.join(DATA_DIR, RESULTS)):\n",
        "        os.remove(os.path.join(DATA_DIR,RESULTS))\n",
        "    val_ids = []\n",
        "\n",
        "    with open(VALIDATION_FILE, \"r\") as file:\n",
        "        val_data = json.load(file)\n",
        "        for lang_type, lang_data in val_data.items():\n",
        "            print(lang_type)\n",
        "            for data_type, data_entries in lang_data.items():\n",
        "                ids, en_sns = [], []\n",
        "                for ent_id, ent_data in data_entries.items():\n",
        "                    val_ids.append(int(ent_id))\n",
        "\n",
        "    print(len(val_ids))\n",
        "    # print(val_ids)\n",
        "\n",
        "    # Reading all the dataframes\n",
        "    files = os.listdir(DATA_DIR)\n",
        "    files = [file for file in files if file.endswith(\".csv\")]\n",
        "    print(files)\n",
        "    data = [pd.read_csv(os.path.join(DATA_DIR, file)) for file in files]\n",
        "    res = pd.concat(data, axis=0, ignore_index=False)\n",
        "    res.drop([\"Unnamed: 0\", \"english\"], axis=1, inplace=True)\n",
        "    res = res.rename(columns={\"id\": \"ID\", \"translated\": \"Translated\"})\n",
        "    res_sorted = res[res[\"ID\"].isin(val_ids)].sort_values(by=\"ID\")\n",
        "    res_sorted.to_csv(os.path.join(DATA_DIR, RESULTS), sep=\"\\t\", index=False, quoting=csv.QUOTE_NONNUMERIC, quotechar='\"', escapechar='\\\\')\n",
        "\n",
        "fetchtestData()"
      ],
      "metadata": {
        "id": "4kKetmDTtHxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27c5263-5d89-46b4-be8d-43b128dff179"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English-Bengali\n",
            "English-Gujarati\n",
            "English-Hindi\n",
            "English-Kannada\n",
            "English-Malayalam\n",
            "English-Tamil\n",
            "English-Telgu\n",
            "114643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "# Direct download URLs\n",
        "hin = 'https://drive.google.com/uc?id=1qw1XxU5MG43gSBJV99NOGvEmJcowcHs5'\n",
        "tam = 'https://drive.google.com/uc?id=1s0c2kW44HthSTTU0HPtQz4u0fU8L6aJV'\n",
        "ben = 'https://drive.google.com/uc?id=1dH1USvoxlKaBQZxIW_GKgcpk4FJeaC73'\n",
        "kan = 'https://drive.google.com/uc?id=1gZHg91Z-eInb506MGWVSmECCpnteyoKI'\n",
        "guj = 'https://drive.google.com/uc?id=1EDTa5hA0bCx7uu6hURA9lTU1hW7ErFx3'\n",
        "tel = 'https://drive.google.com/uc?export=download&id=1FaBjIiP-OhVmqwyUJaDz3e51FXV_rsYI'\n",
        "mal = 'https://drive.google.com/uc?export=download&id=1-ecxLBY7F8ONPDD-ubUFSvFl3PZcPQCK'\n",
        "\n",
        "names = ['hindi', 'tamil', 'bengali', 'kannada', 'gujarati', 'telgu', 'malayalam']\n",
        "nbs = [hin, tam, ben, kan, guj, tel, mal]\n",
        "\n",
        "for index, nm in enumerate(names):\n",
        "    file_url = nbs[index]\n",
        "    output_file = f\"{nm}.pth\"\n",
        "    # Use gdown to download the file\n",
        "    gdown.download(file_url, output_file, quiet=False)\n",
        "\n",
        "# Verify the download\n",
        "print(\"Download complete. Files in the current directory:\")\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBeh7mTRaLSI",
        "outputId": "e675e262-faa4-4b65-8d72-eb5204483a50"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qw1XxU5MG43gSBJV99NOGvEmJcowcHs5\n",
            "To: /content/hindi.pth\n",
            "100%|██████████| 351M/351M [00:05<00:00, 67.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1s0c2kW44HthSTTU0HPtQz4u0fU8L6aJV\n",
            "To: /content/tamil.pth\n",
            "100%|██████████| 320M/320M [00:02<00:00, 121MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dH1USvoxlKaBQZxIW_GKgcpk4FJeaC73\n",
            "To: /content/bengali.pth\n",
            "100%|██████████| 346M/346M [00:02<00:00, 162MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gZHg91Z-eInb506MGWVSmECCpnteyoKI\n",
            "To: /content/kannada.pth\n",
            "100%|██████████| 288M/288M [00:02<00:00, 109MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EDTa5hA0bCx7uu6hURA9lTU1hW7ErFx3\n",
            "To: /content/gujarati.pth\n",
            "100%|██████████| 304M/304M [00:01<00:00, 206MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1FaBjIiP-OhVmqwyUJaDz3e51FXV_rsYI\n",
            "To: /content/telgu.pth\n",
            "100%|██████████| 283M/283M [00:01<00:00, 182MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1-ecxLBY7F8ONPDD-ubUFSvFl3PZcPQCK\n",
            "To: /content/malayalam.pth\n",
            "100%|██████████| 307M/307M [00:20<00:00, 14.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete. Files in the current directory:\n",
            "['.config', 'bengali.csv', 'malayalam.pth', 'gujarati.pth', 'kannada.pkl', 'telgu.pkl', 'hindi.csv', 'eng_hindi.pkl', 'eng_telgu.pkl', 'malayalam.csv', 'hindi.pth', 'kannada.pth', 'kannada.csv', 'tamil.csv', 'gujarati.pkl', 'eng_tamil.pkl', 'eng_gujarati.pkl', 'bengali.pth', 'hindi.pkl', 'telgu.csv', 'gujarati.csv', 'eng_malayalam.pkl', 'eng_bengali.pkl', 'tamil.pth', 'tamil.pkl', 'transformer_translations', 'malayalam.pkl', 'bengali.pkl', 'test_data.json', 'eng_kannada.pkl', 'testData', 'telgu.pth', 'sample_data']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "# Direct download URLs\n",
        "hin_vocab = \"https://drive.google.com/uc?id=1Y0ecIkCqRQT1uAwf71K7nDsV6GDPohU3\"\n",
        "tam_vocab = \"https://drive.google.com/uc?id=145469uB0EYxNP5EYyWdcXFt1Dp4HYa9e\"\n",
        "ben_vocab = \"https://drive.google.com/uc?id=1k1WJRP2Oc-YqQCNokBHD-TcSY1dE6HHk\"\n",
        "kan_vocab = \"https://drive.google.com/uc?id=1XOFK04ESZsfxDUjDr2CdukLzSISkrm6l\"\n",
        "guj_vocab = \"https://drive.google.com/uc?id=1nOs3VlMenaLS4OopzjZrq8QbdGsVQ1l0\"\n",
        "tel_vocab = \"https://drive.google.com/uc?export=download&id=16VudLbpPiaMMCq0rc0jMIMgwKyJ_LitH\"\n",
        "mal_vocab = \"https://drive.google.com/uc?export=download&id=1o8JoV1YWpHIB24o-qEnOK4l9CmSQMIBS\"\n",
        "\n",
        "vocab_names = ['hindi', 'tamil', 'bengali', 'kannada', 'gujarati', 'telgu', 'malayalam']\n",
        "nbs_vocabs = [hin_vocab, tam_vocab, ben_vocab, kan_vocab, guj_vocab, tel_vocab, mal_vocab]\n",
        "\n",
        "for index, nm in enumerate(vocab_names):\n",
        "    file_url = nbs_vocabs[index]\n",
        "    output_file = f\"{nm}.pkl\"\n",
        "    # Use gdown to download the file\n",
        "    gdown.download(file_url, output_file, quiet=False)\n",
        "\n",
        "# Verify the download\n",
        "print(\"Download complete. Files in the current directory:\")\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6YN5ddz11au",
        "outputId": "7fa33909-7004-4214-a180-2b3eb68434c3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Y0ecIkCqRQT1uAwf71K7nDsV6GDPohU3\n",
            "To: /content/hindi.pkl\n",
            "100%|██████████| 2.70M/2.70M [00:00<00:00, 49.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=145469uB0EYxNP5EYyWdcXFt1Dp4HYa9e\n",
            "To: /content/tamil.pkl\n",
            "100%|██████████| 5.15M/5.15M [00:00<00:00, 166MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1k1WJRP2Oc-YqQCNokBHD-TcSY1dE6HHk\n",
            "To: /content/bengali.pkl\n",
            "100%|██████████| 3.49M/3.49M [00:00<00:00, 244MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XOFK04ESZsfxDUjDr2CdukLzSISkrm6l\n",
            "To: /content/kannada.pkl\n",
            "100%|██████████| 3.70M/3.70M [00:00<00:00, 275MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nOs3VlMenaLS4OopzjZrq8QbdGsVQ1l0\n",
            "To: /content/gujarati.pkl\n",
            "100%|██████████| 2.78M/2.78M [00:00<00:00, 166MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=16VudLbpPiaMMCq0rc0jMIMgwKyJ_LitH\n",
            "To: /content/telgu.pkl\n",
            "100%|██████████| 3.28M/3.28M [00:00<00:00, 151MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1o8JoV1YWpHIB24o-qEnOK4l9CmSQMIBS\n",
            "To: /content/malayalam.pkl\n",
            "100%|██████████| 5.51M/5.51M [00:00<00:00, 147MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete. Files in the current directory:\n",
            "['.config', 'bengali.csv', 'malayalam.pth', 'gujarati.pth', 'kannada.pkl', 'telgu.pkl', 'hindi.csv', 'eng_hindi.pkl', 'eng_telgu.pkl', 'malayalam.csv', 'hindi.pth', 'kannada.pth', 'kannada.csv', 'tamil.csv', 'gujarati.pkl', 'eng_tamil.pkl', 'eng_gujarati.pkl', 'bengali.pth', 'hindi.pkl', 'telgu.csv', 'gujarati.csv', 'eng_malayalam.pkl', 'eng_bengali.pkl', 'tamil.pth', 'tamil.pkl', 'transformer_translations', 'malayalam.pkl', 'bengali.pkl', 'test_data.json', 'eng_kannada.pkl', 'testData', 'telgu.pth', 'sample_data']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "# Adjusted URLs for direct downloading\n",
        "eng_hindi_vocab = \"https://drive.google.com/uc?id=17PNs-jgasaCeyhRue4VTskrZ7VuLIfBs\"\n",
        "eng_tamil_vocab = \"https://drive.google.com/uc?id=1voaWLFCn1uS1z1kv8tKVkM0Aiwzq29xY\"\n",
        "eng_bengali_vocab = \"https://drive.google.com/uc?id=1wqmRla1QQvflRKdp56QOmq8WONMi8y6Y\"\n",
        "eng_kannada_vocab = \"https://drive.google.com/uc?export=download&id=1J9HMMxOxevJCe9kifXIEgUJZCIGcLDpD\"\n",
        "eng_gujarati_vocab = \"https://drive.google.com/uc?id=1xXIbsXQcUGB9ypXdf8hODDyDWv5f2xo8\"\n",
        "eng_telgu_vocab = \"https://drive.google.com/uc?export=download&id=1rgWKnMnYcOwDzu83H7uHeywvs0Y006-P\"\n",
        "eng_malayalam_vocab = \"https://drive.google.com/uc?export=download&id=1WIApAQGsbS1wO20DsAPxSEZhXgumyr7d\"\n",
        "\n",
        "vocab_names = ['hindi', 'tamil', 'bengali', 'kannada', 'gujarati', 'telgu', 'malayalam']\n",
        "nbs_vocabs = [eng_hindi_vocab, eng_tamil_vocab, eng_bengali_vocab, eng_kannada_vocab, eng_gujarati_vocab, eng_telgu_vocab, eng_malayalam_vocab]\n",
        "\n",
        "for index, nm in enumerate(vocab_names):\n",
        "    file_url = nbs_vocabs[index]\n",
        "    output_file = \"eng_{}.pkl\".format(nm)\n",
        "    # Use gdown to download the file\n",
        "    gdown.download(file_url, output_file, quiet=False)\n",
        "\n",
        "# Verify the download\n",
        "print(\"Download complete. Files in the current directory:\")\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_wj139x9PN_",
        "outputId": "f7f9fe15-f086-45fd-f08b-02ded3bafa6f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17PNs-jgasaCeyhRue4VTskrZ7VuLIfBs\n",
            "To: /content/eng_hindi.pkl\n",
            "100%|██████████| 1.30M/1.30M [00:00<00:00, 136MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1voaWLFCn1uS1z1kv8tKVkM0Aiwzq29xY\n",
            "To: /content/eng_tamil.pkl\n",
            "100%|██████████| 1.07M/1.07M [00:00<00:00, 114MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wqmRla1QQvflRKdp56QOmq8WONMi8y6Y\n",
            "To: /content/eng_bengali.pkl\n",
            "100%|██████████| 1.23M/1.23M [00:00<00:00, 134MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1J9HMMxOxevJCe9kifXIEgUJZCIGcLDpD\n",
            "To: /content/eng_kannada.pkl\n",
            "100%|██████████| 870k/870k [00:00<00:00, 145MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xXIbsXQcUGB9ypXdf8hODDyDWv5f2xo8\n",
            "To: /content/eng_gujarati.pkl\n",
            "100%|██████████| 975k/975k [00:00<00:00, 113MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1rgWKnMnYcOwDzu83H7uHeywvs0Y006-P\n",
            "To: /content/eng_telgu.pkl\n",
            "100%|██████████| 841k/841k [00:00<00:00, 111MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1WIApAQGsbS1wO20DsAPxSEZhXgumyr7d\n",
            "To: /content/eng_malayalam.pkl\n",
            "100%|██████████| 1.01M/1.01M [00:00<00:00, 74.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete. Files in the current directory:\n",
            "['.config', 'bengali.csv', 'malayalam.pth', 'gujarati.pth', 'kannada.pkl', 'telgu.pkl', 'hindi.csv', 'eng_hindi.pkl', 'eng_telgu.pkl', 'malayalam.csv', 'hindi.pth', 'kannada.pth', 'kannada.csv', 'tamil.csv', 'gujarati.pkl', 'eng_tamil.pkl', 'eng_gujarati.pkl', 'bengali.pth', 'hindi.pkl', 'telgu.csv', 'gujarati.csv', 'eng_malayalam.pkl', 'eng_bengali.pkl', 'tamil.pth', 'tamil.pkl', 'transformer_translations', 'malayalam.pkl', 'bengali.pkl', 'test_data.json', 'eng_kannada.pkl', 'testData', 'telgu.pth', 'sample_data']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://drive.google.com/drive/folders/1PgGeWF6Rp-0Z30LBn6-70Cl-Vz_uxms5?usp=sharing -O models"
      ],
      "metadata": {
        "id": "ApUEbX1nZEgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the dataset name\n",
        "l = [\"Bengali\", \"Gujarati\", \"Hindi\", \"Malayalam\", \"Telgu\", \"Kannada\", \"Tamil\"]\n",
        "\n",
        "for e in names:\n",
        "# Read the CSV file from the specified directory into a DataFrame\n",
        "  data = pd.read_csv(\"/content/testData/testEnglish-{}.csv\".format(e.title()))\n",
        "\n",
        "  # Drop the unnecessary columns \"Unnamed: 0\" and \"entry_id\" from the DataFrame\n",
        "  data.drop([\"Unnamed: 0\", \"id\"], inplace=True, axis=1)\n",
        "\n",
        "  # Note: The next operation seems redundant as \"entry_id\" has already been dropped.\n",
        "  # Rename the column \"entry_id\" to \"id\" (if it exists)\n",
        "  data = data.rename(columns={\"id\": \"id\"})\n",
        "\n",
        "  # Display the first 10 rows of the cleaned DataFrame\n",
        "  # (This will be visible in interactive environments like Jupyter Notebook)\n",
        "  data.head(10)\n",
        "\n",
        "  # Write the cleaned data back to a new CSV file in the current directory\n",
        "  data.to_csv(\"{}.csv\".format(e), index=False)\n"
      ],
      "metadata": {
        "id": "2xaJft7hXvQ_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Convert all the text into lower letters\n",
        "    Remove the words betweent brakets ()\n",
        "    Remove these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
        "    Replace these special characters with space:\n",
        "    Replace extra white spaces with single white spaces\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    text = re.sub('[$)\\\"’°;\\'€%:,(/]', '', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('\\u200d', ' ', text)\n",
        "    text = re.sub('\\u200c', ' ', text)\n",
        "    text = re.sub('-', ' ', text)\n",
        "    text = re.sub('  ', ' ', text)\n",
        "    text = re.sub('   ', ' ', text)\n",
        "    text =\" \".join(text.split())\n",
        "    return text"
      ],
      "metadata": {
        "id": "Y_k0jEQ-bydP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/testData/testEnglish-Malayalam.csv\")\n",
        "data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "VwkOY2MceyHl",
        "outputId": "1776e94c-bfcd-40bd-d304-acf71dee08af"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0      id                                            english\n",
              "0           0  820483  ഹിന്ദുസ്ഥാനിസംഗീതത്തിൻ്റെ ചില വശങ്ങൾ ക്രമപ്പെട...\n",
              "1           1  820484  ജാവനീസ് ഭാഷയിൽ വെൻ്റിസ് എന്ന് പറഞ്ഞാൽ കാൽവണ്ണയ...\n",
              "2           2  820485  നിങ്ങൾ നൽകുന്ന വിവരങ്ങളുടെ കൃത്യത കൂടുന്നതിനനു...\n",
              "3           3  820486                   രാവിലെ ആറ് മണിക്ക് അലാറം വെക്കുക\n",
              "4           4  820487              എന്റെ ഭക്ഷണം എപ്പോഴാണ് ഇവിടെ ഉണ്ടാവുക\n",
              "5           5  820488  ഇത് കൂടാതെ രോഗിയുടെ വയറില്‍ വെള്ളം കിടക്കുന്നത...\n",
              "6           6  820489  റിഗാറ്റസ്' എന്ന് വിളിക്കപ്പെടുന്ന റോയിംഗ് മത്സ...\n",
              "7           7  820490  മറിയം-ഉസ്-സമാനിയുടെയും മറ്റ് ഹിന്ദു ഭാര്യമാരുട...\n",
              "8           8  820491  സമീപ ദശകങ്ങളിൽ, മറാത്തി തമാഷയും ചില പരീക്ഷണാത്...\n",
              "9           9  820492  രാജ്യത്തിലെ 63 % പാല്‍ ഉത്പന്നങ്ങളും ഗുജറാത്തി..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-803a17bf-cd66-4f20-b2b9-6fb7e70b59e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>820483</td>\n",
              "      <td>ഹിന്ദുസ്ഥാനിസംഗീതത്തിൻ്റെ ചില വശങ്ങൾ ക്രമപ്പെട...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>820484</td>\n",
              "      <td>ജാവനീസ് ഭാഷയിൽ വെൻ്റിസ് എന്ന് പറഞ്ഞാൽ കാൽവണ്ണയ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>820485</td>\n",
              "      <td>നിങ്ങൾ നൽകുന്ന വിവരങ്ങളുടെ കൃത്യത കൂടുന്നതിനനു...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>820486</td>\n",
              "      <td>രാവിലെ ആറ് മണിക്ക് അലാറം വെക്കുക</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>820487</td>\n",
              "      <td>എന്റെ ഭക്ഷണം എപ്പോഴാണ് ഇവിടെ ഉണ്ടാവുക</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>820488</td>\n",
              "      <td>ഇത് കൂടാതെ രോഗിയുടെ വയറില്‍ വെള്ളം കിടക്കുന്നത...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>820489</td>\n",
              "      <td>റിഗാറ്റസ്' എന്ന് വിളിക്കപ്പെടുന്ന റോയിംഗ് മത്സ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>820490</td>\n",
              "      <td>മറിയം-ഉസ്-സമാനിയുടെയും മറ്റ് ഹിന്ദു ഭാര്യമാരുട...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>820491</td>\n",
              "      <td>സമീപ ദശകങ്ങളിൽ, മറാത്തി തമാഷയും ചില പരീക്ഷണാത്...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>820492</td>\n",
              "      <td>രാജ്യത്തിലെ 63 % പാല്‍ ഉത്പന്നങ്ങളും ഗുജറാത്തി...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-803a17bf-cd66-4f20-b2b9-6fb7e70b59e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-803a17bf-cd66-4f20-b2b9-6fb7e70b59e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-803a17bf-cd66-4f20-b2b9-6fb7e70b59e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc792e9a-8e44-42ab-a024-7cfc4107144f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc792e9a-8e44-42ab-a024-7cfc4107144f')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc792e9a-8e44-42ab-a024-7cfc4107144f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # train\n",
        "device = \"cpu\" # inference"
      ],
      "metadata": {
        "id": "KQkx0eR-Xyyq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the params of all the models"
      ],
      "metadata": {
        "id": "659oI6V5AZQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerMT(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Transformer model for Machine Translation (MT).\n",
        "\n",
        "    This model consists of an encoder and a decoder, both built using the Transformer architecture.\n",
        "\n",
        "    Parameters:\n",
        "    - nhead (int): Number of heads in the multihead attention mechanism.\n",
        "    - embed_size (int): Dimension of the embedding vector.\n",
        "    - source_vocab_size (int): Size of the source vocabulary.\n",
        "    - target_vocab_size (int): Size of the target vocabulary.\n",
        "    - num_encoder_layers (int): Number of layers in the transformer encoder.\n",
        "    - num_decoder_layers (int): Number of layers in the transformer decoder.\n",
        "    - ffnn_size (int, optional): Size of the feedforward neural network inside transformer layers. Default is 512.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 nhead: int,\n",
        "                 embed_size: int,\n",
        "                 source_vocab_size: int,\n",
        "                 target_vocab_size: int,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 ffnn_size:int = 512):\n",
        "\n",
        "        super(TransformerMT, self).__init__()\n",
        "\n",
        "        # Define encoder and decoder layers\n",
        "        encoder_layer = TransformerEncoderLayer(d_model = embed_size, nhead = nhead, dim_feedforward = ffnn_size)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model = embed_size, nhead = nhead, dim_feedforward = ffnn_size)\n",
        "\n",
        "        # Initialize transformer encoder, decoder, and final fully connected layer\n",
        "        self.tf_encoder = TransformerEncoder(encoder_layer, num_layers = num_encoder_layers)\n",
        "        self.tf_decoder = TransformerDecoder(decoder_layer, num_layers = num_decoder_layers)\n",
        "        self.fc_layer = nn.Linear(embed_size, target_vocab_size)\n",
        "\n",
        "        # Embedding layers for tokens and positional information\n",
        "        self.src_token_embedding = TokenEmbedding(source_vocab_size, embed_size)\n",
        "        self.tar_token_embedding = TokenEmbedding(target_vocab_size, embed_size)\n",
        "        self.positional_embedding = SinusoidalEmbedding(embed_size, dropout = DROPOUT_RATE)\n",
        "\n",
        "    def forward(self,\n",
        "                source: Tensor,\n",
        "                target: Tensor,\n",
        "                source_mask: Tensor,\n",
        "                target_mask: Tensor,\n",
        "                source_padding_mask: Tensor,\n",
        "                target_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the TransformerMT model.\n",
        "\n",
        "        Parameters:\n",
        "        - source (torch.Tensor): Source sequence tensor.\n",
        "        - target (torch.Tensor): Target sequence tensor.\n",
        "        - source_mask (torch.Tensor): Source sequence mask.\n",
        "        - target_mask (torch.Tensor): Target sequence mask.\n",
        "        - source_padding_mask (torch.Tensor): Source padding mask.\n",
        "        - target_padding_mask (torch.Tensor): Target padding mask.\n",
        "        - memory_key_padding_mask (torch.Tensor): Memory key padding mask.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor after passing through the transformer and the fully connected layer.\n",
        "        \"\"\"\n",
        "\n",
        "        source_embedding = self.positional_embedding(self.src_token_embedding(source))\n",
        "        target_embedding = self.positional_embedding(self.tar_token_embedding(target))\n",
        "        memory = self.tf_encoder(source_embedding, source_mask, source_padding_mask)\n",
        "        outputs = self.tf_decoder(target_embedding,\n",
        "                                  memory,\n",
        "                                  target_mask,\n",
        "                                  None,\n",
        "                                  target_padding_mask,\n",
        "                                  memory_key_padding_mask)\n",
        "        outputs = self.fc_layer(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def encode(self, source: Tensor, source_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Encode the source sequence using the transformer encoder.\n",
        "\n",
        "        Parameters:\n",
        "        - source (torch.Tensor): Source sequence tensor.\n",
        "        - source_mask (torch.Tensor): Source sequence mask.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Encoded memory tensor.\n",
        "        \"\"\"\n",
        "        token_rep = self.src_token_embedding(source)\n",
        "        positional_rep = self.positional_embedding(token_rep)\n",
        "        encoder_output = self.tf_encoder(positional_rep, source_mask)\n",
        "        return encoder_output\n",
        "\n",
        "    def decode(self, target: Tensor, memory: Tensor, target_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Decode the memory tensor using the transformer decoder.\n",
        "\n",
        "        Parameters:\n",
        "        - target (torch.Tensor): Target sequence tensor.\n",
        "        - memory (torch.Tensor): Encoded memory tensor.\n",
        "        - target_mask (torch.Tensor): Target sequence mask.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Decoded tensor.\n",
        "        \"\"\"\n",
        "        token_rep = self.src_token_embedding(target)\n",
        "        positional_rep = self.positional_embedding(token_rep)\n",
        "        decoder_output = self.tf_decoder(positional_rep, memory, target_mask)\n",
        "        return decoder_output\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "x76uFRUTtHzU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Sinusoidal Positional Encoding for Transformer models.\n",
        "\n",
        "    The positional encoding module uses sine and cosine functions of different frequencies to\n",
        "    encode the position of tokens in the sequence.\n",
        "\n",
        "    Parameters:\n",
        "    - embed_size (int): Dimension of the embedding vector.\n",
        "    - dropout (float): Dropout rate for the dropout layer.\n",
        "    - max_len (int, optional): Maximum length of the sequence. Default is 5000.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size, dropout, max_len = 5000):\n",
        "        super(SinusoidalEmbedding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "        # Compute the sinusoidal positional encodings\n",
        "        denom = max_len*2\n",
        "        pdist = torch.exp(- torch.arange(0, embed_size, 2) * math.log(denom) / embed_size)\n",
        "        position = torch.arange(0, max_len).reshape(max_len, 1)\n",
        "        position_embedding = torch.zeros((max_len, embed_size))\n",
        "        position_embedding[:, 0::2] = torch.sin(position * pdist)\n",
        "        position_embedding[:, 1::2] = torch.cos(position * pdist)\n",
        "        position_embedding = position_embedding.unsqueeze(-2)\n",
        "\n",
        "        # Register the position embeddings so they get saved with the model's state_dict\n",
        "        self.register_buffer('position_embedding', position_embedding)\n",
        "\n",
        "    def forward(self, token_embed):\n",
        "        \"\"\"\n",
        "        Forward pass of the SinusoidalEmbedding.\n",
        "\n",
        "        Parameters:\n",
        "        - token_embed (torch.Tensor): Token embeddings.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Token embeddings added with positional encodings.\n",
        "        \"\"\"\n",
        "        outputs = token_embed + self.position_embedding[:token_embed.size(0),:]\n",
        "        outputs = self.dropout(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Token Embedding module for Transformer models.\n",
        "\n",
        "    This module converts token indices into dense vectors of fixed size, embed_size.\n",
        "    The embeddings are scaled by the square root of their dimensionality.\n",
        "\n",
        "    Parameters:\n",
        "    - vocab_size (int): Size of the vocabulary.\n",
        "    - embed_size (int): Dimension of the embedding vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"\n",
        "        Forward pass of the TokenEmbedding.\n",
        "\n",
        "        Parameters:\n",
        "        - tokens (torch.Tensor): Tensor of token indices.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Scaled token embeddings.\n",
        "        \"\"\"\n",
        "        outputs = self.embedding(tokens.long())\n",
        "        outputs_scaled = outputs * math.sqrt(self.embed_size)\n",
        "        return outputs_scaled"
      ],
      "metadata": {
        "id": "pbkjA3fTtH1G"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\"\n",
        "    Generate a square mask for the sequence, where the mask indicates subsequent positions.\n",
        "    This mask is used to ensure that a position cannot attend to subsequent positions in the sequence.\n",
        "\n",
        "    Parameters:\n",
        "    - sz (int): Size of the sequence.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Mask tensor of shape (sz, sz) with 0s in positions that can be attended to and negative infinity elsewhere.\n",
        "    \"\"\"\n",
        "\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(source, target):\n",
        "    \"\"\"\n",
        "    Create masks for the source and target sequences.\n",
        "\n",
        "    Parameters:\n",
        "    - source (torch.Tensor): Source sequence tensor.\n",
        "    - target (torch.Tensor): Target sequence tensor.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing source mask, target mask, source padding mask, and target padding mask.\n",
        "    \"\"\"\n",
        "\n",
        "    source_seq_len = source.shape[0]\n",
        "    target_seq_len = target.shape[0]\n",
        "    batch_size = source.shape[1]\n",
        "    source_mask = torch.zeros((source_seq_len, source_seq_len), device=device).type(torch.bool)\n",
        "    target_mask = generate_square_subsequent_mask(target_seq_len)\n",
        "    source_padding_mask = (source == PAD_IDX).transpose(0, 1)\n",
        "    target_padding_mask = (target == PAD_IDX).transpose(0, 1)\n",
        "    return source_mask, target_mask, source_padding_mask, target_padding_mask\n"
      ],
      "metadata": {
        "id": "gq-CZnubtH2r"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOS_IDX = eng.vocab.stoi[\"<sos>\"]\n",
        "# EOS_IDX = eng.vocab.stoi[\"<eos>\"]\n",
        "\n",
        "def get_tokens(model, source, source_mask, max_len, start_symbol, SOS_IDX):\n",
        "    \"\"\"\n",
        "    Get token indices from a given source sequence using a trained model.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The trained transformer model.\n",
        "    - source (torch.Tensor): The source sequence tensor.\n",
        "    - source_mask (torch.Tensor): The mask for the source sequence.\n",
        "    - max_len (int): Maximum length of the target sequence.\n",
        "    - start_symbol (int): The starting symbol for the target sequence.\n",
        "\n",
        "    Returns:\n",
        "    - result (torch.Tensor): The tensor containing token indices of the target sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Move source and its mask to device\n",
        "    source_mask = source_mask\n",
        "    source = source\n",
        "\n",
        "    # Encode the source sequence\n",
        "    memory = model.encode(source, source_mask)\n",
        "\n",
        "    # Initialize result tensor with the start symbol\n",
        "    result = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "    prev_tokens = []\n",
        "    # Decode the memory tensor to get the target sequence\n",
        "    for index in range(max_len - 1):\n",
        "        memory = memory.to(device)\n",
        "        memory_mask = torch.zeros(result.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "        target_mask = (generate_square_subsequent_mask(result.size(0)).type(torch.bool)).to(device)\n",
        "        output = model.decode(result, memory, target_mask)\n",
        "        output = output.transpose(0, 1)\n",
        "\n",
        "        # Get the next word's probability distribution and find the word with the maximum probability\n",
        "        logits = model.fc_layer(output[:, -1])\n",
        "        # Apply repetition penalty\n",
        "        for tok in prev_tokens:\n",
        "            logits[0][tok] /= 3  # Decrease the probability of tokens that have been selected before\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        _, next_word = torch.max(probs, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        # Add the next word to the result\n",
        "        result = torch.cat([result,torch.ones(1, 1).type_as(source.data).fill_(next_word)], dim=0)\n",
        "\n",
        "        # Break if the next word is the start of sequence symbol\n",
        "        if next_word == SOS_IDX:\n",
        "            break\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def translate(model, source, source_vocab, target_vocab, SOS_IDX, EOS_IDX):\n",
        "    \"\"\"\n",
        "    Translate a given source sequence to a target sequence using a trained model.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The trained transformer model.\n",
        "    - source (str or torch.Tensor): The source sequence (string or tensor).\n",
        "    - source_vocab (Vocab): Vocabulary object for the source language.\n",
        "    - target_vocab (Vocab): Vocabulary object for the target language.\n",
        "\n",
        "    Returns:\n",
        "    - str: The translated sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Convert source string to tensor, if it's a string\n",
        "    if type(source) == str:\n",
        "        tokens = [SOS_IDX] + [source_vocab.stoi[tok] for tok in source.split()] + [EOS_IDX]\n",
        "        num_tokens = len(tokens)\n",
        "        source = (torch.LongTensor(tokens).reshape(num_tokens, 1))\n",
        "        source_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "\n",
        "        # Get target token indices from the source tensor\n",
        "        target_tokens = get_tokens(model, source, source_mask, max_len = num_tokens + 5, start_symbol = SOS_IDX, SOS_IDX = SOS_IDX).flatten()\n",
        "\n",
        "        # Convert target token indices to string\n",
        "        return \" \".join([target_vocab.itos[token] for token in target_tokens]).replace(\"<sos>\", \"\").replace(\"<eos>\", \"\").replace(\"<unk>\", \"\")\n",
        "    else:\n",
        "        tokens = source\n",
        ""
      ],
      "metadata": {
        "id": "w6LC0hi_jQEd"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SRC_VOCAB_SIZE = len(eng.vocab)  # Source vocabulary size (English)\n",
        "# TAR_VOCAB_SIZE = len(lang.vocab)  # Target vocabulary size (Other language, inferred from 'lang' variable)\n",
        "EMBEDDING_SIZE = 512  # Size of the embedding vector\n",
        "NHEAD = 8  # Number of heads in the multihead attention mechanism\n",
        "FFNN_DIM = 512  # Dimension of the feed-forward neural network inside transformer layers\n",
        "BATCH_SIZE = 32  # Size of each training batch\n",
        "NUM_ENCODER_LAYERS = 2  # Number of layers in the transformer encoder\n",
        "NUM_DECODER_LAYERS = 2  # Number of layers in the transformer decoder\n",
        "LEARNING_RATE = 0.0001  # Learning rate for the optimizer\n",
        "DROPOUT_RATE = 0.1  # Dropout rate for the dropout layer\n",
        "NUM_EPOCHS = 2  # Number of epochs for training\n",
        "# PAD_IDX = eng.vocab.stoi[\"<pad>\"]  # Index for the padding token\n",
        "# SOS_IDX = eng.vocab.stoi[\"<sos>\"] # index for start token\n",
        "# EOD_IDX = eng.vocab.stoi[\"<eos>\"] # index for end token\n",
        "\n",
        "for idx, e in enumerate(names):\n",
        "  with open(\"/content/{}.pkl\".format(e), \"rb\") as vocab_file:\n",
        "    lang = pickle.load(vocab_file)\n",
        "    print(len(lang))\n",
        "  with open(\"/content/eng_{}.pkl\".format(e), \"rb\") as eng_vocab_file:\n",
        "    eng = pickle.load(eng_vocab_file)\n",
        "    print(len(eng))\n",
        "  SOS_IDX = 2\n",
        "  EOS_IDX = 3\n",
        "  print(e)\n",
        "\n",
        "  # Instantiate the TransformerMT model with the specified configuration\n",
        "  # if e.lower() == \"hindi\":\n",
        "    # break\n",
        "    # model = TransformerMT(NHEAD,\n",
        "    #                     EMBEDDING_SIZE,\n",
        "    #                     len(lang),\n",
        "    #                     len(lang),\n",
        "    #                     NUM_ENCODER_LAYERS,\n",
        "    #                     NUM_DECODER_LAYERS,\n",
        "    #                     FFNN_DIM)\n",
        "  # else:\n",
        "  model = TransformerMT(NHEAD,\n",
        "                        EMBEDDING_SIZE,\n",
        "                        len(lang),\n",
        "                        len(eng),\n",
        "                        NUM_ENCODER_LAYERS,\n",
        "                        NUM_DECODER_LAYERS,\n",
        "                        FFNN_DIM)\n",
        "\n",
        "\n",
        "\n",
        "  # Define the loss criterion\n",
        "  # CrossEntropyLoss is used since this is a classification task, and we ignore the loss computed on padding tokens\n",
        "  criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "  model.load_state_dict(torch.load('/content/{}.pth'.format(e.lower())))\n",
        "  model.to(device)\n",
        "  # Define the optimizer to be used for training\n",
        "  # Adam optimizer is used with specific betas and epsilon values\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
        "# Move the model to the specified device (either GPU or CPU)\n",
        "\n",
        "\n",
        "  # Inference\n",
        "# Check if the \"Translations\" directory exists. If not, create it.\n",
        "  if not os.path.exists(\"transformer_translations\"):\n",
        "      os.makedirs(\"transformer_translations\")\n",
        "\n",
        "  # List to store the predicted translations.\n",
        "  predictions = []\n",
        "\n",
        "  # Read the test data from the specified CSV file.\n",
        "  data = pd.read_csv(\"/content/testData/testEnglish-{}.csv\".format(e.title()))\n",
        "  data = data.iloc[:10, :]\n",
        "\n",
        "  # Loop through each row (sentence) in the test data.\n",
        "  for idx, row in data.iterrows():\n",
        "      # Extract the English sentence.\n",
        "      en = row[\"english\"]\n",
        "\n",
        "      # Translate the English sentence to the target language.\n",
        "      pred = translate(model, en, lang, eng, SOS_IDX, EOS_IDX)\n",
        "\n",
        "      # Print the translated sentence (optional, can be commented out).\n",
        "      print(pred)\n",
        "\n",
        "      # Append the translated sentence to the predictions list.\n",
        "      predictions.append(pred)\n",
        "\n",
        "  # Add the predicted translations as a new column to the original dataframe.\n",
        "  data[\"translated\"] = predictions\n",
        "\n",
        "  # Drop the unwanted column \"Unnamed: 0\" (assuming it exists in the CSV).\n",
        "  data.drop([\"Unnamed: 0\"], inplace=True, axis=1)\n",
        "\n",
        "  # Save the dataframe with translations to a new CSV file.\n",
        "  data.to_csv(os.path.join(\"transformer_translations\", \"answer_{}_test.csv\".format(e)))\n",
        "\n",
        "# Evaluate the model on the Bengali test set.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3KcexPZVX6D",
        "outputId": "57370b03-0c75-4932-e7b9-2d8738a9ddfb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50004\n",
            "50004\n",
            "hindi\n",
            " outbreak disease occur prevention effective control prevention disease one disease one . . . . . . . . . one one one one  . one      . .\n",
            " much strong becomes strong becomes . . . . . . . . . much much much much\n",
            " due television due due due made due later later later later later later could . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            " kolkata big airport century mixed bridge airport form form state . state . . . state . state . . . . . . .\n",
            " famous famous time famous time famous time famous time days . . . . one famous one famous one tourists india . . . . one tourists\n",
            " general using short high high high high high high high may patients patients patients patients patients patients patients . patients . patients . . . patients . patients       . . . cases .    \n",
            " show available center available center state state state centres state  state  state state state\n",
            " could   could written . . . . . . . . . . . . .\n",
            " also house made become 40 home art indian indian indian like indian like . indian . one made indian . one made made indian . . indian . one   .\n",
            " stupa height height height height height height . . . . . . . height . .\n",
            "50004\n",
            "42287\n",
            "tamil\n",
            " people people people people people people people one . . . . . . . . . . .\n",
            " even even till time . . . . . . . .\n",
            " farmers farmers farmers farmers farmers . .  . . . . . .\n",
            " one one first production 2 25 production 2 one . . . . . . . . .\n",
            " the also time . . . . . . . . .\n",
            " central central central central . . . . . . . .\n",
            " want want want new         \n",
            " the also south also etc medicine . etc . . . . . . . . . . . . .\n",
            " people people people people people people people people also people people people people . . . . . . . . . .\n",
            " it i `` `s `` `s `` . . . . . . . . . . . . . .\n",
            "50004\n",
            "48649\n",
            "bengali\n",
            " the the the prepared prepared prepared . . . . . . . . .\n",
            " situated near distance distance situated . . . . . .\n",
            " traditional factors one often good one every one every public . . . one . . . . one\n",
            " built construction built ` construction ` construction ram construction first ram memorial made place memorial made . fort . fort . . . . . . . . . . . . .\n",
            " however one one one one one one one one one treatment . . . . . .\n",
            " the also also also also also also occurs . . . . . . . . .\n",
            " one back back . . . . . . . . .\n",
            " last occurs last towards left . . . . . . . . . . .\n",
            " beautiful beautiful     \n",
            " free free day fun water great beaches best day . day . day way get . way way way way way . . . . . . . . way  . .\n",
            "50004\n",
            "34443\n",
            "kannada\n",
            " government government government eastern government government government government government government . . . . . . . .\n",
            " i i i i i i ?       \n",
            " historical time island historical ancient historical ancient historical historical historical . . . . . . . . . . .\n",
            " tourists tourists tourists . . . . . . . . . .\n",
            " tell `` first tell given said said . people . . . . . . . . . . . . . . .\n",
            " because becomes white used white used less less . . . . . . . . .\n",
            " good good good         \n",
            " start start      \n",
            " however however local local water local water water local . . . . . . .\n",
            " located region located region situated feet place situated feet also found sea sea also . also . . . . . . . . . . . . . .\n",
            "50004\n",
            "38429\n",
            "gujarati\n",
            " situated city located . . . . . . . . . . . . . . . . . . .\n",
            " people people people people people people people people people people . . . . . . . . . . .\n",
            " one first first first . . . . . . . .\n",
            " way also problem also even also . . . . . . . . . . . .\n",
            " also necessary also necessary also 1 2 2 . . . . . . . . . . . . . .\n",
            " the water water place place place . . . . . . . . .\n",
            " also per north 1 15 north north . . . . . . . . . . . . . . .\n",
            " modern many many many many many many . . . . . . . . .\n",
            " also also also also also also city also . . . . . . . . . .\n",
            " oil use oil oil oil per per per . . . . . . . . . . .\n",
            "50004\n",
            "33319\n",
            "telgu\n",
            " people known known known called people known called people known name people named people people people people people people people people . . . . . . . . . . .\n",
            " body body power power body body body form body form . . . . .\n",
            " still many much important many one still first many many many many many many many many . . . . . . . . . . . .\n",
            " flowers plants plants plants plants . . . . . . . . .\n",
            " one one one one also even even even even even even . . . . . . . . . . . . . .\n",
            " describe government include  power  . . . . .\n",
            " describe good best best best best best good . . . . . . . .\n",
            " also known dam dam known two 10 various various agricultural . . . . . . . .\n",
            " also like like like well also also . . . . . . . . .\n",
            " hot water water water water water water water . . . . . . . . . . . .\n",
            "50004\n",
            "39109\n",
            "malayalam\n",
            " many several several several several several several several several several several etc many . . . . . . . .\n",
            " if person example called called called often use . . . . . . . . .\n",
            " might also money money money money money money money . . . . . . . . . . .\n",
            " set six six six six      \n",
            " food food food        \n",
            " also also patient patient patient patient patient . . . . . . . . . . .\n",
            " many many many many many many many . . . . . .\n",
            " hindu hindu hindu hindu hindu hindu hindu hindu hindu hindu hindu hindu hindu hindu hindu . . . . . . .\n",
            " many some hindu indian among tamil . . . . . . . .\n",
            " find 2 good one one last . . . . . . . . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"/content/transformer_translations\"\n",
        "RESULTS = \"answer.csv\"\n",
        "VALIDATION_FILE = \"/content/test_data.json\"\n",
        "\n",
        "if os.path.exists(os.path.join(DATA_DIR, RESULTS)):\n",
        "    os.remove(os.path.join(DATA_DIR,RESULTS))\n",
        "val_ids = []\n",
        "\n",
        "with open(VALIDATION_FILE, \"r\") as file:\n",
        "    val_data = json.load(file)\n",
        "    for lang_type, lang_data in val_data.items():\n",
        "        print(lang_type)\n",
        "        for data_type, data_entries in lang_data.items():\n",
        "            ids, en_sns = [], []\n",
        "            for ent_id, ent_data in data_entries.items():\n",
        "                val_ids.append(int(ent_id))\n",
        "\n",
        "print(len(val_ids))\n",
        "# print(val_ids)\n",
        "\n",
        "# Reading all the dataframes\n",
        "files = os.listdir(DATA_DIR)\n",
        "files = [file for file in files if file.endswith(\".csv\")]\n",
        "print(files)\n",
        "data = [pd.read_csv(os.path.join(DATA_DIR, file)) for file in files]\n",
        "res = pd.concat(data, axis=0, ignore_index=False)\n",
        "res.drop([\"Unnamed: 0\", \"english\"], axis=1, inplace=True)\n",
        "res = res.rename(columns={\"id\": \"ID\", \"translated\": \"Translated\"})\n",
        "res_sorted = res[res[\"ID\"].isin(val_ids)].sort_values(by=\"ID\")\n",
        "res_sorted.to_csv(os.path.join(DATA_DIR, RESULTS), sep=\"\\t\", index=False, quoting=csv.QUOTE_NONNUMERIC, quotechar='\"', escapechar='\\\\')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szJjx5PZVX-l",
        "outputId": "ec2bd1cf-69ed-45b1-d09c-b934f7bd6f38"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English-Bengali\n",
            "English-Gujarati\n",
            "English-Hindi\n",
            "English-Kannada\n",
            "English-Malayalam\n",
            "English-Tamil\n",
            "English-Telgu\n",
            "114643\n",
            "['answer_gujarati_test.csv', 'answer_tamil_test.csv', 'answer_kannada_test.csv', 'answer_hindi_test.csv', 'answer_bengali_test.csv', 'answer_malayalam_test.csv', 'answer_telgu_test.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zo_nnly3VYAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Gd5rU4YVYC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZWMPoWvJVYGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8OxP9hmtH5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}