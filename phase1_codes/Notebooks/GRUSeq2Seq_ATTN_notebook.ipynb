{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089f37d1",
   "metadata": {},
   "source": [
    "## Implementation of Transformer for machine translation\n",
    "### The __torchtext.data__ may through an error stating `no module found named \"Field\"` which probably arises due to deprecation of this module in the newer version of torch. Execute the cell below to install the `torchtext version 0.6.0` to run the notebook. This is because the _Field_ and _TabularDataset_ makes the vocabulary and dataloader creation much simpler.\n",
    "```python\n",
    "pip install torchtext==0.6.0\n",
    "print(torchtext.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49fb724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchtext==0.6.0\n",
    "# print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6efed14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import random\n",
    "from collections import Counter\n",
    "from torchtext import vocab\n",
    "import warnings\n",
    "import re, string\n",
    "from string import digits\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a5ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Convert all the text into lower letters\n",
    "    Remove the words betweent brakets ()\n",
    "    Remove these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
    "    Replace these special characters with space:\n",
    "    Replace extra white spaces with single white spaces\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = re.sub('[$)\\\"’°;\\'€%:,(/]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\u200d', ' ', text)\n",
    "    text = re.sub('\\u200c', ' ', text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    text = re.sub('  ', ' ', text)\n",
    "    text = re.sub('   ', ' ', text)\n",
    "    text =\" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e63a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset name\n",
    "l = \"malayalam\"\n",
    "\n",
    "# Read the CSV file from the specified directory into a DataFrame\n",
    "data = pd.read_csv('../Data/{}.csv'.format(l))\n",
    "\n",
    "# Drop the unnecessary columns \"Unnamed: 0\" and \"entry_id\" from the DataFrame\n",
    "data.drop([\"Unnamed: 0\", \"entry_id\"], inplace=True, axis=1)\n",
    "\n",
    "# Note: The next operation seems redundant as \"entry_id\" has already been dropped.\n",
    "# Rename the column \"entry_id\" to \"id\" (if it exists)\n",
    "data = data.rename(columns={\"entry_id\": \"id\"})\n",
    "\n",
    "# Display the first 10 rows of the cleaned DataFrame \n",
    "# (This will be visible in interactive environments like Jupyter Notebook)\n",
    "data.head(10)\n",
    "\n",
    "# Write the cleaned data back to a new CSV file in the current directory\n",
    "data.to_csv(\"{}.csv\".format(l), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b2bea3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>malayalam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Earlier it was believed that women develops co...</td>\n",
       "      <td>ആദ്യം മനസ്സിലാക്കിയിരുന്നത് ഇത് കേവലം ജനനസംബന്...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This can be hard to do, but having an emergenc...</td>\n",
       "      <td>ഇത് ചെയ്യാൻ ബുദ്ധിമുട്ടായിരിക്കാം, പക്ഷേ ഒരു അ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Encourage them to put money away so they'll se...</td>\n",
       "      <td>പണം മാറ്റിവയ്ക്കാൻ അവരെ പ്രോത്സാഹിപ്പിക്കുകവഴി...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Traditional spices like cloves , cardamom , bl...</td>\n",
       "      <td>ഗ്രാമ്പൂ , ഏലക്കാ , കുരുമുളക് , ജാതിക്കാ , കായ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is said that respectable main six Raga are ...</td>\n",
       "      <td>പറയുന്നത് എന്തെന്നാല്‍ ശ്രീ പ്രമുഖ ആറു രാഗം ശങ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>olly how is the new movie swat</td>\n",
       "      <td>olly പുതിയ ചലച്ചിത്രം ദൃശ്യം എങ്ങനെയുണ്ട്</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In its first weekend the film collected ₹7 mil...</td>\n",
       "      <td>ആദ്യ വാരാന്ത്യത്തിൽ ഈ ചിത്രം 7 ദശലക്ഷം രൂപ (92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Patients becomes weak .</td>\n",
       "      <td>രോഗി ദുര്‍ബലനായി മാറുന്നു .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Singh enrolled on a 12-week transformation pro...</td>\n",
       "      <td>സിംഗ് സ്റ്റീവൻസിനൊപ്പം കർശനമായ പ്രോട്ടീൻ ഡയറ്റ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>As of 2010, the Asia Pacific Floorball Champio...</td>\n",
       "      <td>2010 ലെ കണക്കനുസരിച്ച്, ഏഷ്യ പസഫിക് ഫ്ലോർബോൾ ച...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  Earlier it was believed that women develops co...   \n",
       "1  This can be hard to do, but having an emergenc...   \n",
       "2  Encourage them to put money away so they'll se...   \n",
       "3  Traditional spices like cloves , cardamom , bl...   \n",
       "4  It is said that respectable main six Raga are ...   \n",
       "5                     olly how is the new movie swat   \n",
       "6  In its first weekend the film collected ₹7 mil...   \n",
       "7                            Patients becomes weak .   \n",
       "8  Singh enrolled on a 12-week transformation pro...   \n",
       "9  As of 2010, the Asia Pacific Floorball Champio...   \n",
       "\n",
       "                                           malayalam  \n",
       "0  ആദ്യം മനസ്സിലാക്കിയിരുന്നത് ഇത് കേവലം ജനനസംബന്...  \n",
       "1  ഇത് ചെയ്യാൻ ബുദ്ധിമുട്ടായിരിക്കാം, പക്ഷേ ഒരു അ...  \n",
       "2  പണം മാറ്റിവയ്ക്കാൻ അവരെ പ്രോത്സാഹിപ്പിക്കുകവഴി...  \n",
       "3  ഗ്രാമ്പൂ , ഏലക്കാ , കുരുമുളക് , ജാതിക്കാ , കായ...  \n",
       "4  പറയുന്നത് എന്തെന്നാല്‍ ശ്രീ പ്രമുഖ ആറു രാഗം ശങ...  \n",
       "5          olly പുതിയ ചലച്ചിത്രം ദൃശ്യം എങ്ങനെയുണ്ട്  \n",
       "6  ആദ്യ വാരാന്ത്യത്തിൽ ഈ ചിത്രം 7 ദശലക്ഷം രൂപ (92...  \n",
       "7                        രോഗി ദുര്‍ബലനായി മാറുന്നു .  \n",
       "8  സിംഗ് സ്റ്റീവൻസിനൊപ്പം കർശനമായ പ്രോട്ടീൻ ഡയറ്റ...  \n",
       "9  2010 ലെ കണക്കനുസരിച്ച്, ഏഷ്യ പസഫിക് ഫ്ലോർബോൾ ച...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the cleaned\n",
    "data = pd.read_csv(\"{}.csv\".format(l))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a3ff93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d192131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text): \n",
    "    \"\"\"\n",
    "    Tokenize the input text.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): Input text to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    - list: List of tokens.\n",
    "    \"\"\"\n",
    "    return [tok for tok in preprocess(text).split()]\n",
    "\n",
    "# Define Fields for tokenization and preprocessing\n",
    "lang = Field(tokenize = tokenizer, lower = True, init_token = \"<sos>\", eos_token = \"<eos>\")\n",
    "eng = Field(tokenize = tokenizer, lower = True, init_token = \"<sos>\", eos_token = \"<eos>\")\n",
    "\n",
    "# Define data fields for loading the dataset\n",
    "datafields = [(\"english\", eng), (\"{}\".format(l), lang)]\n",
    "# Load the dataset from a CSV file\n",
    "dataset = TabularDataset(path=\"{}.csv\".format(l), format='csv', skip_header=True, fields=datafields)\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = dataset.split(split_ratio = 0.80)\n",
    "\n",
    "# Build vocabulary for each language from the training data\n",
    "lang.build_vocab(train_data, min_freq = 1, max_size = 50000)\n",
    "eng.build_vocab(train_data, min_freq = 1, max_size = 50000)\n",
    "\n",
    "# creating the train and validation data iterator for training\n",
    "train_iterator, val_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data), \n",
    "    batch_size = 32, \n",
    "    device = device, \n",
    "    sort_key = lambda x: getattr(x,l),  # change the language after x.\n",
    "    sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "137545f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: ['earlier', 'it', 'was', 'believed', 'that', 'women', 'develops', 'colon', 'cancer', 'only', 'for', 'genetic', 'reasons', 'but', 'now', 'with', 'the', 'increasing', 'smoking', 'and', 'drinking', 'habits', 'in', 'women', 'they', 'have', 'high', 'risk', 'of', 'colon', 'cancer', '.']\n",
      "Malayalam: ['ആദ്യം', 'മനസ്സിലാക്കിയിരുന്നത്', 'ഇത്', 'കേവലം', 'ജനനസംബന്ധമായ', 'കാരണങ്ങളാല്', 'സ്ത്രീകളില്', 'വന്', 'കുടല്', 'അര്', 'ബുദം', 'ഉണ്ടാകുന്നു', 'എന്നാണ്', 'എന്നാല്', 'ഇപ്പോള്', 'സ്ത്രീകളില്', 'കൂടിവരുന്ന', 'പുകവലിയും', 'മദ്യപാനശീലത്താലും', 'വന്', 'കുടല്', 'അര്', 'ബുദത്തിന്', 'റെ', 'ഭീതി', 'കൂടുന്നു', '.']\n",
      "---\n",
      "English: ['this', 'can', 'be', 'hard', 'to', 'do', 'but', 'having', 'an', 'emergency', 'fund', 'is', 'necessary', 'to', 'protect', 'you', 'and', 'your', 'family', 'in', 'an', 'emergency', '.']\n",
      "Malayalam: ['ഇത്', 'ചെയ്യാൻ', 'ബുദ്ധിമുട്ടായിരിക്കാം', 'പക്ഷേ', 'ഒരു', 'അടിയന്തിര', 'സാഹചര്യത്തിൽ', 'നിങ്ങളെയും', 'നിങ്ങളുടെ', 'കുടുംബത്തെയും', 'സംരക്ഷിക്കാൻ', 'ഒരു', 'അടിയന്തര', 'ഫണ്ട്', 'ആവശ്യമാണ്', '.']\n",
      "---\n",
      "English: ['encourage', 'them', 'to', 'put', 'money', 'away', 'so', 'theyll', 'see', 'how', 'money', 'grows', 'over', 'time', '.']\n",
      "Malayalam: ['പണം', 'മാറ്റിവയ്ക്കാൻ', 'അവരെ', 'പ്രോത്സാഹിപ്പിക്കുകവഴി', 'പണം', 'എങ്ങനെ', 'കാലക്രമേണ', 'വളരുന്നുവെന്ന്', 'അവർ', 'കാണും', '.']\n",
      "---\n",
      "English: ['traditional', 'spices', 'like', 'cloves', 'cardamom', 'black', 'pepper', 'nutmeg', 'asafoetida', 'turmeric', 'are', 'there', '.']\n",
      "Malayalam: ['ഗ്രാമ്പൂ', 'ഏലക്കാ', 'കുരുമുളക്', 'ജാതിക്കാ', 'കായം', 'മഞ്ഞള്', 'പോലുള്ള', 'പരമ്പരാഗത', 'മസാലകളും', 'ഉണ്ട്', '.']\n",
      "---\n",
      "English: ['it', 'is', 'said', 'that', 'respectable', 'main', 'six', 'raga', 'are', 'the', 'sons', 'of', 'god', 'shankara', '.']\n",
      "Malayalam: ['പറയുന്നത്', 'എന്തെന്നാല്', 'ശ്രീ', 'പ്രമുഖ', 'ആറു', 'രാഗം', 'ശങ്കര്', 'ഭഗവാന്', 'റെ', 'പുത്രനാണ്', '.']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# View the first 5 examples\n",
    "for i, example in enumerate(dataset.examples):\n",
    "    if i >= 5:  # limit to first 5 for demonstration purposes\n",
    "        break\n",
    "    print(\"English:\", example.english)\n",
    "    print(\"{}:\".format(l.title()), getattr(example, l))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64b0a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "\n",
    "# Define hyperparameters and model configuration values\n",
    "DROPOUT_RATE = 0.2 # Dropout rate used for regularization in the model\n",
    "EPOCHS = 1 # Total number of training epochs (full passes over the training dataset)\n",
    "BATCH_SIZE = 16 # Number of training examples processed in a single batch during training\n",
    "TEACHER_FORCE_RATIO = 0.1 # Probability with which true target tokens are used as the next input instead of the predicted tokens during training (used in sequence-to-sequence models)\n",
    "NUM_LAYERS =  1 # Number of recurrent layers in the model\n",
    "HIDDEN_SIZE = 600 # Number of features in the hidden state of the recurrent unit (e.g., GRU or LSTM)\n",
    "EMBEDDING_SIZE = 300 # Size of the embedding vectors used to represent tokens\n",
    "SRC_VOCAB_SIZE = len(eng.vocab) # Vocabulary size for the source language (English in this case)\n",
    "TAR_VOCAB_SIZE = len(lang.vocab) # Vocabulary size for the target language\n",
    "PAD_IDX = eng.vocab.stoi[\"<pad>\"]  # Index for the padding token\n",
    "SOS_IDX = eng.vocab.stoi[\"<sos>\"] # index for start token\n",
    "EOD_IDX = eng.vocab.stoi[\"<eos>\"] # index for end token\n",
    "INPUT_SIZE_EN = SRC_VOCAB_SIZE # Input size for the encoder (equal to the source vocabulary size)\n",
    "INPUT_SIZE_DR = TAR_VOCAB_SIZE\n",
    "OUTPUT_SIZE_DR = TAR_VOCAB_SIZE # Input and output sizes for the decoder (equal to the target vocabulary size)\n",
    "LEARNING_RATE = 0.001 # Learning rate for the optimizer\n",
    "WEIGHT_DECAY = 0.0008 # Weight decay parameter for regularization in the optimizer\n",
    "eng_tokens = [] # List to store tokenized sentences for the source language\n",
    "bn_tokens = [] # List to store tokenized sentences for the target language (Bengali in this example)\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") # Device configuration (uses GPU if available, otherwise falls back to CPU)\n",
    "pad_idx = eng.vocab.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b4add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module for the Seq2Seq architecture with attention.\n",
    "\n",
    "    Attributes:\n",
    "    - embedding: An embedding layer that transforms input tokens into embeddings.\n",
    "    - gru: A bi-directional GRU (Gated Recurrent Unit) layer.\n",
    "    - fc_hidden: A linear layer that reduces the combined forward and backward hidden states to the desired hidden size.\n",
    "    - dropout: A dropout layer for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, bidirectional=True)\n",
    "        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p = DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "\n",
    "        Arguments:\n",
    "        - x: Source sequence.\n",
    "\n",
    "        Returns:\n",
    "        - encoder_states: Outputs of the GRU for each step.\n",
    "        - hidden_state: Combined hidden state for forward and backward GRU.\n",
    "        \"\"\"\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        encoder_states, hidden = self.gru(embedding)\n",
    "\n",
    "        # Combine forward and backward hidden states\n",
    "        forward_hidden = hidden[0:1]\n",
    "        backward_hidden = hidden[1:2]\n",
    "        hidden_concat = torch.cat((forward_hidden, backward_hidden), dim = 2)\n",
    "        hidden_state = self.fc_hidden(hidden_concat)\n",
    "        return encoder_states, hidden_state\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module for the Seq2Seq architecture with attention.\n",
    "\n",
    "    Attributes:\n",
    "    - embedding: An embedding layer that transforms target tokens into embeddings.\n",
    "    - gru: A GRU (Gated Recurrent Unit) layer.\n",
    "    - attention_layer: A linear layer to compute attention scores.\n",
    "    - fc_layer: A linear layer to produce the output tokens.\n",
    "    - dropout: A dropout layer for regularization.\n",
    "    - softmax_layer: Softmax activation for attention scores.\n",
    "    - gelu: GELU activation function used in attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(hidden_dim * 2 + embed_dim, hidden_dim, num_layers)\n",
    "        self.attention_layer = nn.Linear(hidden_dim * 3, 1)\n",
    "        self.fc_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(p = DROPOUT_RATE)\n",
    "        self.softmax_layer = nn.Softmax(dim = 0)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x, encoder_states, hidden_state):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "\n",
    "        Arguments:\n",
    "        - x: Target sequence.\n",
    "        - encoder_states: Output from the encoder.\n",
    "        - hidden_state: Last hidden state from the encoder.\n",
    "\n",
    "        Returns:\n",
    "        - predictions: Predicted output tokens.\n",
    "        - hidden_state: Hidden state after passing through the GRU.\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(0)\n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        # Attention mechanism\n",
    "        hidden_state_reshaped = hidden_state.repeat(sequence_length, 1, 1)\n",
    "        inp_state = torch.cat((hidden_state_reshaped, encoder_states), dim = 2)\n",
    "        attention_score = self.gelu(self.attention_layer(inp_state))\n",
    "        attention_score = self.softmax_layer(attention_score)\n",
    "        context_vector = torch.einsum(\"snk,snl->knl\", attention_score, encoder_states)\n",
    "\n",
    "        gru_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        outputs, hidden_state = self.gru(gru_input, hidden_state)\n",
    "        predictions = self.fc_layer(outputs).squeeze(0)\n",
    "        return predictions, hidden_state\n",
    "\n",
    "\n",
    "class GRUSeq2SeqAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based Seq2Seq model with attention mechanism.\n",
    "\n",
    "    Attributes:\n",
    "    - encoder: Encoder module.\n",
    "    - decoder: Decoder module.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GRUSeq2SeqAttn, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Forward pass of the Seq2Seq model.\n",
    "\n",
    "        Arguments:\n",
    "        - source: Source sequence.\n",
    "        - target: Target sequence.\n",
    "        - teacher_force_ratio: Probability to use true target tokens as next input instead of predictions.\n",
    "\n",
    "        Returns:\n",
    "        - outputs: Predicted target sequence.\n",
    "        \"\"\"\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = TAR_VOCAB_SIZE\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        # Pass source through encoder\n",
    "        encoder_states, hidden_state = self.encoder(source)\n",
    "        x = target[0]\n",
    "\n",
    "        # Decode the encoder's output\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden = self.decoder(x, encoder_states, hidden_state)\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(dim = 1)\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd0b6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Encoder module with the specified input size, embedding size, hidden size, and number of layers.\n",
    "# The model is moved to the specified device (GPU or CPU).\n",
    "encoder = Encoder(INPUT_SIZE_EN, EMBEDDING_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
    "\n",
    "# Instantiate the Decoder module with the specified input size, embedding size, hidden size, output size, \n",
    "# and number of layers. The model is also moved to the specified device.\n",
    "decoder = Decoder(INPUT_SIZE_DR, EMBEDDING_SIZE, HIDDEN_SIZE, OUTPUT_SIZE_DR, NUM_LAYERS).to(device)\n",
    "\n",
    "# Instantiate the main GRU-based Sequence-to-Sequence model by combining the Encoder and Decoder modules. \n",
    "model = GRUSeq2SeqAttn(encoder, decoder).to(device)\n",
    "\n",
    "# Initialize the Adam optimizer with the specified learning rate to optimize the model parameters.\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Define the loss criterion\n",
    "# CrossEntropyLoss is used since this is a classification task, and we ignore the loss computed on padding tokens\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60366488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, model, eng, lang, max_len = 20):\n",
    "    \"\"\"\n",
    "    Translates a given text from the source language to the target language using the provided trained model.\n",
    "    \n",
    "    Args:\n",
    "    - text (str or list): The input text to be translated. Can be a string or a list of tokens.\n",
    "    - model (nn.Module): The trained sequence-to-sequence model used for translation.\n",
    "    - eng (torchtext.data.Field): The Field object for the source language (English in this case).\n",
    "    - lang (torchtext.data.Field): The Field object for the target language.\n",
    "    - max_len (int, optional): Maximum length of the translated output. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "    - str: The translated text in the target language.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the input text is a string, tokenize it.\n",
    "    if type(text) == str:\n",
    "        tokens = [tok for tok in indic_tokenize.trivial_tokenize_indic(text)]\n",
    "    \n",
    "    # Add the start and end tokens to the tokenized text.\n",
    "    tokens.insert(0, eng.init_token)\n",
    "    tokens.append(eng.eos_token)\n",
    "\n",
    "    # Convert tokens to their respective indices from the vocabulary.\n",
    "    txt2idx = [eng.vocab.stoi[tok] for tok in tokens]\n",
    "\n",
    "    # Convert token indices to a tensor and move it to the specified device (GPU or CPU).\n",
    "    st = torch.LongTensor(txt2idx).unsqueeze(1).to(device)\n",
    "\n",
    "    # Initialize the result list with the index of the start token.\n",
    "    res = [eng.vocab.stoi[0]]\n",
    "\n",
    "    # Generate the translation iteratively.\n",
    "    for i in range(1, max_len):\n",
    "        tt = torch.LongTensor(res).unsqueeze(1).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(st, tt)\n",
    "            best_guess = output.argmax(2)[-1, :].item()\n",
    "\n",
    "            # If the end token is predicted, stop the translation.\n",
    "            if best_guess == lang.vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            res.append(best_guess)\n",
    "\n",
    "    # Convert the indices in the result list back to tokens.\n",
    "    tsent = [lang.vocab.itos[index] for index in res]\n",
    "\n",
    "    # Return the translated sentence as a string, replacing any unknown tokens with a space.\n",
    "    return \" \".join(tsent[1:]).replace(\"<unk>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7b83373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch no: 0 / 1]\n",
      "Train loss -> 0 steps: 10.255\n",
      "                                     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Backward pass: Compute gradient of loss w.r.t. model parameters.\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Clip the gradients to prevent them from exploding (a common issue in RNNs).\u001b[39;00m\n\u001b[1;32m     43\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/autogpt/SemEval/str/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autogpt/SemEval/str/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List to store training losses after each epoch.\n",
    "train_losses = []\n",
    "\n",
    "# List to store validation losses after each epoch.\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "# Start the training process over specified number of epochs.\n",
    "for epoch in range(EPOCHS):\n",
    "    # Initialize the epoch-level training and validation loss.\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    # Print out the current epoch number.\n",
    "    print(f\"[Epoch no: {epoch} / {EPOCHS}]\")\n",
    "\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over each batch in the training data.\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Move the input and target data to the specified device.\n",
    "        inp_data = batch.english.to(device)\n",
    "        target = getattr(batch, l).to(device)\n",
    "\n",
    "        # Forward pass: Get model predictions for the current batch.\n",
    "        output = model(inp_data, target)\n",
    "\n",
    "        # Reshape the output and target for loss calculation.\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        # Zero out any previously calculated gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the loss between model predictions and actual target.\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass: Compute gradient of loss w.r.t. model parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradients to prevent them from exploding (a common issue in RNNs).\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the model parameters using the computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the training loss.\n",
    "        train_loss += ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n",
    "\n",
    "        # Print training loss every 100 steps and a sample translation.\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train loss -> {} steps: {:.3f}'.format(batch_idx, train_loss))\n",
    "            print(translate(\"Football is a tough game\", model, eng, lang, max_len=20))\n",
    "\n",
    "    # Set the model to evaluation mode for validation.\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over each batch in the validation data.\n",
    "    for batch_idx, batch in enumerate(val_iterator):\n",
    "        inp_data = batch.english.to(device)\n",
    "        target = getattr(batch, l).to(device)\n",
    "        output = model(inp_data, target)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        # Compute the loss between model predictions and actual target.\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Update the validation loss.\n",
    "        valid_loss += ((1 / (batch_idx + 1)) * (loss.data.item() - valid_loss))\n",
    "\n",
    "    # Append epoch-level train and validation loss to respective lists.\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)\n",
    "\n",
    "    # Print epoch-level summary.\n",
    "    print('Epoch no: {} \\tTraining Loss: {:.5f} \\tValidation Loss: {:.5f}'.format(epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e25e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the plots.\n",
    "PLOT_DIR = \"gru_attention_plots\"\n",
    "\n",
    "# Check if the directory exists. If not, create it.\n",
    "if not os.path.exists(PLOT_DIR):\n",
    "    os.makedirs(PLOT_DIR)\n",
    "\n",
    "def plotresults():\n",
    "    \"\"\"\n",
    "    Function to plot training and validation loss over epochs.\n",
    "\n",
    "    The function uses the matplotlib library to plot the loss curves for \n",
    "    training and validation data. It saves the generated plot in the specified\n",
    "    directory (`plot_dir`) with a filename based on the language (`l`).\n",
    "    \"\"\"\n",
    "    # Plotting the training loss (in black color with circle markers).\n",
    "    plt.plot(range(len(train_losses)), train_losses, marker = \"o\", color = \"black\")\n",
    "\n",
    "    # Plotting the validation loss (in blue color with circle markers).\n",
    "    plt.plot(range(len(val_losses)), val_losses, marker = \"o\", color = \"blue\")\n",
    "\n",
    "    # Adding legend to distinguish between train and validation curves.\n",
    "    plt.legend([\"Train loss\", \"Val loss\"])\n",
    "\n",
    "    # Adding title and axis labels to the plot.\n",
    "    plt.title(\"Loss curves\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss values\")\n",
    "\n",
    "    # Displaying grid for better visualization.\n",
    "    plt.grid()\n",
    "\n",
    "    # Save the plot as a PNG image in the specified directory with a filename\n",
    "    # based on the language (`l`).\n",
    "    plt.savefig(os.path.join(PLOT_DIR,\"loss_{}.png\".format(l)))\n",
    "\n",
    "# Call the function to plot the results.\n",
    "plotresults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a58884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "# Check if the \"Translations\" directory exists. If not, create it.\n",
    "if not os.path.exists(\"gru_attention_translations\"):\n",
    "    os.makedirs(\"gru_attention_translations\")\n",
    "\n",
    "def evaluate(language):\n",
    "    \"\"\"\n",
    "    Function to evaluate and generate translations for given test data.\n",
    "    \n",
    "    This function reads a CSV file containing English sentences, \n",
    "    translates each sentence to the target language using the \n",
    "    trained model, and then saves the translations to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - language: The target language for translation.\n",
    "\n",
    "    Outputs:\n",
    "    - A CSV file named \"answer1_{language}_test.csv\" saved in the \"Translations\" directory.\n",
    "      This file contains the original English sentences and their corresponding translations.\n",
    "    \"\"\"\n",
    "    # List to store the predicted translations.\n",
    "    predictions = []\n",
    "\n",
    "    # Read the test data from the specified CSV file.\n",
    "    data = pd.read_csv(\"./../../testData/testEnglish-{}.csv\".format(language))\n",
    "    data = data.iloc[0:100,:] # selecting first 100 english sents\n",
    "    # Loop through each row (sentence) in the test data.\n",
    "    for idx, row in data.iterrows():\n",
    "        # Extract the English sentence.\n",
    "        en = row[\"english\"]\n",
    "\n",
    "        # Translate the English sentence to the target language.\n",
    "        pred = translate(en, model, eng, lang, max_len=20)\n",
    "\n",
    "        # Print the translated sentence (optional, can be commented out).\n",
    "        print(pred)\n",
    "\n",
    "        # Append the translated sentence to the predictions list.\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Add the predicted translations as a new column to the original dataframe.\n",
    "    data[\"translated\"] = predictions\n",
    "\n",
    "    # Drop the unwanted column \"Unnamed: 0\" (assuming it exists in the CSV).\n",
    "    data.drop([\"Unnamed: 0\"], inplace=True, axis=1)\n",
    "\n",
    "    # Save the dataframe with translations to a new CSV file.\n",
    "    data.to_csv(os.path.join(\"gru_attention_translations\", \"answer_{}_test.csv\".format(language)))\n",
    "\n",
    "# Evaluate the model on the Bengali test set.\n",
    "evaluate(l.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f63db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea402184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "str",
   "language": "python",
   "name": "str"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
