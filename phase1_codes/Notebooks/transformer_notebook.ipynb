{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c659760",
   "metadata": {},
   "source": [
    "## Implementation of Transformer for machine translation\n",
    "### The __torchtext.data__ may through an error stating `no module found named \"Field\"` which probably arises due to deprecation of this module in the newer version of torch. Execute the cell below to install the `torchtext version 0.6.0` to run the notebook. This is because the _Field_ and _TabularDataset_ makes the vocabulary and dataloader creation much simpler.\n",
    "```python\n",
    "pip install torchtext==0.6.0\n",
    "print(torchtext.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9051bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchtext==0.6.0\n",
    "# print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c314c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import vocab\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch import Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder,TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import re, string\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import digits\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "817532c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Convert all the text into lower letters\n",
    "    Remove the words betweent brakets ()\n",
    "    Remove these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
    "    Replace these special characters with space:\n",
    "    Replace extra white spaces with single white spaces\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = re.sub('[$)\\\"’°;\\'€%:,(/]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\u200d', ' ', text)\n",
    "    text = re.sub('\\u200c', ' ', text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    text = re.sub('  ', ' ', text)\n",
    "    text = re.sub('   ', ' ', text)\n",
    "    text =\" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a17fe1",
   "metadata": {},
   "source": [
    "#### Change the l variable to the language you want to translate to.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e9d9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset name\n",
    "l = \"tamil\"\n",
    "# Read the CSV file from the specified directory into a DataFrame\n",
    "data = pd.read_csv('../Data/{}.csv'.format(l))\n",
    "\n",
    "# Drop the unnecessary columns \"Unnamed: 0\" and \"entry_id\" from the DataFrame\n",
    "data.drop([\"Unnamed: 0\", \"entry_id\"], inplace=True, axis=1)\n",
    "\n",
    "# Note: The next operation seems redundant as \"entry_id\" has already been dropped.\n",
    "# Rename the column \"entry_id\" to \"id\" (if it exists)\n",
    "data = data.rename(columns={\"entry_id\": \"id\"})\n",
    "\n",
    "# Display the first 10 rows of the cleaned DataFrame \n",
    "# (This will be visible in interactive environments like Jupyter Notebook)\n",
    "data.head(10)\n",
    "\n",
    "# Write the cleaned data back to a new CSV file in the current directory\n",
    "data.to_csv(\"{}.csv\".format(l), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1248d195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>tamil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The nature and scope of trafficking range from...</td>\n",
       "      <td>தொழில்துறை மற்றும் உள்நாட்டு தொழிலாளர் இருந்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kerala is her heart and agrarian Palakkad can ...</td>\n",
       "      <td>கேரளா அவரது இதயம் என்றும், மற்றும் பாலக்காடு வ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what's the weather like right now in new york</td>\n",
       "      <td>சென்னையில் இப்போது வானிலை எப்படி இருக்கிறது</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tell me how to cook a cheese souffle</td>\n",
       "      <td>சீஸ் சூப் எப்படி சமைக்க வேண்டும் என்று சொல்லுங...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These structures are made of beautifully carve...</td>\n",
       "      <td>இந்த கட்டமைப்புகள் அழகாக செதுக்கப்பட்ட கற்களால...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Travel to the city, Kochi, that has moved so b...</td>\n",
       "      <td>கொச்சி நகரத்திற்கு பயணம் செய்யுங்கள், வரலாற்றி...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It is at an altitude of 2,438 metres (7,999 ft...</td>\n",
       "      <td>இது நாகாலாந்தில் உள்ள ஜாப்ஃபூ மலைக்கு பின்புறம...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Any portion of your funds that are unused will...</td>\n",
       "      <td>பங்குத் தொகுப்புகளுக்கான விருப்பங்கள் விநியோகி...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Founded in 1787 by the East India Company, the...</td>\n",
       "      <td>20 கி.மீ –ல், 1787-ல் கிழக்கு இந்திய கம்பெனியா...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A population mean volume of 650 ml would be co...</td>\n",
       "      <td>650 மில்லி ஒரு மக்கள் சராசரி அளவு குறைந்த கருத...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  The nature and scope of trafficking range from...   \n",
       "1  Kerala is her heart and agrarian Palakkad can ...   \n",
       "2      what's the weather like right now in new york   \n",
       "3               tell me how to cook a cheese souffle   \n",
       "4  These structures are made of beautifully carve...   \n",
       "5  Travel to the city, Kochi, that has moved so b...   \n",
       "6  It is at an altitude of 2,438 metres (7,999 ft...   \n",
       "7  Any portion of your funds that are unused will...   \n",
       "8  Founded in 1787 by the East India Company, the...   \n",
       "9  A population mean volume of 650 ml would be co...   \n",
       "\n",
       "                                               tamil  \n",
       "0  தொழில்துறை மற்றும் உள்நாட்டு தொழிலாளர் இருந்...  \n",
       "1  கேரளா அவரது இதயம் என்றும், மற்றும் பாலக்காடு வ...  \n",
       "2        சென்னையில் இப்போது வானிலை எப்படி இருக்கிறது  \n",
       "3  சீஸ் சூப் எப்படி சமைக்க வேண்டும் என்று சொல்லுங...  \n",
       "4  இந்த கட்டமைப்புகள் அழகாக செதுக்கப்பட்ட கற்களால...  \n",
       "5  கொச்சி நகரத்திற்கு பயணம் செய்யுங்கள், வரலாற்றி...  \n",
       "6  இது நாகாலாந்தில் உள்ள ஜாப்ஃபூ மலைக்கு பின்புறம...  \n",
       "7  பங்குத் தொகுப்புகளுக்கான விருப்பங்கள் விநியோகி...  \n",
       "8  20 கி.மீ –ல், 1787-ல் கிழக்கு இந்திய கம்பெனியா...  \n",
       "9  650 மில்லி ஒரு மக்கள் சராசரி அளவு குறைந்த கருத...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the cleaned data\n",
    "data = pd.read_csv(\"{}.csv\".format(l))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b925ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8ce00226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text): \n",
    "    \"\"\"\n",
    "    Tokenize the input text.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): Input text to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    - list: List of tokens.\n",
    "    \"\"\"\n",
    "    return [tok for tok in preprocess(text).split()]\n",
    "\n",
    "# Define Fields for tokenization and preprocessing\n",
    "lang = Field(tokenize = tokenizer, lower = True, init_token = \"<sos>\", eos_token = \"<eos>\")\n",
    "eng = Field(tokenize = tokenizer, lower = True, init_token = \"<sos>\", eos_token = \"<eos>\")\n",
    "\n",
    "# Define data fields for loading the dataset\n",
    "datafields = [(\"english\", eng), (\"{}\".format(l), lang)]\n",
    "# Load the dataset from a CSV file\n",
    "dataset = TabularDataset(path=\"{}.csv\".format(l), format='csv', skip_header=True, fields=datafields)\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = dataset.split(split_ratio = 0.80)\n",
    "\n",
    "# Build vocabulary for each language from the training data\n",
    "lang.build_vocab(train_data, min_freq = 1, max_size = 50000)\n",
    "eng.build_vocab(train_data, min_freq = 1, max_size = 50000)\n",
    "\n",
    "# creating the train and validation data iterator for training\n",
    "train_iterator, val_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data), \n",
    "    batch_size = 8, \n",
    "    device = device, \n",
    "    sort_key = lambda x: getattr(x,l),  # change the language after x.\n",
    "    sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b10226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42882\n",
      "50004\n"
     ]
    }
   ],
   "source": [
    "print(len(eng.vocab))\n",
    "print(len(lang.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bbd2a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMT(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Transformer model for Machine Translation (MT).\n",
    "    \n",
    "    This model consists of an encoder and a decoder, both built using the Transformer architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    - nhead (int): Number of heads in the multihead attention mechanism.\n",
    "    - embed_size (int): Dimension of the embedding vector.\n",
    "    - source_vocab_size (int): Size of the source vocabulary.\n",
    "    - target_vocab_size (int): Size of the target vocabulary.\n",
    "    - num_encoder_layers (int): Number of layers in the transformer encoder.\n",
    "    - num_decoder_layers (int): Number of layers in the transformer decoder.\n",
    "    - ffnn_size (int, optional): Size of the feedforward neural network inside transformer layers. Default is 512.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 nhead: int,\n",
    "                 embed_size: int, \n",
    "                 source_vocab_size: int, \n",
    "                 target_vocab_size: int,\n",
    "                 num_encoder_layers: int, \n",
    "                 num_decoder_layers: int,\n",
    "                 ffnn_size:int = 512):\n",
    "        \n",
    "        super(TransformerMT, self).__init__()\n",
    "        \n",
    "        # Define encoder and decoder layers\n",
    "        encoder_layer = TransformerEncoderLayer(d_model = embed_size, nhead = nhead, dim_feedforward = ffnn_size)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model = embed_size, nhead = nhead, dim_feedforward = ffnn_size)\n",
    "        \n",
    "        # Initialize transformer encoder, decoder, and final fully connected layer\n",
    "        self.tf_encoder = TransformerEncoder(encoder_layer, num_layers = num_encoder_layers)\n",
    "        self.tf_decoder = TransformerDecoder(decoder_layer, num_layers = num_decoder_layers)\n",
    "        self.fc_layer = nn.Linear(embed_size, target_vocab_size)\n",
    "        \n",
    "        # Embedding layers for tokens and positional information\n",
    "        self.src_token_embedding = TokenEmbedding(source_vocab_size, embed_size)\n",
    "        self.tar_token_embedding = TokenEmbedding(target_vocab_size, embed_size)\n",
    "        self.positional_embedding = SinusoidalEmbedding(embed_size, dropout = DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, \n",
    "                source: Tensor,\n",
    "                target: Tensor,\n",
    "                source_mask: Tensor,\n",
    "                target_mask: Tensor,\n",
    "                source_padding_mask: Tensor,\n",
    "                target_padding_mask: Tensor, \n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the TransformerMT model.\n",
    "        \n",
    "        Parameters:\n",
    "        - source (torch.Tensor): Source sequence tensor.\n",
    "        - target (torch.Tensor): Target sequence tensor.\n",
    "        - source_mask (torch.Tensor): Source sequence mask.\n",
    "        - target_mask (torch.Tensor): Target sequence mask.\n",
    "        - source_padding_mask (torch.Tensor): Source padding mask.\n",
    "        - target_padding_mask (torch.Tensor): Target padding mask.\n",
    "        - memory_key_padding_mask (torch.Tensor): Memory key padding mask.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Output tensor after passing through the transformer and the fully connected layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        source_embedding = self.positional_embedding(self.src_token_embedding(source))\n",
    "        target_embedding = self.positional_embedding(self.tar_token_embedding(target))\n",
    "        memory = self.tf_encoder(source_embedding, source_mask, source_padding_mask)\n",
    "        outputs = self.tf_decoder(target_embedding, \n",
    "                                  memory, \n",
    "                                  target_mask, \n",
    "                                  None,\n",
    "                                  target_padding_mask,\n",
    "                                  memory_key_padding_mask)\n",
    "        outputs = self.fc_layer(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def encode(self, source: Tensor, source_mask: Tensor):\n",
    "        \"\"\"\n",
    "        Encode the source sequence using the transformer encoder.\n",
    "        \n",
    "        Parameters:\n",
    "        - source (torch.Tensor): Source sequence tensor.\n",
    "        - source_mask (torch.Tensor): Source sequence mask.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Encoded memory tensor.\n",
    "        \"\"\"\n",
    "        token_rep = self.src_token_embedding(source)\n",
    "        positional_rep = self.positional_embedding(token_rep)\n",
    "        encoder_output = self.tf_encoder(positional_rep, source_mask)\n",
    "        return encoder_output\n",
    "\n",
    "    def decode(self, target: Tensor, memory: Tensor, target_mask: Tensor):\n",
    "        \"\"\"\n",
    "        Decode the memory tensor using the transformer decoder.\n",
    "        \n",
    "        Parameters:\n",
    "        - target (torch.Tensor): Target sequence tensor.\n",
    "        - memory (torch.Tensor): Encoded memory tensor.\n",
    "        - target_mask (torch.Tensor): Target sequence mask.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Decoded tensor.\n",
    "        \"\"\"\n",
    "        token_rep = self.src_token_embedding(target)\n",
    "        positional_rep = self.positional_embedding(token_rep)\n",
    "        decoder_output = self.tf_decoder(positional_rep, memory, target_mask)\n",
    "        return decoder_output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a180eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding for Transformer models.\n",
    "    \n",
    "    The positional encoding module uses sine and cosine functions of different frequencies to \n",
    "    encode the position of tokens in the sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    - embed_size (int): Dimension of the embedding vector.\n",
    "    - dropout (float): Dropout rate for the dropout layer.\n",
    "    - max_len (int, optional): Maximum length of the sequence. Default is 5000.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size, dropout, max_len = 5000):\n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        # Compute the sinusoidal positional encodings\n",
    "        denom = max_len*2\n",
    "        pdist = torch.exp(- torch.arange(0, embed_size, 2) * math.log(denom) / embed_size)\n",
    "        position = torch.arange(0, max_len).reshape(max_len, 1)\n",
    "        position_embedding = torch.zeros((max_len, embed_size))\n",
    "        position_embedding[:, 0::2] = torch.sin(position * pdist)\n",
    "        position_embedding[:, 1::2] = torch.cos(position * pdist)\n",
    "        position_embedding = position_embedding.unsqueeze(-2)\n",
    "        \n",
    "        # Register the position embeddings so they get saved with the model's state_dict\n",
    "        self.register_buffer('position_embedding', position_embedding)\n",
    "\n",
    "    def forward(self, token_embed):\n",
    "        \"\"\"\n",
    "        Forward pass of the SinusoidalEmbedding.\n",
    "        \n",
    "        Parameters:\n",
    "        - token_embed (torch.Tensor): Token embeddings.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Token embeddings added with positional encodings.\n",
    "        \"\"\"\n",
    "        outputs = token_embed + self.position_embedding[:token_embed.size(0),:]\n",
    "        outputs = self.dropout(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Token Embedding module for Transformer models.\n",
    "    \n",
    "    This module converts token indices into dense vectors of fixed size, embed_size. \n",
    "    The embeddings are scaled by the square root of their dimensionality.\n",
    "    \n",
    "    Parameters:\n",
    "    - vocab_size (int): Size of the vocabulary.\n",
    "    - embed_size (int): Dimension of the embedding vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Forward pass of the TokenEmbedding.\n",
    "        \n",
    "        Parameters:\n",
    "        - tokens (torch.Tensor): Tensor of token indices.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Scaled token embeddings.\n",
    "        \"\"\"\n",
    "        outputs = self.embedding(tokens.long())\n",
    "        outputs_scaled = outputs * math.sqrt(self.embed_size)\n",
    "        return outputs_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2069499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"\n",
    "    Generate a square mask for the sequence, where the mask indicates subsequent positions.\n",
    "    This mask is used to ensure that a position cannot attend to subsequent positions in the sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    - sz (int): Size of the sequence.\n",
    "    \n",
    "    Returns:\n",
    "    - torch.Tensor: Mask tensor of shape (sz, sz) with 0s in positions that can be attended to and negative infinity elsewhere.\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(source, target):\n",
    "    \"\"\"\n",
    "    Create masks for the source and target sequences.\n",
    "    \n",
    "    Parameters:\n",
    "    - source (torch.Tensor): Source sequence tensor.\n",
    "    - target (torch.Tensor): Target sequence tensor.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing source mask, target mask, source padding mask, and target padding mask.\n",
    "    \"\"\"\n",
    "    \n",
    "    source_seq_len = source.shape[0]\n",
    "    target_seq_len = target.shape[0]\n",
    "    batch_size = source.shape[1]\n",
    "    source_mask = torch.zeros((source_seq_len, source_seq_len), device=device).type(torch.bool)\n",
    "    target_mask = generate_square_subsequent_mask(target_seq_len)\n",
    "    source_padding_mask = (source == PAD_IDX).transpose(0, 1)\n",
    "    target_padding_mask = (target == PAD_IDX).transpose(0, 1)\n",
    "    return source_mask, target_mask, source_padding_mask, target_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa4862ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(eng.vocab)  # Source vocabulary size (English)\n",
    "TAR_VOCAB_SIZE = len(lang.vocab)  # Target vocabulary size (Other language, inferred from 'lang' variable)\n",
    "EMBEDDING_SIZE = 512  # Size of the embedding vector\n",
    "NHEAD = 8  # Number of heads in the multihead attention mechanism\n",
    "FFNN_DIM = 512  # Dimension of the feed-forward neural network inside transformer layers\n",
    "BATCH_SIZE = 32  # Size of each training batch\n",
    "NUM_ENCODER_LAYERS = 3  # Number of layers in the transformer encoder\n",
    "NUM_DECODER_LAYERS = 3  # Number of layers in the transformer decoder\n",
    "LEARNING_RATE = 0.0001  # Learning rate for the optimizer\n",
    "DROPOUT_RATE = 0.1  # Dropout rate for the dropout layer\n",
    "NUM_EPOCHS = 50  # Number of epochs for training\n",
    "PAD_IDX = eng.vocab.stoi[\"<pad>\"]  # Index for the padding token\n",
    "SOS_IDX = eng.vocab.stoi[\"<sos>\"] # index for start token\n",
    "EOD_IDX = eng.vocab.stoi[\"<eos>\"] # index for end token\n",
    "\n",
    "\n",
    "# Instantiate the TransformerMT model with the specified configuration\n",
    "model = TransformerMT(NHEAD,\n",
    "                      EMBEDDING_SIZE,\n",
    "                      SRC_VOCAB_SIZE, \n",
    "                      TAR_VOCAB_SIZE,\n",
    "                      NUM_ENCODER_LAYERS, \n",
    "                      NUM_DECODER_LAYERS, \n",
    "                      FFNN_DIM)\n",
    "\n",
    "# Initialize the model's parameters using the Xavier uniform initializer\n",
    "# This helps in achieving a better distribution of activations\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# Move the model to the specified device (either GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss criterion\n",
    "# CrossEntropyLoss is used since this is a classification task, and we ignore the loss computed on padding tokens\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Define the optimizer to be used for training\n",
    "# Adam optimizer is used with specific betas and epsilon values\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "285a31dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerMT(\n",
      "  (tf_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (tf_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_layer): Linear(in_features=512, out_features=50004, bias=True)\n",
      "  (src_token_embedding): TokenEmbedding(\n",
      "    (embedding): Embedding(48399, 512)\n",
      "  )\n",
      "  (tar_token_embedding): TokenEmbedding(\n",
      "    (embedding): Embedding(50004, 512)\n",
      "  )\n",
      "  (positional_embedding): SinusoidalEmbedding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57f01797",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_IDX = eng.vocab.stoi[\"<sos>\"]\n",
    "EOS_IDX = eng.vocab.stoi[\"<eos>\"]\n",
    "\n",
    "def get_tokens(model, source, source_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    Get token indices from a given source sequence using a trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (nn.Module): The trained transformer model.\n",
    "    - source (torch.Tensor): The source sequence tensor.\n",
    "    - source_mask (torch.Tensor): The mask for the source sequence.\n",
    "    - max_len (int): Maximum length of the target sequence.\n",
    "    - start_symbol (int): The starting symbol for the target sequence.\n",
    "    \n",
    "    Returns:\n",
    "    - result (torch.Tensor): The tensor containing token indices of the target sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move source and its mask to device\n",
    "    source_mask = source_mask.to(device)\n",
    "    source = source.to(device)\n",
    "    \n",
    "    # Encode the source sequence\n",
    "    memory = model.encode(source, source_mask)\n",
    "    \n",
    "    # Initialize result tensor with the start symbol\n",
    "    result = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "                \n",
    "    # Decode the memory tensor to get the target sequence\n",
    "    for index in range(max_len - 1):\n",
    "        memory = memory.to(device)\n",
    "        memory_mask = torch.zeros(result.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
    "        target_mask = (generate_square_subsequent_mask(result.size(0)).type(torch.bool)).to(device)\n",
    "        output = model.decode(result, memory, target_mask)\n",
    "        output = output.transpose(0, 1)\n",
    "        \n",
    "        # Get the next word's probability distribution and find the word with the maximum probability\n",
    "        probs = model.fc_layer(output[:, -1])\n",
    "        _, next_word = torch.max(probs, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        \n",
    "        # Add the next word to the result\n",
    "        result = torch.cat([result,torch.ones(1, 1).type_as(source.data).fill_(next_word)], dim=0)\n",
    "        \n",
    "        # Break if the next word is the start of sequence symbol\n",
    "        if next_word == SOS_IDX:\n",
    "            break\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "def translate(model, source, source_vocab, target_vocab):\n",
    "    \"\"\"\n",
    "    Translate a given source sequence to a target sequence using a trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (nn.Module): The trained transformer model.\n",
    "    - source (str or torch.Tensor): The source sequence (string or tensor).\n",
    "    - source_vocab (Vocab): Vocabulary object for the source language.\n",
    "    - target_vocab (Vocab): Vocabulary object for the target language.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The translated sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert source string to tensor, if it's a string\n",
    "    if type(source) == str:\n",
    "        tokens = [SOS_IDX] + [source_vocab.vocab.stoi[tok] for tok in source.split()] + [EOS_IDX]\n",
    "    else: \n",
    "        tokens = source\n",
    "    num_tokens = len(tokens)\n",
    "    source = (torch.LongTensor(tokens).reshape(num_tokens, 1))\n",
    "    source_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "\n",
    "    # Get target token indices from the source tensor\n",
    "    target_tokens = get_tokens(model, source, source_mask, max_len = num_tokens + 5, start_symbol = SOS_IDX).flatten()\n",
    "\n",
    "    # Convert target token indices to string\n",
    "    return \" \".join([target_vocab.vocab.itos[token] for token in target_tokens]).replace(\"<sos>\", \"\").replace(\"<eos>\", \"\").replace(\"<unk>\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26658941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Train the model using the training data.\n",
    "    \n",
    "    This function carries out a single epoch of training. For each batch of data in the training dataset:\n",
    "    - The model's gradients are zeroed.\n",
    "    - The data is passed through the model to get predictions.\n",
    "    - The loss between the predictions and actuals is computed.\n",
    "    - The gradients are computed via backpropagation.\n",
    "    - The model's parameters are updated.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to training mode and move it to the device\n",
    "    model.train().to(device)\n",
    "    losses = 0  # Accumulator for the total loss\n",
    "\n",
    "    # Iterate over each batch in the training data\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        train_loss = 0  # Accumulator for batch loss\n",
    "\n",
    "        # Extract source and target sequences from the batch and move them to the device\n",
    "        src = batch.english.to(device)\n",
    "        target = getattr(batch, l).to(device)\n",
    "        \n",
    "        # Exclude the last token for target input\n",
    "        target_input = target[:-1,:]\n",
    "\n",
    "        # Generate masks for the source and target sequences\n",
    "        source_mask, target_mask, src_padding_mask, tar_padding_mask = create_mask(src, target_input)\n",
    "        \n",
    "        # Pass data through the model\n",
    "        output = model(src, \n",
    "                       target_input, \n",
    "                       None, \n",
    "                       target_mask, \n",
    "                       src_padding_mask, \n",
    "                       tar_padding_mask, \n",
    "                       src_padding_mask)\n",
    "        \n",
    "        # Reset model gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Reshape output for loss computation\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        output_target = target[1:,:].reshape(-1)\n",
    "        \n",
    "        # Compute the loss and backpropagate\n",
    "        loss = criterion(output, output_target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to avoid exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 3)\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update loss accumulators\n",
    "        losses += loss.item()\n",
    "        train_loss += ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
    "        \n",
    "        # Print the loss for every 100 batches and also translate a sample sentence\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train loss at step {batch_idx}: {train_loss:.3f}')\n",
    "            print(translate(model, \"Football is a tough game\", eng, lang))\n",
    "    \n",
    "    # Return average training loss for the epoch\n",
    "    return losses/len(train_iterator)\n",
    "\n",
    "def validate():\n",
    "    \"\"\"\n",
    "    Validate the model using the validation data.\n",
    "    \n",
    "    This function computes the model's performance on the validation dataset. \n",
    "    No parameter updates are performed during this stage.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Average validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to evaluation mode and move it to the device\n",
    "    model.eval().to(device)\n",
    "    losses = 0  # Accumulator for the total loss\n",
    "\n",
    "    # Ensure no computation graph is built during validation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Iterate over each batch in the validation data\n",
    "        for batch_idx, batch in enumerate(val_iterator):\n",
    "            \n",
    "            # Extract source and target sequences from the batch and move them to the device\n",
    "            src = batch.english.to(device)\n",
    "            target = getattr(batch, l).to(device)\n",
    "            \n",
    "            # Exclude the last token for target input\n",
    "            target_input = target[:-1,:]\n",
    "\n",
    "            # Generate masks for the source and target sequences\n",
    "            source_mask, target_mask, src_padding_mask, tar_padding_mask = create_mask(src, target_input)\n",
    "            \n",
    "            # Pass data through the model\n",
    "            output = model(src, \n",
    "                           target_input, \n",
    "                           source_mask, \n",
    "                           target_mask, \n",
    "                           src_padding_mask, \n",
    "                           tar_padding_mask, \n",
    "                           src_padding_mask)\n",
    "            \n",
    "            # Reshape output for loss computation\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            output_target = target[1:,:].reshape(-1)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(output, output_target)\n",
    "            \n",
    "            # Update loss accumulator\n",
    "            losses += loss.item()\n",
    "\n",
    "    # Return average validation loss\n",
    "    return losses/len(val_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a751e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at step 0: 10.843\n",
      " অসাধরণ অসাধরণ অসাধরণ তুলেছে। তুলেছে। তুলেছে। তুলেছে। তুলেছে। তুলেছে। কোদুঙ্গালুরে ভীষণ\n",
      "Train loss at step 100: 0.080\n",
      "           \n",
      "Train loss at step 200: 0.038\n",
      "           \n",
      "Train loss at step 300: 0.026\n",
      "           \n",
      "Train loss at step 400: 0.020\n",
      "           \n",
      "Train loss at step 500: 0.016\n",
      "           \n",
      "Train loss at step 600: 0.013\n",
      "           \n",
      "Train loss at step 700: 0.011\n",
      "           \n",
      "Train loss at step 800: 0.009\n",
      " আমার কি কি কি       \n",
      "Train loss at step 900: 0.009\n",
      " কি কি কি        \n",
      "Train loss at step 1000: 0.008\n",
      " কি কি কি        \n",
      "Train loss at step 1100: 0.008\n",
      " আমার কি কি        \n",
      "Train loss at step 1200: 0.006\n",
      " কি কি         \n",
      "Train loss at step 1300: 0.006\n",
      " কি কি কি কি       \n",
      "Train loss at step 1400: 0.006\n",
      " কি কি কি        \n",
      "Train loss at step 1500: 0.005\n",
      " কি কি কি        \n",
      "Train loss at step 1600: 0.005\n",
      " আমার কি কি কি       \n",
      "Train loss at step 1700: 0.004\n",
      " কি কি কি        \n",
      "Train loss at step 1800: 0.004\n",
      " কি কি কি কি       \n",
      "Train loss at step 1900: 0.004\n",
      " কি কি কি কি       \n",
      "Train loss at step 2000: 0.004\n",
      " কি কি কি        \n",
      "Train loss at step 2100: 0.004\n",
      " আমার কি কি কি       \n",
      "Train loss at step 2200: 0.003\n",
      " কি কি         \n",
      "Train loss at step 2300: 0.003\n",
      " কি কি কি        \n",
      "Train loss at step 2400: 0.003\n",
      "           \n",
      "Train loss at step 2500: 0.003\n",
      " কি কি         \n",
      "Train loss at step 2600: 0.003\n",
      " কি কি কি        \n",
      "Train loss at step 2700: 0.003\n",
      " কি          \n",
      "Train loss at step 2800: 0.003\n",
      " কি কি কি        \n",
      "Train loss at step 2900: 0.002\n",
      " কি কি কি        \n",
      "Train loss at step 3000: 0.002\n",
      " কি কি কি        \n",
      "Train loss at step 3100: 0.002\n",
      " কি কি কি        \n",
      "Train loss at step 3200: 0.002\n",
      "           \n",
      "Train loss at step 3300: 0.002\n",
      " কি কি কি কি       \n",
      "Train loss at step 3400: 0.002\n",
      " কি কি কি        \n",
      "Train loss at step 3500: 0.002\n",
      " কি          \n",
      "Train loss at step 3600: 0.002\n",
      " কি কি         \n",
      "Train loss at step 3700: 0.002\n",
      " আমার কি কি কি       \n",
      "Train loss at step 3800: 0.002\n",
      "           \n",
      "Train loss at step 3900: 0.002\n",
      " কি কি কি        \n",
      "Train loss at step 4000: 0.002\n",
      "           \n",
      "Train loss at step 4100: 0.002\n",
      "           \n",
      "Train loss at step 4200: 0.002\n",
      "           \n",
      "Train loss at step 4300: 0.002\n",
      " কি কি         \n",
      "Train loss at step 4400: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4500: 0.002\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4600: 0.002\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4700: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4800: 0.002\n",
      " কি কি কি        \n",
      "Train loss at step 4900: 0.001\n",
      " কি কি কি কি       \n",
      "Train loss at step 5000: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5100: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5200: 0.001\n",
      " একটি একটি         \n",
      "Train loss at step 5300: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5400: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5500: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5600: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5700: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5800: 0.001\n",
      " কি একটি কি        \n",
      "Train loss at step 5900: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6000: 0.001\n",
      " একটি একটি         \n",
      "Train loss at step 6100: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6200: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6300: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6400: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6500: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 6600: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6700: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6800: 0.001\n",
      " একটি একটি         \n",
      "Epoch: 1/50\n",
      "\tTrain Loss: 7.214\n",
      "\tVal Loss: 6.316\n",
      "\tEpoch time: 159.200s\n",
      "Train loss at step 0: 6.603\n",
      " একটি একটি একটি        \n",
      "Train loss at step 100: 0.062\n",
      " একটি একটি একটি        \n",
      "Train loss at step 200: 0.032\n",
      " একটি একটি একটি        \n",
      "Train loss at step 300: 0.023\n",
      " একটি একটি একটি        \n",
      "Train loss at step 400: 0.018\n",
      " একটি একটি একটি        \n",
      "Train loss at step 500: 0.013\n",
      " একটি একটি একটি        \n",
      "Train loss at step 600: 0.008\n",
      " একটি একটি একটি        \n",
      "Train loss at step 700: 0.010\n",
      " একটি একটি একটি        \n",
      "Train loss at step 800: 0.008\n",
      " একটি একটি একটি        \n",
      "Train loss at step 900: 0.007\n",
      " একটি একটি         \n",
      "Train loss at step 1000: 0.007\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1100: 0.006\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1200: 0.006\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1300: 0.005\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1400: 0.005\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1500: 0.004\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1600: 0.004\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1700: 0.004\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1800: 0.004\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1900: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2000: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2100: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2200: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2300: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2400: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2500: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2600: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2700: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2800: 0.002\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 2900: 0.002\n",
      " একটি একটি         \n",
      "Train loss at step 3000: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3100: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3200: 0.002\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 3300: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3400: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3500: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3600: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3700: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3800: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3900: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4000: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4100: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4200: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4300: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4400: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4500: 0.001\n",
      " একটি একটি         \n",
      "Train loss at step 4600: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4700: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4800: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4900: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5000: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5100: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5200: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5300: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5400: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5500: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5600: 0.001\n",
      " একটি একটি         \n",
      "Train loss at step 5700: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5800: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5900: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6000: 0.001\n",
      " কি একটি         \n",
      "Train loss at step 6100: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6200: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6300: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6400: 0.001\n",
      " একটি একটি         \n",
      "Train loss at step 6500: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6600: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6700: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6800: 0.001\n",
      " একটি একটি একটি        \n",
      "Epoch: 2/50\n",
      "\tTrain Loss: 6.375\n",
      "\tVal Loss: 5.853\n",
      "\tEpoch time: 159.014s\n",
      "Train loss at step 0: 6.188\n",
      " একটি একটি একটি        \n",
      "Train loss at step 100: 0.056\n",
      " একটি একটি একটি        \n",
      "Train loss at step 200: 0.028\n",
      " একটি একটি একটি        \n",
      "Train loss at step 300: 0.021\n",
      " একটি একটি একটি        \n",
      "Train loss at step 400: 0.016\n",
      " কি একটি একটি        \n",
      "Train loss at step 500: 0.010\n",
      " একটি একটি একটি        \n",
      "Train loss at step 600: 0.010\n",
      " একটি একটি একটি        \n",
      "Train loss at step 700: 0.007\n",
      " একটি একটি         \n",
      "Train loss at step 800: 0.008\n",
      " একটি একটি একটি        \n",
      "Train loss at step 900: 0.006\n",
      " কি একটি একটি        \n",
      "Train loss at step 1000: 0.005\n",
      " একটি একটি একটি হয়       \n",
      "Train loss at step 1100: 0.006\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1200: 0.005\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1300: 0.004\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 1400: 0.004\n",
      " একটি একটি একটি        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at step 1500: 0.004\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 1600: 0.004\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1700: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1800: 0.003\n",
      " একটি একটি একটি আছে       \n",
      "Train loss at step 1900: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2000: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2100: 0.003\n",
      " একটি একটি         \n",
      "Train loss at step 2200: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2300: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2400: 0.003\n",
      " কি একটি একটি        \n",
      "Train loss at step 2500: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2600: 0.003\n",
      " একটি একটি         \n",
      "Train loss at step 2700: 0.002\n",
      " একটি একটি একটি হয়       \n",
      "Train loss at step 2800: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2900: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3000: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3100: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3200: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3300: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3400: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3500: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3600: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3700: 0.002\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 3800: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3900: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4000: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4100: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4200: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4300: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4400: 0.001\n",
      " একটি একটি একটি হয়       \n",
      "Train loss at step 4500: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4600: 0.001\n",
      " একটি একটি খেলা একটি       \n",
      "Train loss at step 4700: 0.001\n",
      " কি একটি কি        \n",
      "Train loss at step 4800: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4900: 0.001\n",
      " একটি একটি একটি খেলা       \n",
      "Train loss at step 5000: 0.001\n",
      " একটি একটি একটি হয়       \n",
      "Train loss at step 5100: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 5200: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5300: 0.001\n",
      " একটি একটি খেলা        \n",
      "Train loss at step 5400: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5500: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 5600: 0.001\n",
      " একটি একটি খেলা একটি       \n",
      "Train loss at step 5700: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5800: 0.001\n",
      " কি একটি         \n",
      "Train loss at step 5900: 0.001\n",
      " একটি একটি খেলা        \n",
      "Train loss at step 6000: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 6100: 0.001\n",
      " কি একটি কি        \n",
      "Train loss at step 6200: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 6300: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 6400: 0.001\n",
      " কি একটি         \n",
      "Train loss at step 6500: 0.001\n",
      " একটি একটি একটি খেলা একটি      \n",
      "Train loss at step 6600: 0.001\n",
      " একটি একটি খেলা        \n",
      "Train loss at step 6700: 0.001\n",
      " কি একটি খেলা একটি       \n",
      "Train loss at step 6800: 0.001\n",
      " কি একটি একটি        \n",
      "Epoch: 3/50\n",
      "\tTrain Loss: 5.846\n",
      "\tVal Loss: 5.533\n",
      "\tEpoch time: 159.222s\n",
      "Train loss at step 0: 6.090\n",
      " কি একটি একটি        \n",
      "Train loss at step 100: 0.061\n",
      " কি একটি একটি        \n",
      "Train loss at step 200: 0.021\n",
      " একটি একটি একটি খেলা       \n",
      "Train loss at step 300: 0.019\n",
      " কি একটি একটি        \n",
      "Train loss at step 400: 0.014\n",
      " একটি একটি একটি        \n",
      "Train loss at step 500: 0.011\n",
      " কি একটি একটি        \n",
      "Train loss at step 600: 0.009\n",
      " কি একটি একটি        \n",
      "Train loss at step 700: 0.008\n",
      " কি একটি একটি        \n",
      "Train loss at step 800: 0.007\n",
      " একটি একটি একটি        \n",
      "Train loss at step 900: 0.007\n",
      " কি একটি একটি        \n",
      "Train loss at step 1000: 0.005\n",
      " কি একটি একটি        \n",
      "Train loss at step 1100: 0.004\n",
      " কি একটি একটি        \n",
      "Train loss at step 1200: 0.004\n",
      " কি একটি একটি        \n",
      "Train loss at step 1300: 0.003\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 1400: 0.004\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 1500: 0.003\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 1600: 0.004\n",
      "  একটি একটি একটি       \n",
      "Train loss at step 1700: 0.004\n",
      "  একটি একটি একটি       \n",
      "Train loss at step 1800: 0.003\n",
      " কি একটি একটি        \n",
      "Train loss at step 1900: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2000: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2100: 0.003\n",
      " কি একটি একটি        \n",
      "Train loss at step 2200: 0.003\n",
      " কি একটি একটি        \n",
      "Train loss at step 2300: 0.002\n",
      " কি একটি একটি        \n",
      "Train loss at step 2400: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2500: 0.002\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 2600: 0.002\n",
      " কি একটি একটি        \n",
      "Train loss at step 2700: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2800: 0.002\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 2900: 0.002\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 3000: 0.002\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 3100: 0.002\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 3200: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3300: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3400: 0.002\n",
      " একটি একটি খেলা একটি       \n",
      "Train loss at step 3500: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3600: 0.002\n",
      " একটি একটি খেলা        \n",
      "Train loss at step 3700: 0.001\n",
      " একটি একটি খেলা        \n",
      "Train loss at step 3800: 0.001\n",
      " একটি একটি একটি খেলা       \n",
      "Train loss at step 3900: 0.001\n",
      " একটি খেলা একটি খেলা একটি      \n",
      "Train loss at step 4000: 0.001\n",
      " একটি খেলা একটি খেলা       \n",
      "Train loss at step 4100: 0.001\n",
      " একটি একটি একটি খেলা       \n",
      "Train loss at step 4200: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4300: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4400: 0.001\n",
      " একটি খেলা একটি খেলা       \n",
      "Train loss at step 4500: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 4600: 0.001\n",
      "  একটি খেলা একটি       \n",
      "Train loss at step 4700: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4800: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4900: 0.001\n",
      " কি একটি একটি খেলা       \n",
      "Train loss at step 5000: 0.001\n",
      " এটি একটি খেলা একটি       \n",
      "Train loss at step 5100: 0.001\n",
      " কি একটি একটি কি       \n",
      "Train loss at step 5200: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 5300: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5400: 0.001\n",
      " একটি একটি একটি একটি থাকে      \n",
      "Train loss at step 5500: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 5600: 0.001\n",
      " একটি একটি খেলা একটি       \n",
      "Train loss at step 5700: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5800: 0.001\n",
      " খেলা একটি খেলা        \n",
      "Train loss at step 5900: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 6000: 0.001\n",
      " কি একটি খেলা খেলা       \n",
      "Train loss at step 6100: 0.001\n",
      " কি একটি খেলা একটি       \n",
      "Train loss at step 6200: 0.001\n",
      " একটি একটি খেলা        \n",
      "Train loss at step 6300: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6400: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6500: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 6600: 0.001\n",
      " একটি একটি         \n",
      "Train loss at step 6700: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 6800: 0.001\n",
      " কি একটি একটি        \n",
      "Epoch: 4/50\n",
      "\tTrain Loss: 5.384\n",
      "\tVal Loss: 5.424\n",
      "\tEpoch time: 156.238s\n",
      "Train loss at step 0: 5.338\n",
      "  একটি খেলা        \n",
      "Train loss at step 100: 0.051\n",
      "  একটি একটি        \n",
      "Train loss at step 200: 0.022\n",
      "  একটি একটি        \n",
      "Train loss at step 300: 0.017\n",
      " খেলা একটি খেলা        \n",
      "Train loss at step 400: 0.014\n",
      " কি একটি একটি খেলা       \n",
      "Train loss at step 500: 0.009\n",
      " কি একটি একটি খেলা       \n",
      "Train loss at step 600: 0.009\n",
      " কি একটি খেলা        \n",
      "Train loss at step 700: 0.006\n",
      " খুব একটি খেলা        \n",
      "Train loss at step 800: 0.006\n",
      "  একটি একটি        \n",
      "Train loss at step 900: 0.006\n",
      " কি একটি একটি        \n",
      "Train loss at step 1000: 0.006\n",
      " কি একটি এটি        \n",
      "Train loss at step 1100: 0.005\n",
      " কি একটি কি হল       \n",
      "Train loss at step 1200: 0.004\n",
      " কি একটি একটি        \n",
      "Train loss at step 1300: 0.004\n",
      " কি একটি কি কি       \n",
      "Train loss at step 1400: 0.004\n",
      " কি একটি একটি খেলা       \n",
      "Train loss at step 1500: 0.003\n",
      "  একটি একটি খেলা       \n",
      "Train loss at step 1600: 0.004\n",
      "  একটি একটি        \n",
      "Train loss at step 1700: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1800: 0.002\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 1900: 0.002\n",
      " কি একটি একটি        \n",
      "Train loss at step 2000: 0.003\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2100: 0.003\n",
      " একটি খেলা একটি        \n",
      "Train loss at step 2200: 0.002\n",
      "  খেলা খেলা        \n",
      "Train loss at step 2300: 0.002\n",
      " খেলা খেলা খেলা        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at step 2400: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2500: 0.002\n",
      " কি একটি একটি        \n",
      "Train loss at step 2600: 0.002\n",
      " খেলা একটি একটি খুব       \n",
      "Train loss at step 2700: 0.002\n",
      "  একটি একটি        \n",
      "Train loss at step 2800: 0.002\n",
      " কি একটি একটি        \n",
      "Train loss at step 2900: 0.002\n",
      " কি একটি একটি        \n",
      "Train loss at step 3000: 0.002\n",
      " কি একটি একটি        \n",
      "Train loss at step 3100: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 3200: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 3300: 0.001\n",
      " একটি একটি খেলা        \n",
      "Train loss at step 3400: 0.001\n",
      " কি একটি খেলা        \n",
      "Train loss at step 3500: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3600: 0.001\n",
      " একটি একটি খেলা        \n",
      "Train loss at step 3700: 0.001\n",
      " একটা একটি একটি        \n",
      "Train loss at step 3800: 0.001\n",
      " কি একটি খেলা        \n",
      "Train loss at step 3900: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 4000: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4100: 0.001\n",
      " একটি খেলা একটি        \n",
      "Train loss at step 4200: 0.001\n",
      " একটি খেলা একটি        \n",
      "Train loss at step 4300: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 4400: 0.001\n",
      " একটি একটি একটি খেলা       \n",
      "Train loss at step 4500: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4600: 0.001\n",
      " একটা একটি একটি        \n",
      "Train loss at step 4700: 0.001\n",
      " একটা একটি একটি        \n",
      "Train loss at step 4800: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 4900: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 5000: 0.001\n",
      " খেলা একটি খেলা        \n",
      "Train loss at step 5100: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 5200: 0.001\n",
      " একটি খেলা একটি        \n",
      "Train loss at step 5300: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 5400: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5500: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5600: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5700: 0.001\n",
      " একটা একটি একটি একটি       \n",
      "Train loss at step 5800: 0.001\n",
      " একটা একটি একটি        \n",
      "Train loss at step 5900: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 6000: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 6100: 0.001\n",
      " একটি একটি একটি খেলা       \n",
      "Train loss at step 6200: 0.001\n",
      " কি একটি একটি        \n",
      "Train loss at step 6300: 0.001\n",
      " একটা একটি খেলা খেলা       \n",
      "Train loss at step 6400: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 6500: 0.001\n",
      "  একটি একটি খেলা       \n",
      "Train loss at step 6600: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 6700: 0.001\n",
      " কি একটি একটি কি       \n",
      "Train loss at step 6800: 0.001\n",
      " কি একটি একটি        \n",
      "Epoch: 5/50\n",
      "\tTrain Loss: 4.963\n",
      "\tVal Loss: 5.248\n",
      "\tEpoch time: 156.240s\n",
      "Train loss at step 0: 5.576\n",
      " একটি খেলা একটি        \n",
      "Train loss at step 100: 0.043\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 200: 0.022\n",
      " একটি একটি একটি একটি       একটি\n",
      "Train loss at step 300: 0.015\n",
      "  একটি একটি খেলা       \n",
      "Train loss at step 400: 0.012\n",
      "  একটি একটি খেলা       \n",
      "Train loss at step 500: 0.011\n",
      " খেলা একটি একটি        \n",
      "Train loss at step 600: 0.008\n",
      "  একটি একটি একটি       \n",
      "Train loss at step 700: 0.007\n",
      " খুব একটি খেলা খুব       \n",
      "Train loss at step 800: 0.005\n",
      " খুব একটি একটি        \n",
      "Train loss at step 900: 0.005\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1000: 0.005\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 1100: 0.004\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 1200: 0.003\n",
      " কঠিন একটা একটা        \n",
      "Train loss at step 1300: 0.004\n",
      "  একটি একটি খেলা       \n",
      "Train loss at step 1400: 0.003\n",
      "  একটি একটি        \n",
      "Train loss at step 1500: 0.003\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 1600: 0.003\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 1700: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1800: 0.002\n",
      " একটি একটি একটি খেলা       \n",
      "Train loss at step 1900: 0.003\n",
      " একটি একটি খেলা একটি       \n",
      "Train loss at step 2000: 0.003\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 2100: 0.003\n",
      "  একটি একটি        \n",
      "Train loss at step 2200: 0.002\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 2300: 0.002\n",
      " খুব একটি একটি        \n",
      "Train loss at step 2400: 0.002\n",
      " কি একটি একটি কি       \n",
      "Train loss at step 2500: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 2600: 0.001\n",
      " এটি একটি একটি        \n",
      "Train loss at step 2700: 0.002\n",
      " খুব একটি একটি        \n",
      "Train loss at step 2800: 0.002\n",
      " এটি একটি একটি        \n",
      "Train loss at step 2900: 0.002\n",
      "  একটি একটি        \n",
      "Train loss at step 3000: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3100: 0.002\n",
      " খুব একটি একটি        \n",
      "Train loss at step 3200: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 3300: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 3400: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 3500: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 3600: 0.002\n",
      "  একটি একটি        \n",
      "Train loss at step 3700: 0.001\n",
      "  একটি একটি একটি       \n",
      "Train loss at step 3800: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 3900: 0.001\n",
      " কঠিন একটি একটি        \n",
      "Train loss at step 4000: 0.001\n",
      "  খেলা খেলা        \n",
      "Train loss at step 4100: 0.001\n",
      " খুব একটি একটি        \n",
      "Train loss at step 4200: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4300: 0.001\n",
      "  একটি একটি একটি       \n",
      "Train loss at step 4400: 0.001\n",
      " একটা একটা একটা        \n",
      "Train loss at step 4500: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4600: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4700: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 4800: 0.001\n",
      " কি একটি একটি কি       \n",
      "Train loss at step 4900: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 5000: 0.001\n",
      "  একটি একটি একটি       \n",
      "Train loss at step 5100: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 5200: 0.001\n",
      " এটি একটি         \n",
      "Train loss at step 5300: 0.001\n",
      "  একটি খেলা খেলা       \n",
      "Train loss at step 5400: 0.001\n",
      "  একটি খেলা একটি       \n",
      "Train loss at step 5500: 0.001\n",
      " এটি একটি খেলা খেলা       \n",
      "Train loss at step 5600: 0.001\n",
      " খুব একটি একটি খুব       \n",
      "Train loss at step 5700: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 5800: 0.001\n",
      " দীর্ঘ খেলা একটি খেলা       \n",
      "Train loss at step 5900: 0.001\n",
      " একটি একটি একটি খেলা একটি      \n",
      "Train loss at step 6000: 0.001\n",
      " একটা একটি একটি একটি       \n",
      "Train loss at step 6100: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 6200: 0.001\n",
      " একটা একটা একটা        \n",
      "Train loss at step 6300: 0.001\n",
      " একটা একটি একটি        \n",
      "Train loss at step 6400: 0.001\n",
      " একটা একটা খেলা একটি       \n",
      "Train loss at step 6500: 0.001\n",
      " একটা খেলা একটি        \n",
      "Train loss at step 6600: 0.001\n",
      " একটা খেলা একটি        \n",
      "Train loss at step 6700: 0.001\n",
      " খুব একটি খেলা খেলা       \n",
      "Train loss at step 6800: 0.001\n",
      " একটি খেলা একটি খেলা       \n",
      "Epoch: 6/50\n",
      "\tTrain Loss: 4.580\n",
      "\tVal Loss: 5.247\n",
      "\tEpoch time: 161.970s\n",
      "Train loss at step 0: 5.081\n",
      " একটি একটি খেলা খেলা খেলা      \n",
      "Train loss at step 100: 0.039\n",
      " একটা খেলা একটি        \n",
      "Train loss at step 200: 0.022\n",
      " একটা খেলা একটি খেলা       \n",
      "Train loss at step 300: 0.011\n",
      " কি একটি একটি একটি       \n",
      "Train loss at step 400: 0.011\n",
      "  একটি একটি একটি       \n",
      "Train loss at step 500: 0.008\n",
      " একটা একটি একটি একটি       \n",
      "Train loss at step 600: 0.007\n",
      " একটা একটি খেলা খেলা       \n",
      "Train loss at step 700: 0.007\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 800: 0.006\n",
      " খুব একটি খুবই খেলা       \n",
      "Train loss at step 900: 0.004\n",
      "  একটি খুব        \n",
      "Train loss at step 1000: 0.005\n",
      "  একটি একটি খুব       \n",
      "Train loss at step 1100: 0.002\n",
      " একটা খেলা একটি        \n",
      "Train loss at step 1200: 0.004\n",
      " কি একটি একটি খুব       \n",
      "Train loss at step 1300: 0.004\n",
      "  একটি একটি সহজ       \n",
      "Train loss at step 1400: 0.003\n",
      " একটা একটি একটি একটি       \n",
      "Train loss at step 1500: 0.002\n",
      " খেলা খেলা খেলা খেলা       \n",
      "Train loss at step 1600: 0.002\n",
      " খেলা খেলা খেলা        \n",
      "Train loss at step 1700: 0.003\n",
      " হল হল হল        \n",
      "Train loss at step 1800: 0.003\n",
      " হল একটি খেলা খেলা       \n",
      "Train loss at step 1900: 0.002\n",
      " একটি খেলা একটি        \n",
      "Train loss at step 2000: 0.002\n",
      " একটা একটা খেলা খেলা       \n",
      "Train loss at step 2100: 0.002\n",
      " একটা একটা একটা        \n",
      "Train loss at step 2200: 0.002\n",
      " একটা একটা একটা        \n",
      "Train loss at step 2300: 0.002\n",
      " একটা একটা একটি        \n",
      "Train loss at step 2400: 0.002\n",
      " একটা একটা একটা        \n",
      "Train loss at step 2500: 0.002\n",
      " একটা একটি একটি খেলা       \n",
      "Train loss at step 2600: 0.002\n",
      " একটা একটা একটি        \n",
      "Train loss at step 2700: 0.001\n",
      " একটা একটি একটি        \n",
      "Train loss at step 2800: 0.001\n",
      " একটা একটা খেলা খেলা       \n",
      "Train loss at step 2900: 0.002\n",
      " একটা একটি একটি একটি       \n",
      "Train loss at step 3000: 0.001\n",
      " একটা একটা একটা একটা       \n",
      "Train loss at step 3100: 0.001\n",
      " একটা একটি একটি        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at step 3200: 0.001\n",
      " একটা একটা একটা        \n",
      "Train loss at step 3300: 0.001\n",
      " একটা একটা একটা খেলা খেলা      \n",
      "Train loss at step 3400: 0.001\n",
      " একটা একটা খেলা খেলা       \n",
      "Train loss at step 3500: 0.001\n",
      " একটা একটা একটা খেলা       \n",
      "Train loss at step 3600: 0.001\n",
      " একটা একটা একটা খেলা খেলা      \n",
      "Train loss at step 3700: 0.001\n",
      " একটা একটা একটা খেলা খেলা      \n",
      "Train loss at step 3800: 0.001\n",
      "  একটি একটি একটি       \n",
      "Train loss at step 3900: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 4000: 0.001\n",
      " এটি একটি একটি        \n",
      "Train loss at step 4100: 0.001\n",
      " একটি একটি একটি এটি       \n",
      "Train loss at step 4200: 0.001\n",
      " একটি একটি একটি এটি       \n",
      "Train loss at step 4300: 0.001\n",
      " এটি একটি একটি        \n",
      "Train loss at step 4400: 0.001\n",
      " এটি একটি একটি        \n",
      "Train loss at step 4500: 0.001\n",
      " কঠিন একটা একটা        \n",
      "Train loss at step 4600: 0.001\n",
      " খুব একটা একটা        \n",
      "Train loss at step 4700: 0.001\n",
      " কঠিন একটা একটা খেলা খেলা      \n",
      "Train loss at step 4800: 0.001\n",
      " এটি একটি এটি এটি       \n",
      "Train loss at step 4900: 0.001\n",
      " এটি একটি খুব        \n",
      "Train loss at step 5000: 0.001\n",
      " রক্ত খুব একটি        \n",
      "Train loss at step 5100: 0.001\n",
      " একটা একটা একটা        \n",
      "Train loss at step 5200: 0.001\n",
      " কঠিন একটা একটা        \n",
      "Train loss at step 5300: 0.001\n",
      " কঠিন দীর্ঘ খেলা খেলা       \n",
      "Train loss at step 5400: 0.001\n",
      " কঠিন খেলা খেলা        \n",
      "Train loss at step 5500: 0.001\n",
      " এটি একটি একটি        \n",
      "Train loss at step 5600: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 5700: 0.001\n",
      " একটা একটা খেলা খেলা খেলা      \n",
      "Train loss at step 5800: 0.001\n",
      " কঠিন একটা একটা খেলা খেলা      \n",
      "Train loss at step 5900: 0.000\n",
      " একটা একটি একটি        \n",
      "Train loss at step 6000: 0.001\n",
      " একটা খেলা খেলা খেলা       \n",
      "Train loss at step 6100: 0.001\n",
      " একটা একটি একটি তা       \n",
      "Train loss at step 6200: 0.001\n",
      " একটা একটি একটি একটি       \n",
      "Train loss at step 6300: 0.001\n",
      " এটি একটি একটি        \n",
      "Train loss at step 6400: 0.001\n",
      " একটা খেলা খেলা খেলা       \n",
      "Train loss at step 6500: 0.001\n",
      " একটা একটি একটি        \n",
      "Train loss at step 6600: 0.001\n",
      "  একটি         \n",
      "Train loss at step 6700: 0.001\n",
      " এটি একটি  খেলা খেলা      \n",
      "Train loss at step 6800: 0.001\n",
      " এটি একটি একটি        \n",
      "Epoch: 7/50\n",
      "\tTrain Loss: 4.246\n",
      "\tVal Loss: 5.395\n",
      "\tEpoch time: 161.585s\n",
      "Train loss at step 0: 5.118\n",
      " এটি একটি খেলা খেলা       \n",
      "Train loss at step 100: 0.040\n",
      " এটি একটি একটি        \n",
      "Train loss at step 200: 0.016\n",
      " এটি একটি একটি        \n",
      "Train loss at step 300: 0.015\n",
      " এটি একটি একটি এটি       \n",
      "Train loss at step 400: 0.006\n",
      " এটি একটি একটি        \n",
      "Train loss at step 500: 0.007\n",
      "  একটি একটি        \n",
      "Train loss at step 600: 0.006\n",
      "  একটি করে একটি       \n",
      "Train loss at step 700: 0.005\n",
      "  একটি         \n",
      "Train loss at step 800: 0.005\n",
      " এটা একটা         \n",
      "Train loss at step 900: 0.005\n",
      " একটা খেলা খেলা খেলা       \n",
      "Train loss at step 1000: 0.004\n",
      " কঠিন একটা খেলা খেলা       \n",
      "Train loss at step 1100: 0.004\n",
      "  একটি একটি        \n",
      "Train loss at step 1200: 0.003\n",
      "  একটি একটি        \n",
      "Train loss at step 1300: 0.003\n",
      " একটা খেলা খেলা খেলা       \n",
      "Train loss at step 1400: 0.003\n",
      " একটা খেলা খেলা খেলা       \n",
      "Train loss at step 1500: 0.003\n",
      "  একটি একটি        \n",
      "Train loss at step 1600: 0.003\n",
      "  একটি একটি ও       \n",
      "Train loss at step 1700: 0.002\n",
      "  একটি একটি        \n",
      "Train loss at step 1800: 0.002\n",
      " একটি একটি একটি        \n",
      "Train loss at step 1900: 0.002\n",
      " একটা একটি কি একটা       \n",
      "Train loss at step 2000: 0.002\n",
      " একটা একটা খেলা        \n",
      "Train loss at step 2100: 0.002\n",
      " একটা একটা খেলা খুব       \n",
      "Train loss at step 2200: 0.002\n",
      " একটা একটা খেলা খেলা       \n",
      "Train loss at step 2300: 0.001\n",
      " একটা খেলা একটি        \n",
      "Train loss at step 2400: 0.001\n",
      " একটি একটি একটি একটি       \n",
      "Train loss at step 2500: 0.002\n",
      " একটা একটি একটি        \n",
      "Train loss at step 2600: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2700: 0.001\n",
      " একটি একটি একটি        \n",
      "Train loss at step 2800: 0.002\n",
      " একটি একটি         \n",
      "Train loss at step 2900: 0.001\n",
      " একটি  খেলা খেলা       \n",
      "Train loss at step 3000: 0.001\n",
      " একটা খেলা একটি        \n",
      "Train loss at step 3100: 0.001\n",
      " হল হল হল        \n",
      "Train loss at step 3200: 0.001\n",
      " একটা একটি একটি        \n",
      "Train loss at step 3300: 0.001\n",
      " এটি একটি একটি        \n",
      "Train loss at step 3400: 0.001\n",
      " একটা একটি একটি তা       \n",
      "Train loss at step 3500: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 3600: 0.001\n",
      "  একটি খেলা খেলা       \n",
      "Train loss at step 3700: 0.001\n",
      " একটা খেলা খেলা খেলা       \n",
      "Train loss at step 3800: 0.001\n",
      "  একটি খেলা খেলা       \n",
      "Train loss at step 3900: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 4000: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 4100: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 4200: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 4300: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 4400: 0.001\n",
      " একটা একটি একটি        \n",
      "Train loss at step 4500: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 4600: 0.001\n",
      " একটা একটা খেলা খেলা       \n",
      "Train loss at step 4700: 0.001\n",
      " কঠিন একটা খেলা খেলা       \n",
      "Train loss at step 4800: 0.001\n",
      " একটা একটা খেলা খেলা খেলা      \n",
      "Train loss at step 4900: 0.001\n",
      " একটা খেলা খেলা খেলা       \n",
      "Train loss at step 5000: 0.001\n",
      " একটা একটা খেলা খেলা       \n",
      "Train loss at step 5100: 0.001\n",
      "  একটি একটি        \n",
      "Train loss at step 5200: 0.001\n",
      " কঠিন একটা খেলা খেলা       \n",
      "Train loss at step 5300: 0.001\n",
      " একটা একটা একটা        \n",
      "Train loss at step 5400: 0.001\n",
      "  কঠিন         \n",
      "Train loss at step 5500: 0.001\n",
      "  একটা         \n",
      "Train loss at step 5600: 0.001\n",
      " একটা একটা খেলা খেলা       \n",
      "Train loss at step 5700: 0.001\n",
      " রক্ত একটা         \n",
      "Train loss at step 5800: 0.001\n",
      " কঠিন একটা         \n",
      "Train loss at step 5900: 0.001\n",
      " খেলা একটা         \n",
      "Train loss at step 6000: 0.001\n",
      " একটা একটা খেলা খেলা       \n",
      "Train loss at step 6100: 0.001\n",
      " একটা খেলা খেলা খেলা       \n",
      "Train loss at step 6200: 0.001\n",
      " কঠিন একটা খেলা খেলা       \n",
      "Train loss at step 6300: 0.001\n",
      " কঠিন একটা         \n",
      "Train loss at step 6400: 0.001\n",
      " কঠিন একটা খেলা খেলা       \n",
      "Train loss at step 6500: 0.001\n",
      " একটা একটা একটি এটি       \n",
      "Train loss at step 6600: 0.001\n",
      " খেলা একটি একটি        \n",
      "Train loss at step 6700: 0.001\n",
      " এটি একটি একটি        \n",
      "Train loss at step 6800: 0.001\n",
      " খুবই একটা একটি        \n",
      "Epoch: 8/50\n",
      "\tTrain Loss: 3.956\n",
      "\tVal Loss: 5.492\n",
      "\tEpoch time: 158.005s\n",
      "Stopping training due to early stopping criteria!\n"
     ]
    }
   ],
   "source": [
    "def train_and_validate(num_epochs, patience):\n",
    "    \"\"\"\n",
    "    Trains and validates the model over a specified number of epochs.\n",
    "    Implements early stopping based on the validation loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_epochs (int): Maximum number of epochs to train.\n",
    "    - patience (int): Number of epochs to wait without improvement before stopping training.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of training losses over the epochs.\n",
    "    - list: A list of validation losses over the epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the best validation loss to a high value\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0  # Counter for epochs without validation loss improvement\n",
    "    train_losses, val_losses = [], []  # Lists to store training and validation losses\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss = train()\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss = validate()\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # Print training and validation statistics for the epoch\n",
    "        print(f'Epoch: {epoch}/{num_epochs}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "        print(f'\\tVal Loss: {val_loss:.3f}')\n",
    "        print(f'\\tEpoch time: {elapsed_time:.3f}s')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            # Save the current state of the model if validation loss is the best seen so far\n",
    "            torch.save(model.state_dict(), \"best_model_{}.pt\".format(l))\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            # Stop training if validation loss hasn't improved for a number of epochs specified by 'patience'\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"Stopping training due to early stopping criteria!\")\n",
    "                break\n",
    "\n",
    "    # Load the best model weights after all epochs are completed or early stopping is triggered\n",
    "    model.load_state_dict(torch.load(\"best_model_{}.pt\".format(l)))\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train and validate the model for a specified number of epochs with early stopping criteria\n",
    "train_losses, val_losses = train_and_validate(NUM_EPOCHS, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51615e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB4R0lEQVR4nO3dd3yN5//H8dfJJIg9QkKoFZLY1IhYsRWx96zWTpWqLqsoLapLS5VWqWpstWLFrhlCldp7VgUxIjm/P85Pvg1BQpL75OT9fDzOo8597nOfTy6pvHNd131dJrPZbEZERETERtgZXYCIiIhIUlK4EREREZuicCMiIiI2ReFGREREbIrCjYiIiNgUhRsRERGxKQo3IiIiYlMUbkRERMSmKNyIiIiITVG4EREREZuicCMiT5g1axYmk4ndu3cbXYqISKIp3IiIiIhNUbgREXmKmJgY7t27Z3QZIpJICjci8sL27dtHgwYNcHV1JWPGjNSuXZsdO3bEOScqKoqRI0dSpEgR0qVLR/bs2alWrRohISGx51y6dIlu3brh7u6Os7Mzbm5uNG3alFOnTj23hr/++ovWrVuTM2dO0qdPT7FixXj//fdjX+/atSuenp5PvG/EiBGYTKY4x0wmE/369WPOnDmULFkSZ2dnli1bRrZs2ejWrdsT14iIiCBdunQMHjw49tj9+/cZPnw4hQsXxtnZGQ8PD9555x3u378f570hISFUq1aNLFmykDFjRooVK8Z777333K9XRJ7PwegCRCR1OnToEH5+fri6uvLOO+/g6OjId999R40aNQgNDaVSpUqAJUSMGzeOnj17UrFiRSIiIti9ezd79+4lICAAgBYtWnDo0CH69++Pp6cnV65cISQkhDNnzsQbTB45cOAAfn5+ODo60qtXLzw9PTl+/DjLli1jzJgxL/R1rV+/nvnz59OvXz9y5MhBkSJFaN68OQsXLuS7777Dyckp9tzFixdz//592rZtC1h6el577TW2bNlCr1698PLyIjw8nMmTJ3P06FEWL14c23aNGzfG19eXUaNG4ezszLFjx9i6desL1SwijzGLiDxm5syZZsC8a9eup57TrFkzs5OTk/n48eOxxy5cuGDOlCmTuXr16rHHSpUqZW7UqNFTr3Pjxg0zYP70008TXWf16tXNmTJlMp8+fTrO8ZiYmNg/d+nSxVygQIEn3jt8+HDz4/8EAmY7OzvzoUOH4hxfvXq1GTAvW7YszvGGDRuaCxUqFPt89uzZZjs7O/PmzZvjnPftt9+aAfPWrVvNZrPZPHnyZDNgvnr1asK/WBFJMA1LiUiiRUdHs2bNGpo1a0ahQoVij7u5udG+fXu2bNlCREQEAFmyZOHQoUP8/fff8V4rffr0ODk5sXHjRm7cuJHgGq5evcqmTZvo3r07+fPnj/Pa48NNieHv70+JEiXiHKtVqxY5cuTg119/jT1248YNQkJCaNOmTeyx3377DS8vL4oXL861a9diH7Vq1QJgw4YNgKVNAJYsWUJMTMwL1yoi8VO4EZFEu3r1KpGRkRQrVuyJ17y8vIiJieHs2bMAjBo1in///ZeiRYvi4+PDkCFDOHDgQOz5zs7OjB8/npUrV5I7d26qV6/OhAkTuHTp0jNrOHHiBADe3t5J+JVBwYIFnzjm4OBAixYtWLJkSezcmYULFxIVFRUn3Pz9998cOnSInDlzxnkULVoUgCtXrgDQpk0bqlatSs+ePcmdOzdt27Zl/vz5CjoiSUThRkSSVfXq1Tl+/Dg//PAD3t7efP/995QtW5bvv/8+9pygoCCOHj3KuHHjSJcuHR9++CFeXl7s27fvpT//ab040dHR8R5Pnz59vMfbtm3LrVu3WLlyJQDz58+nePHilCpVKvacmJgYfHx8CAkJiffRp0+f2M/YtGkTa9eupVOnThw4cIA2bdoQEBDw1LpEJBGMHhcTEevzvDk3Dx8+NLu4uJhbt279xGtvvvmm2c7Oznzz5s1433vr1i1zmTJlzPny5Xvq5x89etTs4uJi7tChw1PPuXLlihkwDxw48Jlfy1tvvWXOnDnzE8c7deoU75ybvn37xnud6Ohos5ubm7lt27bmq1evmh0cHMzDhw+Pc07Dhg3N+fLlizPnJ6HGjBljBswhISGJfq+IxKWeGxFJNHt7e+rWrcuSJUvi3K59+fJl5s6dS7Vq1XB1dQXg+vXrcd6bMWNGChcuHDu8ExkZ+cRaMq+88gqZMmV64vbp/8qZMyfVq1fnhx9+4MyZM3FeM5vNca518+bNOENhFy9eZNGiRYn6mu3s7GjZsiXLli1j9uzZPHz4MM6QFEDr1q05f/4806dPf+L9d+/e5c6dOwD8888/T7xeunRpgGd+zSKSMCbzf/8VEBHBsv1Ct27d6N27N3nz5n3i9YEDB3LmzBkqVapElixZ6NOnDw4ODnz33XecP38+zq3guXPnpkaNGpQrV45s2bKxe/dupk2bRr9+/fjiiy8ICwujdu3atG7dmhIlSuDg4MCiRYsICQkhODiYFi1aPLXO/fv3U61aNZydnenVqxcFCxbk1KlT/P7774SFhQGWcFWgQAFy587NgAEDiIyMZOrUqeTMmZO9e/fGCUImk4m+ffvy1Vdfxft5W7dupVq1amTKlAlPT884gQksw1JNmjRh5cqVsfNqoqOj+euvv5g/fz6rV6+mfPnyBAUFsWnTJho1akSBAgW4cuUK33zzDSaTiYMHD5I5c+bE/pWJyH8Z23EkItbo0bDU0x5nz541m81m8969e8316tUzZ8yY0ezi4mKuWbOmedu2bXGu9fHHH5srVqxozpIlizl9+vTm4sWLm8eMGWN+8OCB2Ww2m69du2bu27evuXjx4uYMGTKYM2fObK5UqZJ5/vz5Car14MGD5ubNm5uzZMliTpcunblYsWLmDz/8MM45a9asMXt7e5udnJzMxYoVM//8889PvRX8acNSZrPlFnMPDw8zYP7444/jPefBgwfm8ePHm0uWLGl2dnY2Z82a1VyuXDnzyJEjY4fq1q1bZ27atKk5b968ZicnJ3PevHnN7dq1Mx89ejRBX7OIPJt6bkRERMSmaM6NiIiI2BSFGxEREbEpCjciIiJiUxRuRERExKYo3IiIiIhNUbgRERERm+JgdAEpLSYmhgsXLpApU6aX2jlYREREUo7ZbObWrVvkzZsXO7tn982kuXBz4cIFPDw8jC5DREREXsDZs2dxd3d/5jlpLtxkypQJsDTOo71vkkpUVBRr1qyhbt26ODo6Jum1bY3aKuHUVgmntko4tVXiqL0SLrnaKiIiAg8Pj9if48+S5sLNo6EoV1fXZAk3Li4uuLq66pv/OdRWCae2Sji1VcKprRJH7ZVwyd1WCZlSognFIiIiYlMMDTeenp6YTKYnHn379o33/FmzZj1xbrp06VK4ahEREbFmhg5L7dq1i+jo6NjnBw8eJCAggFatWj31Pa6urhw5ciT2ue54EhERkf8yNNzkzJkzzvNPPvmEV155BX9//6e+x2QykSdPnuQuTUREUqno6GiioqKS9JpRUVE4ODhw7969OL+Uy5Nepq2cnJyee5t3QljNhOIHDx7w888/M2jQoGf2xty+fZsCBQoQExND2bJlGTt2LCVLlnzq+ffv3+f+/fuxzyMiIgBL4yfHN/9//ytPp7ZKOLVVwqmtEs4W28psNnPlypXYf+eT+tp58uThzJkzGjF4jpdpKzs7O/Lnzx/vROTEfK+azGazOVGfnEzmz59P+/btOXPmDHnz5o33nO3bt/P333/j6+vLzZs3+eyzz9i0aROHDh166j3vI0aMYOTIkU8cnzt3Li4uLkn6NYiIiHEyZcpE1qxZyZEjB05OTgohqYzZbObq1avcuHGDf/7554nXIyMjad++PTdv3nzu3c5WE27q1auHk5MTy5YtS/B7oqKi8PLyol27dowePTrec+LrufHw8ODatWvJcit4SEgIAQEBulXwOdRWCae2Sji1VcLZWltFR0dz4sQJcubMSfbs2ZP8+o9Wx9Xq9s/3Mm0VERHBhQsXKFiwIA4ODk+8liNHjgSFG6sYljp9+jRr165l4cKFiXqfo6MjZcqU4dixY089x9nZGWdn53jfm1z/QyfntW2N2irh1FYJp7ZKOFtpq+joaEwmExkzZkySORuPi4mJASzzPpPj+rbkZdrK2dk59m7ox78vE/N9ahV/QzNnziRXrlw0atQoUe+Ljo4mPDwcNze3ZKpMRERSE/WqpG5J9fdneLiJiYlh5syZdOnS5YkuqM6dOzNs2LDY56NGjWLNmjWcOHGCvXv30rFjR06fPk3Pnj1TuuwnREdHExoayqZNmwgNDdVsehEREYMYHm7Wrl3LmTNn6N69+xOvnTlzhosXL8Y+v3HjBq+//jpeXl40bNiQiIgItm3bRokSJVKy5CcsXLgQT09PAgICmDRpEgEBAXh6eiZ6mE1ERORleXp68vnnnxt+DSMZPuembt26PG1O88aNG+M8nzx5MpMnT06BqhJu4cKFtGzZ8omv4fz587Rs2ZLg4GACAwMNqk5ERBIjOjqazZs3c/HiRdzc3PDz88Pe3j5ZPut5QzDDhw9nxIgRib7url27yJAhwwtWZRsMDzepWXR0NAMHDow3nJnNZkwmE0FBQTRt2jTZ/ucQEZGksXDhQgYOHMi5c+dij7m7uzNlyhSaNWuW5J/335GJX3/9lY8++ijOCvwZM2aM/bPZbCY6OvqJ6RvxeXyB3LTI8GGp1Gzz5s1x/id4nNls5uzZs2zevDkFqxIRkcR61Av/+L/pj3rhk2OaQZ48eWIfmTNnjl2BP0+ePPz1119kypSJlStXUq5cOZydndmyZQvHjx+nadOm5M6dm4wZM1KhQgXWrl0b57qPDymZTCa+//57mjdvjouLC0WKFGHp0qWJqvXMmTM0bdqUjBkz4urqSuvWrbl8+XLs6/v376dmzZpkypSJLFmyUKNGDXbv3g1Y7ohu0qQJWbNmJUOGDJQsWZIVK1a8eMMlgHpuXsJ/U3dSnCciIknDbDYTGRmZoHOjo6MZMGDAc3vht23bhr29/XNvb3ZxcUmyu37effddPvvsMwoVKkTWrFk5e/YsDRs2ZMyYMTg7O/PTTz/RpEkTjhw5Qv78+Z96nZEjRzJhwgQ+/fRTvvzySzp06MDp06fJli3bc2uIiYmJDTahoaE8fPiQvn370qZNm9jpIx06dKBMmTJMnToVk8nE9u3bY2/d7tu3Lw8ePGDTpk1kyJCBP//8M06vVHJQuHkJCb0FXbeqi4ikrMjIyCT7AWo2mzl//jwFChRI0Pm3b99Osjkvo0aNIiAgIPZ5tmzZKFWqVOzz0aNHs2jRIpYuXUq/fv2eep2uXbvSrl07AMaOHcsXX3zBzp07qV+//nNrWLduHeHh4Zw8eRIPDw8AfvrpJ0qWLMmuXbuoUKECZ86cYciQIRQvXpyYmBhy584du9DemTNnaNGiBT4+PgAUKlQo8Q2RSBqWegl+fn64u7s/M6G7u7vj5+eXglWJiIitKF++fJznt2/fZvDgwXh5eZElSxYyZszI4cOHOXPmzDOv4+vrG/vnDBky4OrqypUrVxJUw+HDh/Hw8IgNNgAlSpQgS5YsHD58GIBBgwbRs2dP6tSpw/jx4zl58mTsuQMGDODjjz+matWqDB8+nAMHDiToc1+Gws1LsLe3Z8qUKcDTZ70XLFhQi0qJiKQwFxcXbt++naBHQud/zJ8/n4iIiOdeLyn3LXy8B2jw4MEsWrSIsWPHsnnzZsLCwvDx8eHBgwfPvM7jq/uaTKbYlYSTwogRIzh06BCNGjVi/fr1vPrqqyxatAiAnj17cuLECTp16kR4eDjly5fnyy+/TLLPjo/CzUsKDAwkODiYfPnyxTmeI0cO7Ozs2Lx5M3369Hnq7e4iIpL0TCYTGTJkSNCjbt26z+yFN5lMeHh4UKtWrQRdLzl/od26dStdu3alefPm+Pj4kCdPHk6dOpVsnwfg5eXF2bNnOXv2bOyxP//8k3///TfOOnNFixblrbfeYvXq1TRu3JhZs2bFvubh4cGbb77JwoULefvtt5k+fXqy1qxwkwQCAwM5deoUISEhDBo0iJCQEC5dusTcuXMxmUx89913DBkyRAFHRMQKPasX/tHzSZMmWcWSHkWKFGHhwoWEhYWxf/9+2rdvn6Q9MPGpU6cOPj4+dOjQgb1797Jz5046d+6Mv78/5cuX5+7du/Tr14+NGzdy+vRptm7dyr59+/Dy8gIgKCiI1atXc/LkSfbu3cuGDRtiX0suCjdJxN7eHn9/f6pXr46/vz/29va0adMmNp1OnDjxqTuXi4iIsZ7WC+/u7m5Vi7FOmjSJrFmzUqVKFZo0aUK9evUoW7Zssn6myWRiyZIlZM2alerVq1OnTh0KFSrEr7/+Clh+/l2/fp3OnTtTtGhR2rZtS506dWIXIIyOjqZv3754eXlRv359ihYtyjfffJOsNetuqWTWo0cPbt++TVBQEMOHDydTpky89dZbRpclIiKPCQwMpGnTpvGuUJzcvSNdu3ala9eusc9r1KgRb2+/p6cn69evj3Osb9++cZ4/PkwV33X+/fffZ9bz+DXy58/PkiVL4j3XycmJX375JfZ5TEwMERERpEuXDiDZ59fER+EmBQwcOJBbt27x4YcfMmjQIDJlymQVm32KiEhc9vb21KhRw+gy5CVpWCqFvP/++wwZMgSAXr16MW/ePIMrEhERsU0KNynEZDIxfvx43nzzTcxmM506dWLZsmVGlyUiImJzFG5SkMlk4uuvv6Zjx448fPiQVq1asW7dOqPLEhERsSkKNynMzs6OmTNn0rx5c+7fv89rr73Gtm3bjC5LRETEZijcGMDBwYFffvmFunXrEhkZScOGDdm3b5/RZYmIiNgEhRuDODs7s2jRIqpVq8bNmzepW7du7B4dIiIi8uIUbgzk4uLC8uXLKVeuHNeuXSMgICDOZmMiIiKSeAo3BsucOTOrVq2iRIkSnD9/ntq1a3P+/HmjyxIREUm1FG6sQI4cOVi7di2vvPIKJ0+eJCAggKtXrxpdloiIpAI1atQgKCjoqa+PGDGC0qVLp1g91kDhxkq4ubmxdu1a3N3dOXz4MPXq1Xvu8tgiIpK0oqNh40b45RfLf6Ojk++zmjRpQv369eN9bfPmzZhMJg4cOJB8BdgwhRsr4unpydq1a8mZMyf79u2jUaNG3Llzx+iyRETShIULwdMTataE9u0t//X0tBxPDj169CAkJIRz58498drMmTMpX748vr6+yfPhNk7hxsoUK1aMkJAQsmTJwrZt22jWrBn37t0zuiwREZu2cCG0bAmP54zz5y3HkyPgNG7cmJw5czJr1qw4x2/fvs1vv/1Gjx49uH79Ou3atSNfvny4uLjg4+MTZ5PKFxETE8OoUaNwd3fH2dmZ0qVLs2rVqtjXHzx4QL9+/XBzcyNdunQUKFCAcePGAZZNOEeMGEH+/PlxdnYmb968DBgw4KXqSQ4KN1aoVKlSrFy5kgwZMrB27Vratm1LVFSU0WWJiKQaZjPcuZOwR0QEDBhgeU981wEICjIREZGw68V3nfg4ODjQuXNnZs2aFWfn7t9++43o6GjatWvHvXv3KFeuHL///jsHDx6kV69edOrUiZ07d75w20yZMoWJEyfy2WefceDAAerVq8drr73G33//DcAXX3zB0qVLmT9/PkeOHGHOnDl4enoCsGDBAiZPnsx3333H33//zeLFi/Hx8XnhWpKLdgW3Uq+++irLli2jQYMGLFmyhK5du/LTTz9hb29vdGkiIlYvMhIyZkyaa5nNcP68iQIFsiTo/Nu3IUOGhF27e/fufPrpp4SGhsbuRj5z5kxatGhB5syZyZw5M4MHD449v3///qxevZr58+dTsWLFRH4lFp999hlDhw6lbdu2AIwfP54NGzbw+eef8/XXX3PmzBmKFClCtWrVMJlMFChQIPa9Z86cIU+ePNSpUwdHR0fy58//wnUkJ/XcWLGaNWuyYMECHBwcmDt3Ln369ImT7kVEJHUrXrw4VapU4YcffgDg2LFjbN68mR49egAQHR3N6NGj8fHxIVu2bGTMmJHVq1dz5syZF/q8iIgILly4QNWqVeMcr1q1auxCsl27diUsLIxixYoxYMAA1qxZE3teq1atuHv3LoUKFeL1119n0aJFPHz48IVqSU4KN1auUaNGzJkzBzs7O6ZNm8aQIUMUcEREnsPFxdKDkpDHihUJu+b8+beJiIh57vVcXBJXa48ePViwYAG3bt1i5syZvPLKK/j7+wPw6aefMmXKFIYOHcqGDRsICwujXr16PHjwIJEtknBly5bl5MmTjB49mrt379K6dWtatmwJgIeHB0eOHOGbb74hffr09OnTh+rVq1vd1AmFm1SgdevWTJ8+HYCJEycyatQogysSEbFuJpNlaCghj7p1wd3d8p6nXcvDw0ytWg8TdL2nXedpWrdujZ2dHXPnzuWnn36ie/fumP7/Ilu3bqVp06Z07NiRUqVKUahQIY4ePfrC7eLq6krevHnZunVrnONbt26lRIkScc5r06YN06dP59dff2XBggX8888/AKRPn54mTZrwxRdfsHHjRrZv3054ePgL15QcNOcmlejevTu3bt0iKCiIESNGkClTJgYNGmR0WSIiqZ69PUyZYrkrymSKOyH4UVCZNMlMck15zJgxI23atGHYsGFERETQtWvX2NeKFClCcHAw27ZtI2vWrEyaNInLly/HCSKJNWTIEIYPH84rr7xC6dKlmTlzJmFhYcyZMweASZMm4ebmRpkyZbCzs+O3334jT548ZMmShVmzZhEdHU2lSpVwcXHh559/Jn369HHm5VgD9dykIgMHDmT06NEAvP3227G9OSIi8nICAyE4GPLli3vc3d1yPDAweT+/R48e3Lhxg3r16pE3b97Y4x988AFly5alXr161KhRgzx58tCsWbOX+qwBAwYwaNAg3n77bXx8fFi1ahVLly6lSJEiAGTKlIkJEyZQvnx5KlSowKlTp1ixYgV2dnZkyZKF6dOnU7VqVXx9fVm7di3Lli0je/bsL1VTUjOZ09gEjoiICDJnzszNmzdxdXVN0mtHRUWxYsUKGjZsiKOjY5Je+xGz2cy7777LhAkTMJlMzJkzh3bt2iXLZyWnlGgrW6G2Sji1VcLZWlvdu3ePkydPUrBgQdKlS/fC14mOhs2b4eJFcHMDPz9Lz05MTAwRERG4urpiZ6d+gWd5mbZ61t9jYn5+a1gqlTGZTHzyySfcunWLqVOn0qlTJzJkyMBrr71mdGkiIqmevT38/x3ZkoopfqZCJpOJr776ik6dOhEdHU2rVq1Yu3at0WWJiIhYBYWbVMrOzo4ffviB5s2b8+DBA5o2bcq2bduMLktERMRwCjepmIODA7/88gv16tUjMjKShg0bsnfvXqPLEhERMZTCTSrn7OzMwoUL8fPz4+bNm9SrVy92lUkRkbQmjd0jY3OS6u9P4cYGuLi4sGzZMsqVK8e1a9eoU6cOJ06cMLosEZEU8+iOr8jISIMrkZfxaOXll91HUXdL2YjMmTOzevVq/P39OXToEHXq1GHz5s3ke3zRBhERG2Rvb0+WLFm4cuUKYPmlz5TYpYKfISYmhgcPHnDv3j3dCv4cL9pWMTExXL16FRcXFxwcXi6eKNzYkOzZsxMSEoKfnx/Hjx+nTp06bNq0iZw5cxpdmohIssuTJw9AbMBJSmazmbt375I+ffokDU226GXays7Ojvz58790Gyvc2Bg3NzfWrVtHtWrV+Ouvv6hXrx7r168nS5YsRpcmIpKsTCYTbm5u5MqVK8k3coyKimLTpk1Ur17dJhY9TE4v01ZOTk5J0jOmcGODChQowLp16/Dz82Pfvn00atSINWvWkCFDBqNLExFJdvb29i89ZyO+az58+JB06dIp3DyHNbSVBg5tVNGiRVmzZg1ZsmRh27ZtNG3alHv37hldloiISLJTuLFhpUqVYtWqVWTIkIF169bRpk2bJO+qFRERsTYKNzauUqVKLFu2jHTp0rF06VK6dOlCdHS00WWJiIgkG4WbNKBmzZoEBwfHrmjcu3dvLXQlIiI2S+EmjWjUqBFz587Fzs6O6dOnM3jwYAUcERGxSQo3aUirVq34/vvvAZg0aRKjRo0yuCIREZGkp3CTxnTr1o0pU6YAMGLECCZNmmRwRSIiIklL4SYNGjBgAB9//DEAb7/9NtOmTTO4IhERkaSjcJNGvffeewwdOhSAN998k7lz5xpckYiISNJQuEmjTCYT48aNo0+fPpjNZjp37sySJUuMLktEROSlKdykYSaTiS+//JLOnTsTHR1N69atWbt2rdFliYiIvBSFmzTOzs6OGTNmEBgYyIMHD2jatClbt241uiwREZEXpnAjODg4MHfuXOrXr09kZCQNGzZk7969RpclIiLyQgwNN56enphMpiceffv2fep7fvvtN4oXL066dOnw8fFhxYoVKVix7XJ2dmbBggX4+fkRERFB3bp1+fPPP40uS0REJNEMDTe7du3i4sWLsY+QkBDAsthcfLZt20a7du3o0aMH+/bto1mzZjRr1oyDBw+mZNk2y8XFheXLl1O+fHmuX79OQEAAJ06cMLosERGRRDE03OTMmZM8efLEPpYvX84rr7yCv79/vOdPmTKF+vXrM2TIELy8vBg9ejRly5blq6++SuHKbZerqyurVq2iZMmSXLhwgdq1a3P+/HmjyxIREUkwB6MLeOTBgwf8/PPPDBo0CJPJFO8527dvZ9CgQXGO1atXj8WLFz/1uvfv3+f+/fuxzyMiIgCIiooiKirq5Qv/j0fXS+rrpjRXV1dWrFhB7dq1OXbsGLVr12bdunXkypUryT7DVtoqJaitEk5tlXBqq8RReyVccrVVYq5nMlvJ7onz58+nffv2nDlzhrx588Z7jpOTEz/++CPt2rWLPfbNN98wcuRILl++HO97RowYwciRI584PnfuXFxcXJKmeBt15coV3nvvPa5du0bBggUZPXo0GTNmNLosERFJgyIjI2nfvj03b97E1dX1medaTc/NjBkzaNCgwVODzYsaNmxYnN6eiIgIPDw8qFu37nMbJ7GioqIICQkhICAAR0fHJL22UapUqUKtWrU4efIkX331FStWrEiSgGOLbZVc1FYJp7ZKOLVV4qi9Ei652urRyEtCWEW4OX36NGvXrmXhwoXPPC9PnjxP9NBcvnyZPHnyPPU9zs7OODs7P3Hc0dEx2b5Bk/PaKa1kyZKEhIRQo0YNduzYQcuWLfn9999Jly5dklzfltoquamtEk5tlXBqq8RReyVcUrdVYq5lFevczJw5k1y5ctGoUaNnnle5cmXWrVsX51hISAiVK1dOzvLSPF9fX1auXEnGjBlZv349rVu31riziIhYLcPDTUxMDDNnzqRLly44OMTtSOrcuTPDhg2LfT5w4EBWrVrFxIkT+euvvxgxYgS7d++mX79+KV12mlOpUiWWLVtGunTpWLZsWeyWDSIiItbG8HCzdu1azpw5Q/fu3Z947cyZM1y8eDH2eZUqVZg7dy7Tpk2jVKlSBAcHs3jxYry9vVOy5DSrRo0aLFiwAAcHB+bNm8ebb76JlcxHFxERiWX4nJu6des+9Qfkxo0bnzjWqlWrpy7yJ8mvYcOGzJ07l7Zt2/L999+TKVMmJk6c+NTb90VERFKa4T03kvq0atWKGTNmADB58uR4b7UXERExisKNvJCuXbvyxRdfADBy5EgmTpxocEUiIiIWCjfywvr378+YMWMAGDx4MN99953BFYmIiCjcyEt67733ePfddwHo3bs3c+bMMbgiERFJ6xRu5KWNHTuWvn37Yjab6dKlyzP3+hIREUluCjfy0kwmE1988QVdunQhOjqaNm3aEBISYnRZIiKSRincSJKws7Pj+++/p0WLFjx48IBmzZqxZcsWo8sSEZE0SOFGkoyDgwNz586lfv36REZG0qhRI/bu3Wt0WSIiksYo3EiScnJyYsGCBVSvXp2IiAjq1q3Ln3/+aXRZIiKShijcSJJzcXFh2bJlVKhQgevXr1OnTh2OHz9udFkiIpJGKNxIsnB1dWXlypV4e3tz8eJF6tSpw7lz54wuS0RE0gCFG0k22bNnJyQkhMKFC3Pq1Cnq1KnDlStXiI6OJjQ0lE2bNhEaGqrdxUVEJEkp3EiyypMnD2vXrsXDw4MjR45QsWJF8ufPT0BAAJMmTSIgIABPT08WLlxodKkiImIjFG4k2RUoUIB169aROXNmTp8+zYULF+K8fv78eVq2bKmAIyIiSULhRlJEoUKFSJcuXbyvmc1mAIKCgjREJSIiL03hRlLE5s2buXz58lNfN5vNnD17ls2bN6dgVSIiYosUbiRFXLx4MUnPExEReRqFG0kRbm5uSXqeiIjI0yjcSIrw8/PD3d0dk8n01HMcHR3JmjVrClYlIiK2SOFGUoS9vT1TpkwBeGrAiYqKolKlSkyZMoWYmJiULE9ERGyIwo2kmMDAQIKDg8mXL1+c4x4eHsyYMYOGDRty//59goKCaNiwoebfiIjIC1G4kRQVGBjIqVOnCAkJYdCgQYSEhHDy5Em6d+/O8uXL+frrr0mXLh2rV6/Gx8eHxYsXG12yiIikMgo3kuLs7e3x9/enevXq+Pv7Y29vD1iGq/r06cPevXspU6YM169fp3nz5rz++uvcvn3b4KpFRCS1ULgRq+Pl5cWOHTt45513MJlMfP/995QpU4adO3caXZqIiKQCCjdilZycnBg/fjzr16/H3d2dY8eOUaVKFT7++GOtYiwiIs+kcCNWrUaNGhw4cIA2bdoQHR3Nhx9+iL+/PydPnjS6NBERsVIKN2L1smbNyi+//MJPP/1EpkyZ2Lp1K6VKlWL27Nmx+1KJiIg8onAjqYLJZKJTp07s37+fqlWrcuvWLTp37ky7du24ceOG0eWJiIgVUbiRVKVgwYJs3LiR0aNHY29vz6+//kqpUqXYuHGj0aWJiIiVULiRVMfBwYEPPviAbdu2UbhwYc6ePUutWrUYOnQoDx48MLo8ERExmMKNpFoVK1Zk37599OzZE7PZzIQJE3j11Vc5fPiw0aWJiIiBFG4kVcuYMSPTp09n4cKFZM+enX379lGuXDm++eYbTTYWEUmjFG7EJjRv3pwDBw5Qt25d7t69S9++fWnSpAmXL182ujQREUlhCjdiM/LmzcvKlSuZMmUKzs7O/P777/j4+LB8+XKjSxMRkRSkcCM2xc7OjgEDBrB79258fX25evUqTZo0oU+fPkRGRhpdnoiIpACFG7FJ3t7e/PHHHwwaNAiAqVOnUq5cOfbu3WtwZSIiktwUbsRmpUuXjokTJxISEkLevHn566+/ePXVVxk/frz2pxIRsWEKN2Lz6tSpw4EDBwgMDCQqKop3332X2rVrc+bMGaNLExGRZKBwI2lC9uzZCQ4OZsaMGWTIkIHQ0FB8fX2ZN2+e0aWJiEgSU7iRNMNkMtG9e3fCwsKoVKkSN2/epF27dnTq1ImbN28aXZ6IiCQRhRtJcwoXLszmzZv56KOPsLOz4+eff6ZUqVJs2bLF6NJERCQJKNxImuTo6MjIkSPZsmULhQoV4vTp0/j7+/P+++8TFRVldHkiIvISFG4kTatcuTJhYWF07dqVmJgYxo4dS9WqVTl69KjRpYmIyAtSuJE0L1OmTMycOZPffvuNrFmzsmvXLsqUKcP06dO1P5WISCqkcCPy/1q2bMmBAweoVasWkZGR9OrVi+bNm3Pt2jWjSxMRkURQuBH5D3d3d0JCQvjss89wcnJiyZIl+Pj4sHr1aqNLExGRBFK4EXmMnZ0db7/9Njt37qREiRJcunSJ+vXrM3DgQO7evWt0eSIi8hwKNyJPUapUKXbv3k3//v0B+OKLL6hQoQIHDhwwuDIREXkWhRuRZ0ifPj1ffPEFK1asIHfu3Bw6dIgKFSowadIkYmJijC5PRETioXAjkgANGjQgPDyc1157jQcPHvD2229Tt25dzp8/b3RpIiLyGIUbkQTKmTMnixcv5rvvvsPFxYV169bh4+NDcHCw0aWJiMh/KNyIJILJZKJXr17s27eP8uXLc+PGDVq1akW3bt24deuW0eWJiAgKNyIvpGjRomzbto333nsPk8nErFmzKF26NNu3bze6NBGRNE/hRuQFOTo6MmbMGEJDQylQoAAnTpzAz8+PESNG8PDhQ6PLExFJswwPN+fPn6djx45kz56d9OnT4+Pjw+7du596/saNGzGZTE88Ll26lIJVPyk6GkJDTWzalI/QUBPR0YaWIynIz8+P/fv306FDB6Kjoxk5ciR+fn4cP37c6NJERNIkQ8PNjRs3qFq1Ko6OjqxcuZI///yTiRMnkjVr1ue+98iRI1y8eDH2kStXrhSoOH4LF4KnJwQEODBpUnkCAhzw9LQcl7Qhc+bM/Pzzz8ydO5fMmTOzY8cOSpcuzaxZs7Q/lYhICjM03IwfPx4PDw9mzpxJxYoVKViwIHXr1uWVV1557ntz5cpFnjx5Yh92dsZ8KQsXQsuWcO5c3OPnz1uOK+CkLe3atWP//v1Ur16d27dv061bN1q3bs0///xjdGkiImmGg5EfvnTpUurVq0erVq0IDQ0lX7589OnTh9dff/257y1dujT379/H29ubESNGULVq1XjPu3//Pvfv3499HhERAUBUVBRRUVEvVX90NAwY4IDlF3NTnNfMZjCZzAwcCA0bPsTe/qU+yuY8avuX/TuwRnnz5mX16tVMnDiRESNGEBwczPbt25kxYwa1atVK9PVsua2Smtoq4dRWiaP2SrjkaqvEXM9kNrDPPF26dAAMGjSIVq1asWvXLgYOHMi3335Lly5d4n3PkSNH2LhxI+XLl+f+/ft8//33zJ49mz/++IOyZcs+cf6IESMYOXLkE8fnzp2Li4vLS9UfHp6dDz+s9tzzRo/ego/P9Zf6LEmdjh07xuTJk2MX+2vatCkdO3bE0dHR4MpERFKXyMhI2rdvz82bN3F1dX3muYaGGycnJ8qXL8+2bdtijw0YMIBdu3Yl6pZaf39/8ufPz+zZs594Lb6eGw8PD65du/bcxnmeefNMdO78/M6vn356SNu2mnfxX1FRUYSEhBAQEGDzP+jv3LnD0KFDmTZtGgA+Pj789NNPlCxZMkHvT0tt9bLUVgmntkoctVfCJVdbRUREkCNHjgSFG0OHpdzc3ChRokScY15eXixYsCBR16lYsSJbtmyJ9zVnZ2ecnZ2fOO7o6PjSje7hkdDzHND/C/FLir8Ha5clSxa+++47GjduTI8ePQgPD+fVV19lwoQJ9O/fH5PJ9PyLkDbaKqmorRJObZU4aq+ES+q2Ssy1DJ1QXLVqVY4cORLn2NGjRylQoECirhMWFoabm1tSlpYgfn7g7g7P+tnk6gpVqqRcTWK9mjRpQnh4OA0bNuT+/fsMHDiQBg0acPHiRaNLExGxKYaGm7feeosdO3YwduxYjh07xty5c5k2bRp9+/aNPWfYsGF07tw59vnnn3/OkiVLOHbsGAcPHiQoKIj169fHeU9KsbeHKVMsf35awImIgHr14MKFlKtLrFfu3LlZvnw5X331FenSpWP16tX4+vqyZMkSo0sTEbEZhoabChUqsGjRIn755Re8vb0ZPXo0n3/+OR06dIg95+LFi5w5cyb2+aMdmX18fPD392f//v2sXbuW2rVrG/ElEBgIwcGQL1/c4x4eEBQEGTPCxo1QujSsWWNAgWJ1TCYTffv2Zc+ePZQuXZpr167RrFkzevXqxZ07d4wuT0Qk1TN8heLGjRsTHh7OvXv3OHz48BO3gc+aNYuNGzfGPn/nnXc4duwYd+/e5fr162zYsIGaNWumcNVxBQbCqVMQEvKQQYN2ExLykJMnYfJk2LMHSpWCq1ehfn14/33QyvwCUKJECXbs2MGQIUMwmUxMnz6dMmXKsGvXrthzoqOjCQ0NZdOmTYSGhhKtpa9FRJ7L8HBjK+ztwd/fTPXq5/H3N8eua1O0KOzYAW++aVn7ZuxYqFXryUX/JG1ydnZmwoQJrFu3Dnd3d/7++2+qVKnCmDFjCA4OxtPTk4CAACZNmkRAQACenp4s1MqQIiLPlCTh5t9//02Ky9isdOlg6lT49VfIlAk2b7YMU61caXRlYi1q1qzJgQMHaN26NQ8fPuSDDz6gVatWnHssBZ8/f56WLVsq4IiIPEOiw8348eP59ddfY5+3bt2a7Nmzky9fPvbv35+kxdma1q1h714oWxauX4eGDWHoUNCClwKQNWtW5s2bx8yZM596e/ijZamCgoI0RCUi8hSJDjfffvstHv+/wEtISAghISGsXLmSBg0aMGTIkCQv0NYULgzbtkG/fpbnEyZAjRrwnznTkoaZTCY8PT2fudmm2Wzm7NmzbN68OQUrExFJPRIdbi5duhQbbpYvX07r1q2pW7cu77zzTpyJkPJ0zs7w5ZeWu6wyZ7aEnTJlYNkyoysTa5DQdW+0Po6ISPwSHW6yZs3K2bNnAVi1ahV16tQBLL9Nqps8cVq0sAxTVagA//wDr70Gb78NDx4YXZkYKaELUhqxcKWISGqQ6HATGBhI+/btCQgI4Pr16zRo0ACAffv2Ubhw4SQv0NYVKgRbtljWxAGYNAmqV7fcWi5pk5+fH+7u7s/clsHBwSHebUVEROQFws3kyZPp168fJUqUICQkhIwZMwKWLvI+ffokeYFpgZOTZU2cxYshSxb44w/LMNXixQYXJoawt7dnyv8vff20gPPw4UP8/PwYPnw4UZqRLiISR6LDjaOjI4MHD2bKlCmUKVMm9vhbb71Fz549k7S4tKZpUwgLg0qV4N9/oXlzS4+OhqnSnsDAQIKDg8n32NLXHh4e/Pjjj7Rv357o6GhGjRpF5cqVOXz4sEGViohYnxda52b27NlUq1aNvHnzcvr0aeB/ez7JyylQADZtssy9AcveVVWrwokTxtYlKS8wMJBTp04REhLCoEGDCAkJ4eTJk3Tu3Jk5c+Ywb948smbNyp49eyhbtixTpkwhJibG6LJFRAyX6HAzdepUBg0aRIMGDfj3339jJxFnyZKFzz//PKnrS5OcnOCzzyx3T2XLBrt3W4apgoONrkxSmr29Pf7+/lSvXh1/f3/sHy19DbRp04aDBw9Sr1497t27R1BQEHXr1o2d8C8iklYlOtx8+eWXTJ8+nffffz/OP7Tly5cnPDw8SYtL6xo3tgxTVali2V28VSvo2xfu3TO6MrEWefPmZeXKlXzzzTe4uLiwbt06fHx8mD179jPXyhERsWWJDjcnT56MM9fmEWdnZ+1onAw8PCy7ig8dann+zTeWsPP334aWJVbEZDLRu3dvwsLCqFSpEjdv3qRz5860atWKa9euGV2eiEiKS3S4KViwIGFhYU8cX7VqFV5eXklRkzzG0RE++QRWrIAcOWDfPihXDubNM7oysSZFihRhy5YtfPzxxzg4OLBgwQJ8fHz4/fffjS5NRCRFJTrcDBo0iL59+/Lrr79iNpvZuXMnY8aMYdiwYbzzzjvJUaP8vwYNLMNUfn5w6xa0a2fZbfzuXaMrE2vh4ODA+++/zx9//EGJEiW4dOkSjRs35o033uD27dtGlycikiISHW569uzJ+PHj+eCDD4iMjKR9+/ZMnTqVKVOm0LZt2+SoUf4jXz5Yvx7efx9MJvjuO3j1VThyxOjKxJqULVuWPXv28NZbbwEwbdo0SpUqxdatWw2uTEQk+b3QreAdOnTg77//5vbt21y6dIlz587Ro0ePpK5NnsLBAT7+GFavhpw54cAByzDVnDlGVybWJF26dEyaNIn169eTP39+Tpw4QfXq1Rk2bBgPtHiSiNiwFwo3j7i4uJArV66kqkUSKSAA9u+37Cp+5w507Ag9e0JkpNGViTWpWbMmBw4coEuXLsTExPDJJ59QsWJF3d0oIjbrhSYUFypU6KkPSVlubrB2LQwfbhmmmjHDssKxFqyV/8qcOTOzZs1i4cKF5MiRg/3791O+fHk+/fRTbXgrIjbHIbFvCHq0w+P/i4qKYt++faxatYohQ4YkVV2SCPb2MGKEZcPNDh3g4EEoX95y23iXLkZXJ9akefPmVK5cmddff53ly5fzzjvvsGzZMn788UcKFixodHkiIkki0eFm4MCB8R7/+uuv2b1790sXJC+uVi3L3VQdO1p6c7p2hQ0b4OuvIUMGo6sTa5EnTx6WLl3KDz/8QFBQEJs3b8bX15cpU6bQrVu3Z+5GLiKSGrzUnJv/atCgAQsWLEiqy8kLyp0bVq2C0aPBzg5+/BEqVLD05og8YjKZ6NGjB/v376datWrcvn2bHj160KxZMy5fvmx0eSIiLyXJwk1wcDDZsmVLqsvJS7C3hw8+sNwynjevZf5NxYqW+ThakV/+q1ChQmzcuJHx48fj5OTE0qVL8fHxYfHixUaXJiLywhI9LFWmTJk43dZms5lLly5x9epVvvnmmyQtTl6Ov79lmKpTJ8tt4z17Woapvv0WMmY0ujqxFvb29rzzzjvUr1+fTp06ceDAAZo3b07Xrl2ZMmUKrq6uRpcoIpIoiQ43zZo1i/Pczs6OnDlzUqNGDYoXL55UdUkSyZnTsm3DhAmW3pw5c2DXLvjtN/D1Nbo6sSa+vr7s3LmT4cOHM2HCBGbNmsWGDRuYNWsWNWrUMLo8EZEES3S4GT58eHLUIcnIzg7efReqVbNs2XD0qGWYasoU6NXLcgu5CFg2wP3kk09o3LgxnTt35uTJk9SqVYu33nqLMWPGkC5dOqNLFBF5rgTNuYmIiEjwQ6xXtWqWTTcbNYL79y37UrVrB/prk8dVq1aN/fv38/rrr2M2m5k0aRLly5dn3759RpcmIvJcCQo3WbJkIWvWrM98PDpHrFuOHLB0KXz6qWUbh19/tWzdoJ9Z8rhMmTIxbdo0li1bRu7cuTl06BCVKlVi7NixPHz40OjyRESeKkHDUhs2bEjuOiQF2dnB4MFQtSq0bQvHjlk235w0Cfr00TCVxNW4cWPCw8N58803WbhwIe+//z7Lly/np59+onDhwkaXJyLyhASFG39//+SuQwxQubKlx6ZbN0tvTr9+sHEjfP89ZM5sdHViTXLmzElwcDCzZ8+mf//+bN++nVKlSjFx4kTeeOMNLfwnIlblhde5iYyM5K+//uLAgQNxHpK6ZMsGixdbem0cHSE4GMqUAS02LY8zmUx07tyZ8PBwatasSWRkJL1796Zhw4ZcuHDB6PJERGIlOtxcvXqVxo0bkylTJkqWLEmZMmXiPCT1MZngrbdgyxbw9ISTJ6FKFcvdVFr0Tx6XP39+1q5dy+TJk3F2dmbVqlX4+Pgwf/58o0sTEQFeINwEBQXx77//8scff5A+fXpWrVrFjz/+SJEiRVi6dGly1CgppGJFyzBV8+YQFQVBQRAYCDduGF2ZWBs7OzuCgoLYu3cvZcuW5Z9//qFNmzZ06NCBG/qGERGDJTrcrF+/Pva2UDs7OwoUKEDHjh2ZMGEC48aNS44aJQVlyQILFsAXX4CTk2XIqkwZ+OMPoysTa1SiRAm2b9/OBx98gJ2dHXPnzsXHx4e1a9caXZqIpGGJDjd37twhV65cAGTNmpWrV68C4OPjw969e5O2OjGEyQT9+8O2bVCoEJw+bVkjZ9IkDVPJk5ycnBg9ejRbt26lSJEinD9/noCAAAYMGEBkZKTR5YlIGpTocFOsWDGOHDkCQKlSpfjuu+84f/483377LW5ubkleoBinXDnYuxdatYKHD+Htt6FpU/jnH6MrE2v06quvsm/fPvr06QPAl19+SdmyZdm1a5fBlYlIWpPocDNw4EAuXrwIWLZiWLlyJfnz5+eLL75g7NixSV6gGCtzZstCf998A87OsGwZlC5t6dUReVyGDBn4+uuvWbVqFXnz5uXIkSNUrlyZESNGEBUVZXR5IpJGJDrcdOzYka5duwJQrlw5Tp8+za5duzh79ixt2rRJ6vrECphM0Ls37NgBRYrA2bNQvbplM86YGKOrE2tUr149wsPDadu2LdHR0YwcOZIqVarw119/GV2aiKQBiQ43W7ZsifPcxcWFsmXLkiNHjiQrSqxT6dKwZ49lP6roaBg6FBo3hmvXjK5MrFG2bNn45Zdf+OWXX8iSJQu7d++mTJkyfPHFF8QoFYtIMkp0uKlVqxYFCxbkvffe488//0yOmsSKZcoEc+bAtGmQLh2sXGkJPZs3G12ZWKu2bdty8OBB6taty7179xg4cCB169bl7NmzRpcmIjYq0eHmwoULvP3224SGhuLt7U3p0qX59NNPOXfuXHLUJ1bIZILXX7fcHl6sGJw/DzVrwtixGqaS+OXLl49Vq1bx9ddfkz59etatW4ePjw8///wzZt2CJyJJLNHhJkeOHPTr14+tW7dy/PhxWrVqxY8//oinpye1atVKjhrFSvn6WrZp6NTJMkz1/vtQvz5cuWJ0ZWKNTCYTffr0ISwsjIoVK3Lz5k06depE69atuX79utHliYgNeeG9pQAKFizIu+++yyeffIKPjw+hoaFJVZekEhkzwo8/wg8/QPr0EBJiGabauNHoysRaFS1alK1btzJq1CgcHBwIDg7G29ubFStWGF2aiNiIFw43W7dupU+fPri5udG+fXu8vb35/fffk7I2SSVMJsvO4rt2QYkScPEi1K4No0ZZenREHufg4MCHH37Ijh07KF68OJcuXaJRo0a8+eab3L592+jyRCSVS3S4GTZsGAULFqRWrVqcOXOGKVOmcOnSJWbPnk39+vWTo0ZJJUqWhJ07LUEnJgaGD4e6deHSJaMrE2tVrlw59u7dS1BQEADfffcdpUuXZpsWUhKRl5DocLNp0yaGDBnC+fPnWb58Oe3atcPFxSU5apNUKEMGyxDVTz+BiwusX28Zplq37n/nREdDaKiJTZvyERpqUu9OGpc+fXomT57MunXr8PDw4Pjx4/j5+fHee+/x4MEDo8sTkVQo0eHm0XCU1rWRZ+nUybImjrc3XL4MAQHw0UcQHAyenhAQ4MCkSeUJCHDA0xMWLjS6YjFarVq1OHDgAJ06dSImJoZx48ZRsWJFDh48aHRpIpLKvNSEYpFnKV7cMkz1+uuWDTdHj7bsU/X4qgHnz0PLlgo4AlmyZOGnn34iODiY7Nmzs3//fsqVK8dnn31GtLr4RCSBFG4kWaVPb1nwb/Zsy8Tj+Dxa5iQoSBOQxaJFixYcPHiQRo0a8eDBA4YMGUKtWrU4deqU0aWJSCqgcCMpwt39fyEmPmazZc8qrXQsj+TJk4dly5Yxbdo0MmTIwKZNm/D19WXmzJla+E9EnknhRlLE/28kn2TnSdpgMpl4/fXX2b9/P1WrVuXWrVt0796d5s2bc0WrRYrIUyQ63Jw9ezbOVgs7d+4kKCiIadOmJWlhYlvc3BJ23g8/WO6w0jYO8l+vvPIKoaGhfPLJJzg6OrJkyRK8vb1ZsmRJ7DnR0dGEhoayadMmQkNDNUdHJA1LdLhp3749GzZsAODSpUsEBASwc+dO3n//fUaNGpXkBYpt8POzDE09bd7NI2vXWhYAfOUVGDECNMVCHrG3t2fo0KHs2rULHx8frl69SrNmzejevTs///wznp6eBAQEMGnSJAICAvD09GShZqmLpEmJDjcHDx6kYsWKAMyfPx9vb2+2bdvGnDlzmDVrVqILOH/+PB07diR79uykT58eHx8fdu/e/cz3bNy4kbJly+Ls7EzhwoVf6HMlZdnbw5Qplj8/HnBMJstj3Dh44w1wdbWEmpEjoWBBS9j5+WeIjEzxssUKlSpVil27djFkyBBMJhMzZ86kU6dOT2zee/78eVq2bKmAI5IGJTrcREVF4ezsDMDatWt57bXXAChevDgXEzlh4saNG1StWhVHR0dWrlzJn3/+ycSJE8maNetT33Py5EkaNWpEzZo1CQsLIygoiJ49e7J69erEfimSwgIDLevc5MsX97i7u+X4u+/Ct99aVjSeMwfq1LGEnvXrLevmuLlBr16wffuzJyeL7XN2dmbChAmsX78ee3v7eM95NOk4KChIQ1QiaUyiw03JkiX59ttv2bx5MyEhIbFbLly4cIHs2bMn6lrjx4/Hw8ODmTNnUrFiRQoWLEjdunV55ZVXnvqeb7/9loIFCzJx4kS8vLzo168fLVu2ZPLkyYn9UsQAgYGWXpmQkIcMGrSbkJCHnDxpOf5I+vTQvr1lE86TJy17VBUsCBERMH06VKkCXl4wfjxcuGDYlyJW4lnBxWw2c/bsWTbrNjyRNCXR4Wb8+PF899131KhRg3bt2lGqVCkAli5dGjtclVBLly6lfPnytGrVily5clGmTBmmT5/+zPds376dOnXqxDlWr149tm/fnrgvRAxjbw/+/maqVz+Pv7+Zp/ziDUCBAvDhh3DsGGzYAJ07W7Z1OHLE0tPj4QGNGll6fu7fT7mvQaxDQnuLE9urLCKpm0Ni31CjRg2uXbtGREREnOGjXr16JXqPqRMnTjB16lQGDRrEe++9x65duxgwYABOTk506dIl3vdcunSJ3LlzxzmWO3duIiIiuHv3LunTp4/z2v3797n/n596ERERgGV4LSoqKlH1Ps+j6yX1dW3Ri7RV1aqWx+TJEBxs4scf7di2zY4VK2DFCsiWzUy7djF07hxDmTLJVXnK0/fV0+XMmTPB56n94tL3VeKovRIuudoqMdczmRO5Gtbdu3cxm82xQeb06dMsWrQILy8v6tWrl6hCnZycKF++fJwdgAcMGMCuXbue2hNTtGhRunXrxrBhw2KPrVixgkaNGhEZGflEuBkxYgQjR4584jpz587Vhp824Pz5DKxfn5+NGz24fv1/f/eenjepXfsM/v7ncHXV5ou2Kjo6ml69enH9+vVnnte3b1/q1KmD6Xm364mI1YqMjKR9+/bcvHkTV1fXZ56b6J6bpk2bEhgYyJtvvsm///5LpUqVcHR05Nq1a0yaNInevXsn+Fpubm6UKFEizjEvLy8WLFjw1PfkyZOHy5cvxzl2+fJlXF1dnwg2AMOGDWPQoEGxzyMiIvDw8KBu3brPbZzEioqKIiQkhICAABwdHZP02rYmKdvq9dct2zasXfuQH3+0Y+lSE6dOZWbGDB9++smbRo3MdOkSQ716ZhwS/R1vPH1fPds333xD27ZtAeKsXGwymWKff/3115w7d45vvvkmwb09tk7fV4mj9kq45GqrRyMvCZHof+r37t0bO3k3ODiY3Llzs2/fPhYsWMBHH32UqHBTtWpVjhw5EufY0aNHKVCgwFPfU7lyZVasWBHnWEhICJUrV473fGdn59i7u/7L0dEx2b5Bk/Patiap2srRERo3tjz++Qd++QVmzoQ9e0wsXmxi8WI78uSx3HXVrZtlQnJqo++r+LVu3RoHBwcGDhwY53Zwd3d3Jk2axLFjx/joo49YsmQJO3bsYMaMGTRq1MjAiq2Lvq8SR+2VcEndVom5VqInFEdGRpIpUyYA1qxZQ2BgIHZ2drz66qucPn06Udd666232LFjB2PHjuXYsWPMnTuXadOm0bdv39hzhg0bRufOnWOfv/nmm5w4cYJ33nmHv/76i2+++Yb58+fz1ltvJfZLERuVLRv07Qu7d8OBA/DWW5Azp+UW808/hRIl4NVX4bvv4OZNo6uVpBAYGMipU6cICQlh0KBBhISEcPLkSVq2bMm7777LH3/8QYkSJbh8+TKNGzemd+/e3Llzx+iyRSSZJDrcFC5cmMWLF3P27FlWr15N3bp1Abhy5Uqih3kqVKjAokWL+OWXX/D29mb06NF8/vnndOjQIfacixcvcubMmdjnBQsW5PfffyckJIRSpUoxceJEvv/++0TP95G0wccHJk2Cc+dg0SJ47TXL3Vp//AFvvgl58kDHjrBunbZ8SO3s7e3x9/enevXq+Pv7x1n/pkyZMuzevZugoCDAsqRE6dKl+eOPPwyqVkSSU6LDzUcffcTgwYPx9PSkYsWKscNBa9asocwL3KLSuHFjwsPDuXfvHocPH+b111+P8/qsWbPYuHFjnGM1atRg37593L9/n+PHj9O1a9dEf66kLU5O0KwZLFkC58/DZ59ByZJw797/FgwsVAiGD7esrSO2J3369EyePJm1a9fi7u7OsWPHqFq1KsOHD9cdMCI2JtHhpmXLlpw5c4bdu3fHWRW4du3aWkhPUoXcueHttyE8HHbutPTgZM4Mp09bFgwsVAhq1oSffgKNXNie2rVrc+DAAdq3b090dDSjRo2Kd/6fiKReiQ43YLljqUyZMly4cCF2Al/FihUpXrx4khYnkpxMJqhQAaZOhYsXYe5cCAiwHN+4Ebp0sWz50LMnbN2qLR9sSdasWZkzZw6//PILWbJkYdeuXZQpU4ZvvvmGRK6OISJWKNHhJiYmhlGjRpE5c2YKFChAgQIFyJIlC6NHjyZGkxYklUqfHtq1gzVrLNtDjB5t2Zn81i2YMQOqVYPixS2be54/b3S1klTatm1LeHg4tWvX5u7du/Tt25cGDRpwQft6iKRqiQ4377//Pl999RWffPIJ+/btY9++fYwdO5Yvv/ySDz/8MDlqFElR+fPDBx/A339DaCh07QoZMsDRo/Dee5bXGzSA+fO15YMtcHd3Z82aNUyZMoV06dKxevVqfHx8CA4ONro0EXlBiQ43P/74I99//z29e/fG19cXX19f+vTpw/Tp05k1a1YylChiDJMJqle3rJdz6RL88AP4+Vnuqlq1Ctq0sQxb9esHe/Zo2Co1s7OzY8CAAezZs4cyZcrwzz//0KpVKzp37sxNrRcgkuokOtz8888/8c6tKV68OP/880+SFCVibTJmtCz+t2mTpUfn/ffB3R1u3ICvv4by5aFUKcu+V1evGl2tvKgSJUqwY8cO3nvvPezs7Jg9eza+vr6EhoYaXZqIJEKiw02pUqX46quvnjj+1Vdfxe4QLmLLCheGjz+2zM1ZvRratgVnZ8vdV4MGQd680Lw5LF0KusM49XFycmLMmDFs2rSJQoUKcebMGWrWrMmQIUPibMIrItYr0eFmwoQJ/PDDD5QoUYIePXrQo0cPSpQowaxZs/j000+To0YRq2RvD3XrWrZ6uHgRvvnGcvfVw4eweDE0bQoeHjB4MBw6ZHS1klhVq1YlLCyMnj17Yjab+eyzz6hQoQIHDhwwujQReY5Ehxt/f3+OHj1K8+bN+ffff/n3338JDAzkyJEj+Pn5JUeNIlYva1bo3duybk54uGUdnVy54PJlmDgRvL2hYkX49lv491+jq5WEypQpE9OnT2fJkiXkzJmT8PBwKlSowKeffkp0dLTR5YnIU7zQOjd58+ZlzJgxLFiwgAULFvDxxx8TExNDr169kro+kVTH29uyAvK5c5YVkZs2BQcH2LXLEoDc3KB9ewgJsexmLtbvtddeIzw8nCZNmvDgwQPeeecdateunej99EQkZbxQuInP9evXmTFjRlJdTiTVc3S07GW1eLFlbZxHPTj37lmGsurWhYIF4cMP4fjx+K8RHQ2hoSY2bcpHaKhJYchAuXPnZsmSJUyfPp0MGTIQGhqKr68vP/30kxb+E7EySRZuROTpcuWyTDY+cMDSg9OnD2TJAmfPWiYnFy4M/v4waxbcvm15z8KF4OkJAQEOTJpUnoAABzw9LcfFGCaTiZ49exIWFkblypWJiIigS5cutGrVimvXrhldnoj8P4UbkRRkMlluG//6a8sk5HnzoF49y/FNmyy3m+fJA7VqQYsWlqGt/zp/Hlq2VMAxWuHChdm0aRMff/wxDg4OLFiwAB8fH1atWmV0aSKCwo2IYdKlsywEuGoVnDkDY8ZYenDu3IENG+J/z6PRj6AgzdcxmoODA++//z47duygePHiXLp0iQYNGtC3b18iIyONLk8kTXNI6ImBgYHPfP1f3QIi8sLc3S1bOwwbBl99BQMGPP1cs9kynLV5M9SokWIlylOUK1eOvXv3MnToUL788ku++eYb1q5dy+zZs6lYsaLR5YmkSQnuucmcOfMzHwUKFKBz587JWauIzTOZIEeOhJ3bv7/lrqzwcG39YLT06dPzxRdfsHr1avLmzcvRo0epUqUKo0aN4uHDh0aXJ5LmJLjnZubMmclZh4j8Pze3hJ138CAMGWJ5uLlZ7r6qWxcCAiBnzuStUeJXt25dwsPD6d27N/Pnz2f48OH8/vvvzJ49m6JFixpdnkiaoTk3IlbGz88yTGUyxf+6yWSZdDxxomV38vTpLZOTf/wROnSw3JlVrpxlmGvjRnjwIEXLT/OyZcvGvHnzmDNnDpkzZ2bnzp2UKVOGb7/9VreMi6QQhRsRK2NvD1OmWP78eMB59Pzrry23lq9YAf/8A2vXWnpwHm3vtncvjBsHNWtCtmzQpIllLs/RoxrCSgkmk4n27dsTHh5OzZo1iYyMpHfv3jRu3JhLly4ZXZ6IzVO4EbFCgYEQHAz58sU97u5uOf7f+f3p0kHt2jBhAoSFWXpxfvrJ0ouTM6fl7qvlyy1zdIoVg0KF4I03LLeT6z6A5OXh4cHatWuZNGkSzs7OrFixAm9vbxbqXn6RZKVwI2KlAgMtO4+HhDxk0KDdhIQ85OTJuMEmPnnyQKdO8PPPcOmSpRfnk08svTiOjpZrTptmWUcnRw6oWhVGjYIdO3R7eXKws7PjrbfeYvfu3ZQqVYrr16/TokULunXrRkREhNHlidgkhRsRK2ZvD/7+ZqpXP4+/vxl7+8S9384OypSBoUNh/XrLENZ/e3Gio2HbNhg+HCpXtoSdVq3g++8ta+9I0vH29uaPP/7g3XffxWQyMWvWLHx9fdm8ebPRpYnYHIUbkTQkY0Zo1Ai++AL++ituL06WLJZhquBgeP11KFAAvLxg4EDL3J47dwwu3gY4Ozszbtw4QkND8fT05PTp0/j7+/Puu+9y//59o8sTsRkKNyJpWIECliATHAxXr1p6cUaMsPTi2NlZAtAXX1gCUbZslrk948db5vbExBhdferl5+fH/v376datG2azmfHjx1OpUiUOHjxodGkiNkHhRkQAcHCwhJrhwy0h5/p1S+jp1csSgh48sAxtvfuuZagrb17L3J7Zs+HyZaOrT31cXV354YcfWLhwITly5GD//v2UK1eOSZMmEaPkKPJSFG5EJF5ZsliGq777Dk6ehCNHLL04jRtDhgyWQPPzz9C5s2US86O5PevWgUZYEq558+aEh4fTqFEjHjx4wNtvv02dOnU4o0lPIi9M4UZEnstkgqJFLRORly2z9Ops2GDpxSlb1nJOWJjldvQ6dSxDWA0bWtbr+esvra3zPHny5GHZsmV8++23uLi4sGHDBnx9fZkzZ44W/hN5AQo3IpJozs6WTTvHjYM9eyy9OHPm/K8XJzISVq607F7u5fW/uT2//QY3bhhdvXUymUy88cYbhIWFUalSJW7evEnHjh1p27Yt//zzj9HliaQqCjci8tJy5YL27S1bQFy4APv3/68Xx9nZsov5999D69aW281ffdUyt2frVtC+knEVKVKELVu2MGrUKOzt7Zk/fz4+Pj6sWbPG6NJEUg2FGxFJUiYT+PpatoMICbGsrfOoF6dECctdVn/8YVk4sFo1yJ7dsjDho7k9Ag4ODnz44Yds376dokWLcuHCBerVq0f//v2JjIw0ujwRq6dwIyLJysUF6teHyZPh0CHL4oAzZlh6cbJlg4gIWLQI3nzTsjVE0aLQr59lbs+tW0ZXb6wKFSqwb98++vbtC8BXX31FuXLl2LNnj8GViVg3hRsRSVEeHtC9O/z6K1y5YunFGT3a0otjbw9//23ZGPS11yy9Ov+d2/OsO6SjoyE01MSmTfkIDTXZzFYSLi4ufPXVV6xcuRI3Nzf++usvXn31VT7++GMeakxPJF4KNyJiGHt7qFgRPvgANm+23IX1qBenYEGIioLQUHjvPShfHnLnjju355GFC8HTEwICHJg0qTwBAQ54elqO24r69esTHh5OixYtePjwIR9++CHVq1fn2LFjRpcmEstafslQuBERq5E5MzRrBlOnwokTcXtxMmaEa9fgl1+ga1fLjum+vtC0qWU9nnPn4l7r/Hlo2dK2Ak727Nn57bff+Omnn3B1dWX79u2ULl2a6dOn65ZxMZw1/ZKhcCMiVqtwYejTB5YssUxMDg2F99+39OKYTBAeDkuXxv/eRz/rg4Jsa7dzk8lEp06dOHDgAP7+/ty5c4devXrx2muvcVlLRYtBFi60/DJhLb9kKNyISKrg6AjVq8PHH8OuXZb5Oh9++Oz3mM2W29DfeMNyx5Yt/ewvUKAA69ev57PPPsPJyYnly5fj4+PDkiVLjC5N0pjoaMsGu/F1Hhr1S4bCjYikSjlyWBYITIgZMywrJufJY9kTq3FjSzBauNCyM3pqHdGxs7Pj7bffZvfu3fj4+HD16lWaNWtGz549uZXWbzWTFLNmzZM9Nv/16JeMzZtTriaHlPsoEZGk5eaWsPNq1bJMQD5yBC5ehN9/tzweyZrVsjdWmTKW7STKlLHckm5vnzx1JzUfHx927drFhx9+yGeffcaMGTNYv349s2fPpmrVqkaXJzbi2jU4fPjJR0K3Qbt4MXnr+y+FGxFJtfz8wN3dMq4fX++LyWR5fc0aS1C5fRsOHIC9e2HfPst/Dx2ybAmxfr3l8YiLC5Qq9b+wU6YMlCxpWXHZGjk7OzNhwgQaNWpE586dOXnyJNWrV2fo0KGMGDECJycno0uUVMBstvTCPB5g/vzTEm5eRkJ/GUkKCjcikmrZ21s252zZ0hJk/htwTCbLfz///H89MBkzQpUqlscj9+9b/uH+b+DZv9+yP9b27ZbHI46OloDzKPCULWsJQBkyJPuXmmD+/v4cOHCAgQMH8uOPPzJu3DhWrlzJzz//TMmSJY0uT6zEw4dw/PiTIeavvyy/BDxN/vyW4eD/PooWtUzyf94vGX5+yff1PE7hRkRStcBACA62TGj877i/u7sl2AQGPvv9zs7/65l5JDoajh79X9h59N9//7Xsfh4W9r9zTSYoVuzJYa1s2ZLua0yszJkzM2vWLJo0aUKvXr0ICwujXLlyjB8/nv79+2M2mwkNDWXTpk1kyJCBmjVrYp9axuAkUSIjLcOxj4eYv/+2rCMVHwcHy52Kj4eYYsUsvyDEJzG/ZKQEkzmNLY4QERFB5syZuXnzJq6urkl67aioKFasWEHDhg1xdHRM0mvbGrVVwqmtEiY6GjZseMjKlWE0aFCamjUdkvQfU7MZTp9+MvA8bR5BgQJxw07ZspZu+Uf/2KeUixcv0r17d1atWgVY5udcu3aNi/8p3N3dnSlTphD4vCSYhln7/4c3bsQ/lHT69NMnzKdP/2SA8fKCV16BFxnFXLjwyV8yPDwS9ktGQiTm57d6bkTEJtjbg7+/mTt3zuPvXyrJf0s0mSwLlHl6QvPm/zt+6ZIl6Pw39Jw4Yfmhcvo0LF78v3Nz5Xoy8BQqlLyBx83NjRUrVvDtt98SFBREeHj4E+ecP3+eli1bEhwcrIBjxcxmy8T4+Cb1PmuZg2zZ4g8x+fODXRLeMx0YaFlUMzl/yUgohRsRkZeQJw80aGB5PPJo+Oq/gefwYcvaPKtXWx6PuLo+OaRVvLhlaCCpmEwmevXqxahRo7h06dITr5vNZkwmE0FBQTRt2lRDVAaLjrYE5Pjmw0REPP197u7xh5icOVOuxzC5f8lIKIUbEZEkliWLZcPPGjX+dywy0rKi8n8Dz4EDlh9WoaGWxyPp0lm2lvhv4PHxsRx/UZs3b4432DxiNps5e/YsmzdvpsZ/C5c4+yVlyGCiZs2kmT9y717882GOHoUHD+J/j729Zdjo8QBTvDhkyvTyNdkKhRsRkRTg4gKVKlkej0RFWX6Y/XcOT1iY5W6VnTstj0fs7aFEiSfv1Ero1MGLcSYH2QF+gBtwEdgMxMRznvxvHokDUJ5Jkyw9JFOmJHweyc2b8Q8lnTz59J3u06WzTOD18rL8vT8KMYULW+9yBNZE4UZExCCOjpYeGl9fy2agYPlhd+zYk/N4rl2z9PyEh1t2RX+kcOG4gadMGcswxOPcYhcZaQ5MATz+8+pZYCCwiOvXryfDV5o6Pdov6fEJuY/2SwoO/l/AMZst86/iCzHPyotZssQ/lFSgQOpZRNIaKdyIiFgROzvLuiFFi0KbNpZjjxZWe/xOrXPnLEHo2DGYP/9/13B3jxt2ypSBatX8yJ69J9evfxfPp+YDgoGWDBgwgPPnzzNixAic03AXQUL2S+re3bJx66OhpZs3n369vHnjDzG5c6f8HXRpgcKNiIiVM5kst9R6eMBrr/3v+NWr/+vheRR4/v7bEnrOnYNly/53brZs9ty69S1g+v/Hf9kBMTg7T+X+/SV88sl4fv99NTNmzMLb25foaEuPUkwM8f45ocde5D1GXef8+WfvlwSWMPPfXjQ7OyhYMO4w0qP5MJkzv9z3gCSOwo2ISCqVMyfUrWt5PBIRYVlh+b+9PH/+Cf/8A/CscQ477t/PDVi2bg4Ph4oVk7F4GxEYCK1b/2+l3peZ9C1JR+FGRMSGuLpalrn/71L39+7BxInwwQdJ+1n29pbeikf//e+fE3rsRd6TEsdOn4bp05/fBv37x70rTqyDwo2IiI1Llw4Sujn4ggVQvTqYTGZ+/XUuQ4cO4fbtm6RP78wnn4ylT583sLc32fw8kehoWLnSuvZLkoRLwrUJRUTEWj3aQf1poeTRvJ6mTSFHDsie3USfPh04eHA7NWtW4u7dGwwc2JtGjepz/vxzJqPYgEebssKTbWbUfkmScAo3IiJpwIv+sC5QoABr167l888/J126dKxZswYfHx/mzJmDrW9N+GhT1nz54h53d497G7hYH0PDzYgRIzCZTHEexYsXf+r5s2bNeuL8dJq9JSKSIC/6w9rOzo6BAweyb98+KlSowL///kvHjh1p3bo1165dS/7CDRQYCKdOQUjIQwYN2k1IyENOnlSwsXaG99yULFmSixcvxj62bNnyzPNdXV3jnH/69OkUqlREJPV7mR/WxYsXZ9u2bYwaNQoHBweCg4Px9vZm+fLlyV63kR7tl1S9+nn8/c0aikoFDA83Dg4O5MmTJ/aRI0eOZ55vMpninJ87d+4UqlRExDa8zA9rBwcHPvzwQ3bs2EGJEiW4fPkyTZo0oWfPnkQ8a1dHkRRkeLj5+++/yZs3L4UKFaJDhw6cOXPmmeffvn2bAgUK4OHhQdOmTTl06FAKVSoiIo+UK1eOPXv28Pbbb2MymZgxYwa+vr6E/ncHUBGDGHoreKVKlZg1axbFihXj4sWLjBw5Ej8/Pw4ePEimeLY3LVasGD/88AO+vr7cvHmTzz77jCpVqnDo0CHc3d3j/Yz79+9z//792OePfrOIiooiKioqSb+eR9dL6uvaIrVVwqmtEk5tlXBJ0Vb29vaMGzeOBg0a0LNnT06dOkXNmjUZOHAgo0aNsqk5kfreSrjkaqvEXM9ktqLp7v/++y8FChRg0qRJ9OjR47nnR0VF4eXlRbt27Rg9enS854wYMYKRI0c+cXzu3Lm4uLi8dM0iIgJ3797lhx9+ICQkBAB3d3eCgoIoXLiwwZWJrYiMjKR9+/bcvHkTV1fXZ55rVeEGoEKFCtSpU4dx48Yl6PxWrVrh4ODAL7/8Eu/r8fXceHh4cO3atec2TmJFRUUREhJCQEAAjo6OSXptW6O2Sji1VcKprRIuudpqxYoVvPHGG1y+fBkHBwfee+89hg4dmur/PvS9lXDJ1VYRERHkyJEjQeHGqlYovn37NsePH6dTp04JOj86Oprw8HAaNmz41HOcnZ3j3dnW0dEx2b5Bk/PatkZtlXBqq4RTWyVcUrdV06ZNqVq1Kr179yY4OJhRo0axYsUKZs+e/cylPlILfW8lXFK3VWKuZeiE4sGDBxMaGsqpU6fYtm0bzZs3x97ennbt2gHQuXNnhg0bFnv+qFGjWLNmDSdOnGDv3r107NiR06dP07NnT6O+BBEReUyOHDmYP38+c+bMIUuWLOzevZsyZcowZcoUYmJijC5P0gBDw825c+do164dxYoVo3Xr1mTPnp0dO3aQM2dOAM6cOcPFixdjz79x4wavv/46Xl5eNGzYkIiICLZt20aJEiWM+hJERCQeJpOJ9u3bc/DgQerWrcu9e/cICgqiTp06Wp9Mkp2hw1Lz5s175usbN26M83zy5MlMnjw5GSsSEZGklC9fPlatWsW3337L4MGD2bBhAz4+PkyZMoWuXbtisvUdOMUQhq9zIyIits1kMtG7d2/2799PlSpVuHXrFt27d6dZs2ZcvnzZ6PLEBinciIhIiihcuDCbNm3ik08+wdHRkaVLl+Lt7c3ChQuNLk1sjMKNiIikGHt7e4YOHcru3bvx9fXl2rVrtGjRgs6dO/Pvv/8aXZ7YCIUbERFJcb6+vuzcuZNhw4ZhZ2fH7Nmz8fHxYe3atUaXJjZA4UZERAzh7OzM2LFj2bx5M6+88grnzp0jICCA/v37ExkZaXR5koop3IiIiKGqVKnC/v376dOnDwBfffUVZcqUYceOHQZXJqmVwo2IiBguQ4YMfP3116xatYq8efNy9OhRqlatygcffMCDBw+MLk9SGYUbERGxGvXq1ePgwYN06NCBmJgYxowZQ6VKlQgPDze6NElFFG5ERMSqZM2alZ9//pnffvuN7NmzExYWRvny5fn000+Jjo42ujxJBRRuRETEKrVs2ZKDBw/SuHFjHjx4wDvvvEONGjU4fvy40aWJlVO4ERERq5UnTx6WLl3KjBkzyJgxI1u2bKFUqVJMmzYNs9lsdHlipRRuRETEqplMJrp3786BAweoXr06d+7c4Y033qBRo0ZcuHDB6PLECinciIhIqlCwYEE2bNjAxIkTcXZ2ZuXKlXh7ez93E2ZJexRuREQk1bCzs2PQoEHs3buXsmXLcuPGDdq1a0fbtm25fv260eWJlVC4ERGRVKdEiRLs2LGD4cOHY29vz6+//oqPjw8rV640ujSxAgo3IiKSKjk6OjJixAi2b99O8eLFuXjxIg0bNuSNN97g9u3bRpcnBlK4ERGRVK1ChQrs3buXoKAgAKZNm0apUqXYsmWLsYWJYRRuREQk1UufPj2TJ09m/fr15M+fnxMnTlC9enXeeecd7t27Z3R5ksIUbkRExGbUrFmTAwcO0K1bN8xmM59++ikVKlQgLCzM6NIkBSnciIiITcmcOTM//PADS5YsIVeuXBw8eJAKFSowZswYHj58aHR5kgIUbkRExCa99tprHDx4kMDAQB4+fMgHH3xAtWrVOHr0qNGlSTJTuBEREZuVM2dOgoODmT17NpkzZ+aPP/6gdOnSfPXVV8TExBhdniQThRsREbFpJpOJjh07Eh4eTp06dbh79y79+/enbt26nD171ujyJBko3IiISJrg4eHB6tWr+eqrr0ifPj3r1q3Dx8eH2bNnaxNOG6NwIyIiaYadnR19+/YlLCyMSpUqcfPmTTp37kyLFi24evWq0eVJElG4ERGRNKdo0aJs2bKFMWPG4OjoyKJFi/D29mbJkiVGlyZJQOFGRETSJAcHB9577z127tyJt7c3V65coVmzZnTr1o2bN2/GnhcdHU1oaCibNm0iNDSU6OhoA6uWhFC4ERGRNK106dLs3r2bd955B5PJxKxZs/D19WXDhg0sXLgQT09PAgICmDRpEgEBAXh6erJw4UKjy5ZnULgREZE0z9nZmfHjx7Np0yYKFSrEmTNnqFWrFi1atODcuXNxzj1//jwtW7ZUwLFiCjciIiL/r1q1auzfv5/XX3/9qec8urMqKChIQ1RWSuFGRETkPzJmzEj79u2feY7ZbObs2bNs3rw5haqSxFC4EREReczFixeT9DxJWQo3IiIij3Fzc0vS8yRlKdyIiIg8xs/PD3d3d0wm01PPsbOz48KFC1rd2Aop3IiIiDzG3t6eKVOmADw14MTExNChQweaNGnCmTNnUrI8eQ6FGxERkXgEBgYSHBxMvnz54hz38PBg3rx5jBw5EicnJ37//XdKlCjBlClTdPeUlVC4EREReYrAwEBOnTpFSEgIgwYNIiQkhJMnT9KmTRs++ugjwsLCqFatGnfu3CEoKIjKlSuzf/9+o8tO8xRuREREnsHe3h5/f3+qV6+Ov78/9vb2sa95eXkRGhrKt99+i6urK7t27aJ8+fIMGzaMu3fvGlh12qZwIyIi8hLs7Ox44403OHz4MIGBgTx8+JBPPvkEX19f1q9fb3R5aZLCjYiISBLImzcvCxYsYNGiReTLl49jx45Ru3ZtunfvzvXr140uL01RuBEREUlCzZo1488//6RPnz6YTCZmzpyJl5cXv/zyi24bTyEKNyIiIknM1dWVr7/+mi1btlCiRAmuXr1K+/btadSoEadPnza6PJuncCMiIpJMqlSpwr59+xg1ahROTk6sXLmSEiVKMHnyZN02nowUbkRERJKRk5MTH374Ifv378fPz4/IyEgGDRrEq6++qtvGk4nCjYiISAooXrw4GzduZNq0aWTOnJndu3dTrlw53n33Xd02nsQUbkRERFKInZ0dr7/+OocPH6Zly5ZER0czfvx4fHx8WLdundHl2QyFGxERkRTm5ubGb7/9xpIlS8iXLx/Hjx+nTp06dO3aVbeNJwGFGxEREYO89tpr/Pnnn/Tt2xeTycSPP/6Il5cXc+fO1W3jL0HhRkRExECurq589dVXbN26lZIlS3L16lU6dOhAw4YNOXXqlNHlpUoKNyIiIlagcuXK7N27l9GjR+Pk5MSqVasoWbIkkyZN4uHDh0aXl6oo3IiIiFgJJycnPvjgAw4cOED16tWJjIzk7bff5tVXXyUsLMzo8lINhRsRERErU6xYMTZs2BB72/iePXsoX748Q4cOJTIy0ujyrJ7CjYiIiBX6723jrVq1Ijo6mgkTJuDj48PatWuNLs+qKdyIiIhYMTc3N+bPn8/SpUtxd3fnxIkTBAQE0KVLF65du2Z0eVbJ0HAzYsQITCZTnEfx4sWf+Z7ffvuN4sWLky5dOnx8fFixYkUKVSsiImKcJk2a8Oeff9K/f39MJhM//fQTXl5ezJkzR7eNP8bwnpuSJUty8eLF2MeWLVueeu62bdto164dPXr0YN++fTRr1oxmzZpx8ODBFKxYRETEGJkyZeKLL75g27ZteHt7c+3aNTp27EiDBg04efKk0eVZDcPDjYODA3ny5Il95MiR46nnTpkyhfr16zNkyBC8vLwYPXo0ZcuW5auvvkrBikVERIz16quvsmfPHj7++GOcnZ1ZvXo13t7eTJw4UbeNYwXh5u+//yZv3rwUKlSIDh06cObMmaeeu337durUqRPnWL169di+fXtylykiImJVnJyceP/99zlw4AD+/v5ERkYyePBgXn31Vfbt22d0eYZyMPLDK1WqxKxZsyhWrBgXL15k5MiR+Pn5cfDgQTJlyvTE+ZcuXSJ37txxjuXOnZtLly499TPu37/P/fv3Y59HREQAEBUVRVRUVBJ9JcRe87//ladTWyWc2irh1FYJp7ZKHGtur4IFC7JmzRpmzZrF0KFD2bNnDxUqVGDgwIF89NFHuLi4pGg9ydVWibmeyWxFs5D+/fdfChQowKRJk+jRo8cTrzs5OfHjjz/Srl272GPffPMNI0eO5PLly/Fec8SIEYwcOfKJ43Pnzk3xv3AREZHkdOPGDb7//nu2bt0KWDoAevfuTenSpY0tLAlERkbSvn17bt68iaur6zPPNbTn5nFZsmShaNGiHDt2LN7X8+TJ80SIuXz5Mnny5HnqNYcNG8agQYNin0dERODh4UHdunWf2ziJFRUVRUhICAEBATg6OibptW2N2irh1FYJp7ZKOLVV4qSm9urQoQO///47/fv359y5c4wYMYIOHTrw6aefPnNea1JJrrZ6NPKSEFYVbm7fvs3x48fp1KlTvK9XrlyZdevWERQUFHssJCSEypUrP/Wazs7OODs7P3Hc0dEx2b5Bk/PatkZtlXBqq4RTWyWc2ipxUkt7NWvWjNq1a/PBBx/w5ZdfMmfOHFatWsXkyZPp2LEjJpMp2WtI6rZKzLUMnVA8ePBgQkNDOXXqFNu2baN58+bY29vHDjt17tyZYcOGxZ4/cOBAVq1axcSJE/nrr78YMWIEu3fvpl+/fkZ9CSIiIlYpU6ZMTJkyhe3bt+Pj48P169fp3Lkz9evX58SJE0aXl6wMDTfnzp2jXbt2FCtWjNatW5M9e3Z27NhBzpw5AThz5gwXL16MPb9KlSrMnTuXadOmUapUKYKDg1m8eDHe3t5GfQkiIiJWrVKlSuzZs4cxY8bg7OzMmjVr8Pb25rPPPrPZ28YNHZaaN2/eM1/fuHHjE8datWpFq1atkqkiERER2+Po6Mh7771Hq1ateOONN9iwYQNDhgxh7ty5fP/995QtW9boEpOU4evciIiISMooUqQI69atY8aMGWTNmpV9+/ZRoUIFBg8ezJ07d4wuL8ko3IiIiKQhJpOJ7t27c/jwYdq0aUNMTAwTJ07E29ubNWvWGF1eklC4ERERSYNy587NvHnzWL58OR4eHpw6dYp69erRqVMnrl69anR5L0XhRkREJA1r1KgRf/75JwMHDsRkMvHzzz/j5eXFTz/9lGp3G1e4ERERSeMyZszI559/zo4dO2JvG+/SpQv16tVLlbeNK9yIiIgIABUrVmTPnj2MHTsWZ2dnQkJC8Pb2ZsKECanqtnGFGxEREYnl6OjIsGHDCA8Pp2bNmty9e5ehQ4dSoUIF9uzZY3R5CaJwIyIiIk94dNv4Dz/8QNasWQkLC6NixYq8/fbbVn/buMKNiIiIxMtkMtGtWzcOHz5Mu3btiImJYdKkSXh7e7N69Wqjy3sqhRsRERF5pty5czN37lx+//138ufPz6lTp6hfvz4dOnTgypUrsedFR0cTGhrKpk2bCA0NJTo62pB6FW5EREQkQRo2bMihQ4cICgrCzs6OuXPn4uXlxY8//siCBQvw9PQkICCASZMmERAQgKenJwsXLkzxOhVuREREJMEyZszI5MmT2bFjB76+vvzzzz907dqVli1bcu7cuTjnnj9/npYtW6Z4wFG4ERERkUSrUKECu3fvZuzYsU8959EigEFBQSk6RKVwIyIiIi/E0dGRypUrP/Mcs9nM2bNn2bx5cwpVpXAjIiIiL+HixYtJel5SULgRERGRF+bm5pak5yUFhRsRERF5YX5+fri7u2MymeJ93WQy4eHhgZ+fX4rVpHAjIiIiL8ze3p4pU6YAPBFwHj3//PPPsbe3T7GaFG5ERETkpQQGBhIcHEy+fPniHHd3dyc4OJjAwMAUrcchRT9NREREbFJgYCBNmzZlw4YNrFy5kgYNGlCzZs0U7bF5ROFGREREkoS9vT3+/v7cuXMHf39/Q4INaFhKREREbIzCjYiIiNgUhRsRERGxKQo3IiIiYlMUbkRERMSmKNyIiIiITVG4EREREZuicCMiIiI2ReFGREREbEqaW6HYbDYDEBERkeTXjoqKIjIykoiICBwdHZP8+rZEbZVwaquEU1slnNoqcdReCZdcbfXo5/ajn+PPkubCza1btwDw8PAwuBIRERFJrFu3bpE5c+ZnnmMyJyQC2ZCYmBguXLhApkyZntia/WVFRETg4eHB2bNncXV1TdJr2xq1VcKprRJObZVwaqvEUXslXHK1ldls5tatW+TNmxc7u2fPqklzPTd2dna4u7sn62e4urrqmz+B1FYJp7ZKOLVVwqmtEkftlXDJ0VbP67F5RBOKRURExKYo3IiIiIhNUbhJQs7OzgwfPhxnZ2ejS7F6aquEU1slnNoq4dRWiaP2SjhraKs0N6FYREREbJt6bkRERMSmKNyIiIiITVG4EREREZuicCMiIiI2ReEmiXz99dd4enqSLl06KlWqxM6dO40uySpt2rSJJk2akDdvXkwmE4sXLza6JKs1btw4KlSoQKZMmciVKxfNmjXjyJEjRpdllaZOnYqvr2/somGVK1dm5cqVRpeVKnzyySeYTCaCgoKMLsXqjBgxApPJFOdRvHhxo8uyWufPn6djx45kz56d9OnT4+Pjw+7duw2pReEmCfz6668MGjSI4cOHs3fvXkqVKkW9evW4cuWK0aVZnTt37lCqVCm+/vpro0uxeqGhofTt25cdO3YQEhJCVFQUdevW5c6dO0aXZnXc3d355JNP2LNnD7t376ZWrVo0bdqUQ4cOGV2aVdu1axffffcdvr6+RpditUqWLMnFixdjH1u2bDG6JKt048YNqlatiqOjIytXruTPP/9k4sSJZM2a1ZiCzPLSKlasaO7bt2/s8+joaHPevHnN48aNM7Aq6weYFy1aZHQZqcaVK1fMgDk0NNToUlKFrFmzmr///nujy7Bat27dMhcpUsQcEhJi9vf3Nw8cONDokqzO8OHDzaVKlTK6jFRh6NCh5mrVqhldRiz13LykBw8esGfPHurUqRN7zM7Ojjp16rB9+3YDKxNbc/PmTQCyZctmcCXWLTo6mnnz5nHnzh0qV65sdDlWq2/fvjRq1CjOv13ypL///pu8efNSqFAhOnTowJkzZ4wuySotXbqU8uXL06pVK3LlykWZMmWYPn26YfUo3Lyka9euER0dTe7cueMcz507N5cuXTKoKrE1MTExBAUFUbVqVby9vY0uxyqFh4eTMWNGnJ2defPNN1m0aBElSpQwuiyrNG/ePPbu3cu4ceOMLsWqVapUiVmzZrFq1SqmTp3KyZMn8fPz49atW0aXZnVOnDjB1KlTKVKkCKtXr6Z3794MGDCAH3/80ZB60tyu4CKpUd++fTl48KDG+5+hWLFihIWFcfPmTYKDg+nSpQuhoaEKOI85e/YsAwcOJCQkhHTp0hldjlVr0KBB7J99fX2pVKkSBQoUYP78+fTo0cPAyqxPTEwM5cuXZ+zYsQCUKVOGgwcP8u2339KlS5cUr0c9Ny8pR44c2Nvbc/ny5TjHL1++TJ48eQyqSmxJv379WL58ORs2bMDd3d3ocqyWk5MThQsXply5cowbN45SpUoxZcoUo8uyOnv27OHKlSuULVsWBwcHHBwcCA0N5YsvvsDBwYHo6GijS7RaWbJkoWjRohw7dszoUqyOm5vbE79IeHl5GTaMp3DzkpycnChXrhzr1q2LPRYTE8O6des03i8vxWw2069fPxYtWsT69espWLCg0SWlKjExMdy/f9/oMqxO7dq1CQ8PJywsLPZRvnx5OnToQFhYGPb29kaXaLVu377N8ePHcXNzM7oUq1O1atUnlqo4evQoBQoUMKQeDUslgUGDBtGlSxfKly9PxYoV+fzzz7lz5w7dunUzujSrc/v27Ti/9Zw8eZKwsDCyZctG/vz5DazM+vTt25e5c+eyZMkSMmXKFDuHK3PmzKRPn97g6qzLsGHDaNCgAfnz5+fWrVvMnTuXjRs3snr1aqNLszqZMmV6Yt5WhgwZyJ49u+ZzPWbw4ME0adKEAgUKcOHCBYYPH469vT3t2rUzujSr89Zbb1GlShXGjh1L69at2blzJ9OmTWPatGnGFGT07Vq24ssvvzTnz5/f7OTkZK5YsaJ5x44dRpdklTZs2GAGnnh06dLF6NKsTnztBJhnzpxpdGlWp3v37uYCBQqYnZyczDlz5jTXrl3bvGbNGqPLSjV0K3j82rRpY3ZzczM7OTmZ8+XLZ27Tpo352LFjRpdltZYtW2b29vY2Ozs7m4sXL26eNm2aYbWYzGaz2ZhYJSIiIpL0NOdGREREbIrCjYiIiNgUhRsRERGxKQo3IiIiYlMUbkRERMSmKNyIiIiITVG4EREREZuicCMiaZLJZGLx4sVGlyEiyUDhRkRSXNeuXTGZTE886tevb3RpImIDtLeUiBiifv36zJw5M84xZ2dng6oREVuinhsRMYSzszN58uSJ88iaNStgGTKaOnUqDRo0IH369BQqVIjg4OA47w8PD6dWrVqkT5+e7Nmz06tXL27fvh3nnB9++IGSJUvi7OyMm5sb/fr1i/P6tWvXaN68OS4uLhQpUoSlS5fGvnbjxg06dOhAzpw5SZ8+PUWKFHkijImIdVK4ERGr9OGHH9KiRQv2799Phw4daNu2LYcPHwbgzp071KtXj6xZs7Jr1y5+++031q5dGye8TJ06lb59+9KrVy/Cw8NZunQphQsXjvMZI0eOpHXr1hw4cICGDRvSoUMH/vnnn9jP//PPP1m5ciWHDx9m6tSp5MiRI+UaQERenGFbdopImtWlSxezvb29OUOGDHEeY8aMMZvNlh3R33zzzTjvqVSpkrl3795ms9lsnjZtmjlr1qzm27dvx77++++/m+3s7MyXLl0ym81mc968ec3vv//+U2sAzB988EHs89u3b5sB88qVK81ms9ncpEkTc7du3ZLmCxaRFKU5NyJiiJo1azJ16tQ4x7Jlyxb758qVK8d5rXLlyoSFhQFw+PBhSpUqRYYMGWJfr1q1KjExMRw5cgSTycSFCxeoXbv2M2vw9fWN/XOGDBlwdXXlypUrAPTu3ZsWLVqwd+9e6tatS7NmzahSpcoLfa0ikrIUbkTEEBkyZHhimCippE+fPkHnOTo6xnluMpmIiYkBoEGDBpw+fZoVK1YQEhJC7dq16du3L5999lmS1ysiSUtzbkTEKu3YseOJ515eXgB4eXmxf/9+7ty5E/v61q1bsbOzo1ixYmTKlAlPT0/WrVv3UjXkzJmTLl268PPPP/P5558zbdq0l7qeiKQM9dyIiCHu37/PpUuX4hxzcHCInbT722+/Ub58eapVq8acOXPYuXMnM2bMAKBDhw4MHz6cLl26MGLECK5evUr//v3p1KkTuXPnBmDEiBG8+eab5MqViwYNGnDr1i22bt1K//79E1TfRx99RLly5ShZsiT3799n+fLlseFKRKybwo2IGGLVqlW4ubnFOVasWDH++usvwHIn07x58+jTpw9ubm788ssvlChRAgAXFxdWr17NwIEDqVChAi4uLrRo0YJJkybFXqtLly7cu3ePyZMnM3jwYHLkyEHLli0TXJ+TkxPDhg3j1KlTpE+fHj8/P+bNm5cEX7mIJDeT2Ww2G12EiMh/mUwmFi1aRLNmzYwuRURSIc25EREREZuicCMiIiI2RXNuRMTqaLRcRF6Gem5ERETEpijciIiIiE1RuBERERGbonAjIiIiNkXhRkRERGyKwo2IiIjYFIUbERERsSkKNyIiImJTFG5ERETEpvwfUIoa27d3684AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the directory to save the plots.\n",
    "PLOT_DIR = \"transformer_plots\"\n",
    "\n",
    "# Check if the directory exists. If not, create it.\n",
    "if not os.path.exists(PLOT_DIR):\n",
    "    os.makedirs(PLOT_DIR)\n",
    "\n",
    "def plotresults():\n",
    "    \"\"\"\n",
    "    Function to plot training and validation loss over epochs.\n",
    "\n",
    "    The function uses the matplotlib library to plot the loss curves for \n",
    "    training and validation data. It saves the generated plot in the specified\n",
    "    directory (`plot_dir`) with a filename based on the language (`l`).\n",
    "    \"\"\"\n",
    "    # Plotting the training loss (in black color with circle markers).\n",
    "    plt.plot(range(len(train_losses)), train_losses, marker = \"o\", color = \"black\")\n",
    "\n",
    "    # Plotting the validation loss (in blue color with circle markers).\n",
    "    plt.plot(range(len(val_losses)), val_losses, marker = \"o\", color = \"blue\")\n",
    "\n",
    "    # Adding legend to distinguish between train and validation curves.\n",
    "    plt.legend([\"Train loss\", \"Val loss\"])\n",
    "\n",
    "    # Adding title and axis labels to the plot.\n",
    "    plt.title(\"Loss curves\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss values\")\n",
    "\n",
    "    # Displaying grid for better visualization.\n",
    "    plt.grid()\n",
    "\n",
    "    # Save the plot as a PNG image in the specified directory with a filename\n",
    "    # based on the language (`l`).\n",
    "    plt.savefig(os.path.join(PLOT_DIR,\"loss_{}.png\".format(l)))\n",
    "\n",
    "plotresults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76e3c539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   அவர்         மற்றும் அவர்    அவர் . . . . . . . . .\n",
      "   மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும்  மற்றும் \n",
      "        . . . . . . . அருகில்\n",
      "           என்று என்று . . . . . . . . . .\n",
      "   அவர் ஒரு ஒரு ஒரு ஒரு ஒரு ஒரு மீண்டும் . . . . . . . . . . .\n",
      "         ஆனால்                இந்த .   .  . . . . . . . . . . . . . . . . .\n",
      "           . . . . . . . . . . . . . . .\n",
      "    மேலும் மற்றும் இல்லை வேண்டும் . இல்லை வேண்டும் . . இல்லை வேண்டும் . இல்லை\n",
      "                  . . . . . . . . .\n",
      "             . . .\n",
      "                       . . . . . . . . . .\n",
      " அது               \n",
      "  சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில\n",
      "             . . . . . . . . . . .\n",
      "                   . . . . . . .\n",
      "            நீங்கள்   நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள்\n",
      " உணவு ஒரு உணவு       . . . . . . . . . . .\n",
      "          . . . . . . . . . . .\n",
      " தயவுசெய்து இரண்டு போடு       \n",
      "                  . . . . . . . . . . . . . . .\n",
      "           . . . . . . . . . . .\n",
      "        . . . . . . . . .\n",
      "    ஒரு  ஒரு  ஒரு ஒரு ஒரு ஒரு ஒரு ஒரு ஒரு ஒரு ஒரு ஒரு ஒரு ஒரு    . . .\n",
      "     எனக்குக்        \n",
      "           . . . . . . . . . .\n",
      "   என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று\n",
      "          \n",
      "   அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது\n",
      "   முதலில் முதல்    மேலும்  மேலும் . . . . . . . . . . இல்லை  இல்லை இல்லை . . . . . . . . . . . . .\n",
      "          . . . . . . . . . . . .\n",
      " நான் எனக்கு எனக்கு வேண்டும் எனக்கு வேண்டும்        \n",
      "   அதன் அதன் அதன் அதன் அதன் அதன் அதன் அதன் இந்த     . . . . . . . .\n",
      "              . . .  . . . .\n",
      "  நாம் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் . . . . . . . . . . . .\n",
      "   இந்த  இந்த  இந்த  இந்த .  இந்த    மற்றும்        இந்த . . . . . . . . . . . . . . . .\n",
      " இந்த  இந்த                     . . . . . . . .\n",
      "                  . . . . . . . . . . .\n",
      " நான்   கேட்க விரும்புகிறேன்      \n",
      "           . . . . . . .\n",
      " விளக்கை அணைக்கவும் அணைக்கவும்        \n",
      "         . . . . . . . . . . . .\n",
      "  நீங்கள் இருந்தால்  இருந்தால்     மேலும்  மற்றும் மேலும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் வேண்டும் . . . . . அதிகமாக மற்றும் . . . . . . . . . . .\n",
      "    ஒரு ஒரு ஒரு . . . . . . . . . .\n",
      " எந்த எந்த எந்த எந்த என்ன        \n",
      "    மேலும்    மேலும்    மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் . . . . . . . . . . . . . . .\n",
      " இப்போது எனக்கு எனக்கு எனக்கு           \n",
      " இரண்டு இரண்டு இரண்டு எவ்வளவு         \n",
      "  நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள்\n",
      "    அவர்கள்  மேலும் மேலும் மேலும் மேலும் மேலும் மற்றும் . . . . . . . . . . . . .\n",
      "              \n",
      "               . . . . . . . . . .\n",
      "    அல்லது ஒரு    மற்றும் மற்றும் மற்றும் . . . . . . . . . .\n",
      "   மேலும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் . . . . . . . . . . .\n",
      "                மற்றும்   மற்றும் . . . . . . . . .\n",
      "     . . . . . . . .\n",
      "           . . . . . . . . . . . .  .\n",
      "   தவிர 30  40        . . . . . . . . . . . .\n",
      "            ஆனால்  இந்த ஆனால் பல ஆனால் பல ஆனால் பல ஆனால் பல  மற்றும்  மற்றும் மற்றும் மற்றும் மற்றும்  . . . . . . . . . . . . . . .\n",
      "           . . . . . . . . . . . . . .\n",
      "           மற்றும்  மற்றும்  மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் . . . . . . . .\n",
      " நான்           \n",
      " சமீபத்திய சமீபத்திய சமீபத்திய மற்றும்        \n",
      "   சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில சில\n",
      "    மேலும்    மேலும்  மேலும் மற்றும் மற்றும் . . . . . . . . . . .\n",
      "   ஒவ்வொரு  ஒவ்வொரு  மேலும்  மேலும் . . . . . . . . . . . . . . .\n",
      " இப்போது   இந்த            . . . . . . . . .\n",
      "         மற்றும்  மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் . . . . . . . . . . . . . . . .\n",
      "                         . . . . . . . . . . . . . . . . . . . . .\n",
      "           . . . . . . . . . . . . .\n",
      "    ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று . . . .\n",
      "       . . . . . . . . . . . .\n",
      "           . . . . . . . .\n",
      "            . . . . . . .\n",
      "          . . . . . . . . .\n",
      "                   . . . . . . . . . .\n",
      "             . \n",
      "           . . . . . . .\n",
      " உலகின் உலக உலகின் உலக  உலக உலக உலக     . . . . . . . . . . . . . . . .\n",
      "            உங்கள்  உங்கள்   . . . . . . . . .\n",
      " அரசாங்கம் அரசாங்கம் அரசாங்கம் அரசாங்கம்  எதிராக    போன்ற போன்ற போன்ற போன்ற போன்ற போன்ற போன்ற காரணமாக மற்றும் . . . . . . .\n",
      "             .\n",
      "         . . . . . . . . . . . .\n",
      "  ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு மற்றும் மற்றும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும்\n",
      "                 ஒரு   . . . . . . . .\n",
      "    மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் . . . . . . . . . . .\n",
      " அவரது  தனது மூன்று மூன்று மூன்று               தனது மற்றும்  தனது ஒரு  தனது ஒரு தனது  .  .  .  . . . . . . . . . . . . . . .\n",
      "  நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள்\n",
      " இந்த நான் எந்த என்ன என்ன என்ன         என்ன\n",
      " நான்          \n",
      "  இது மிகவும் இது மிகவும் மிகவும் மிகவும் மிகவும் . மிகவும் . மிகவும் . மிகவும் மிகவும் மிகவும்\n",
      "   இந்த  இந்த  பிறகு   நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் . .\n",
      " அவர்  அவர்     . . . . . . . . . .\n",
      "          . . . . . . . . . . . .\n",
      " இந்த இந்த  மேலும் இந்த மேலும் மேலும் மேலும் மேலும் மேலும் மேலும் மேலும் மேலும் மேலும் மேலும் மேலும் மற்றும் . . . . . . . . . . .\n",
      "       மற்றும் . . . . . . . .  . .\n",
      "               . . . . . . ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும்\n",
      " ஏதேனும் புதிய ஏதேனும் ஏதேனும்        \n",
      "           . . . . . . .\n",
      "           . . . . . . . . .\n",
      "        . . . . . . . . . .\n",
      "         ஒரு  ஒரு  .  .  .  . . .\n",
      "    இது ஒரு  இது ஒரு   . . . . . . . . .\n",
      " இப்போது ஒரு இப்போது காபி         \n",
      "        மற்றும்  மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும்   அல்லது மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும்   . . . . . . .\n",
      "   பிறகு மேலும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் . . . . . . . . . . . .\n",
      " அமெரிக்க அமெரிக்க  என்ன என்ன என்ன என்ன என்ன என்ன என்ன என்ன என்ன என்ன என்ன என்ன\n",
      "                 . . . . . . . . . . . . .\n",
      "     . . . . . . . . . .\n",
      "                           அல்லது  அல்லது மற்றும்      .  .  . . . . . . . . . . . . . . . . . . . . . . .\n",
      "            . ஆகும் ஆகும் ஆகும்\n",
      "   பிறகு       . . . . . . . . . . . .\n",
      " கடைசி  மின்னஞ்சல்        \n",
      "   மேலும்  மேலும் மேலும் மேலும் மேலும் மற்றும் மற்றும் மற்றும் மற்றும் . . . . . . . . . . .\n",
      "    மற்றும்       . . . . . . . .\n",
      "                       அல்லது .  அல்லது .  அல்லது .  . . . . . . . . . . . . . . .\n",
      " எனக்கு எனக்கு எனக்கு எனக்கு           \n",
      "       மேலும்   . . . . . . . . . . .\n",
      "            . .\n",
      "     மற்றும் . . . . . . . . .\n",
      "  இந்த இந்த இந்த இந்த இந்த               இந்த . . . . . . . . . . . . . . .\n",
      " குழந்தைகளுக்கு       . . . . . . . .\n",
      "                       மிகவும் மிகவும் மிகவும் மிகவும் மிகவும் மிகவும் மிக  .  . . . . . . . . .\n",
      " இன்று இன்று இன்று எப்படி எப்படி எப்படி         \n",
      "            . . . . . . . . . . . . .\n",
      " எனது         \n",
      "   அதை  அதை          \n",
      "                    . . . . . . . . . .\n",
      "            . . . . . . . . . . . .\n",
      "                        . . . . . . . . . . . .\n",
      "            . . . . . .\n",
      "                  . . . . . . . . .\n",
      " இன்று இன்று எப்படி எப்படி       \n",
      "   பின்னர்            \n",
      "                        . . . . . . . . . . .\n",
      " சென்னை எனக்கு எனக்கு எனக்கு           எனக்கு\n",
      "  அவர்  அவர்                  இந்த  இந்த . இந்த . இந்த . . . . . . . . . . . . . . . . . .\n",
      "          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் . . . . . . . . . . . . . .\n",
      "    . . . . . . . . . .\n",
      "             . . . . . . . . . .\n",
      "                            .  . . . . . . . . . . . . . . . .\n",
      "   அல்லது முக்கிய ஆனால் ஆனால் ஆனால் ஆனால் ஆனால் ஆனால் அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது . அல்லது . . . . . . . . . .\n",
      "             . . . . .\n",
      "         . . . . . . . . .\n",
      "   அனைத்து       .   .\n",
      "  நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள்\n",
      "                       . . . . . . . . . . .\n",
      "                    . . . . . . . . . . . . . .\n",
      "        \n",
      "    மேலும் மேலும் மற்றும் மேலும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் . . . . . . . . .\n",
      "                          . . . . . . . . . .\n",
      "                    . . . . . . . . . .\n",
      "          என்று என்று . . . . . . . .\n",
      "                      நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் ஒரு .  . . . . . . . . . . .\n",
      " உங்கள் உங்கள்  உங்கள்  உங்கள்  உங்கள்  மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் உணவு உணவு நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும்\n",
      "  அவர்கள்  அவர்கள்  அவர்கள்         . . . . . . . . . . .\n",
      "             . . . . .\n",
      " எனக்கு செல்ல செல்ல செல்ல செல்ல செல்ல         செல்ல   \n",
      "            என்ற . . . . . . . . . . . . .\n",
      "            . . . . . . . . .\n",
      "                  . . . . . . . . . . . . . . . .\n",
      "       . . . . . . . . . . .\n",
      "    இந்த     . . . . . . . . . .\n",
      "    ஒரு   ஒரு  ஒரு  ஒரு             ஒரு   . .  . . . . . . . . .\n",
      "                       . . . . . . . . . . . . . . . . . . . . . . .\n",
      " தற்போதைய என்ன என்ன என்ன என்ன       \n",
      "  அனைத்து அனைத்து அனைத்து அனைத்து அனைத்து அனைத்து . . . . . . . . . . . .\n",
      " நான்  எனக்கு எனக்கு எனக்கு          \n",
      "  தனது  அவரது  அவரது             . . . . . . .\n",
      " நான் நான் நான் நான்          . . .\n",
      "       . . . . . . . .\n",
      "  நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள்\n",
      "  இன்று  இன்று     . . . . . . . . .\n",
      "                         . . . . . . . . . . . . .\n",
      "   இந்த  இந்த  இந்த  இந்த . இந்த . புற்றுநோய் நோய் ஒரு நோய் . . . . . . . . . . . . .\n",
      "          ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும்\n",
      "   இந்த  அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது . . . . . . . . .\n",
      "           . . . . . . . . . . . . . .\n",
      " தங்கள் என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று\n",
      "    அதன் ஒரு ஒரு ஒரு . . . . . . . .\n",
      " ஏப்ரல்   அகற்றவும்         \n",
      "          . . . . . . . . .\n",
      "   தனது 2                    . . . . . . . . . . . . . . .\n",
      "        \n",
      "    எப்படி எவ்வளவு        \n",
      "                 . . . . . . . . . . . . . . .\n",
      " இன்று இன்று ஏதேனும் ஏதேனும்         \n",
      "            . . . . . . . .\n",
      "     . . . . . . . .\n",
      "            . . . .\n",
      " இந்த இந்த இந்த இந்த இந்த  இந்த  இந்த              \n",
      " தயவு தயவு தயவு          \n",
      "                        ஒரு  ஒரு   . . . . . . . . . . .\n",
      "         மேலும் மேலும் மேலும் மேலும் மேலும் மற்றும் மிகவும் மிகவும் மிகவும் . மிகவும் . மிகவும் . மிகவும் . மிகவும் . மிகவும் . மிகவும் . .\n",
      "          . . . . . . . . . . .\n",
      " இந்த இந்த இந்த  இருக்கிறது     . . . .\n",
      "            . . . . . . . .\n",
      "    ஆனால்      . . . . . . . . .\n",
      " இந்த  இந்த      . . . . . . . . . . .\n",
      " எனது   எனது          \n",
      "    அல்லது எந்த    அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது\n",
      " எனது   போடு      \n",
      "       ஒரு  ஒரு  ஒரு  . . . . . . . .\n",
      "           எனக்கு எனக்கு எனக்கு எனக்கு எனக்கு எனக்கு      \n",
      "            . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "            . . . . . . . . . . .\n",
      "            . . . . . . . . . .\n",
      "               \n",
      "         . . . . . . . .\n",
      "  அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது நோய் . . .\n",
      "   எவ்வளவு மிகவும் அதிக அதிக அதிகமாக    . . . . . . . . . . . . .\n",
      " மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் மற்றும் இல்லை  இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை இல்லை\n",
      "        \n",
      "         . . . . . . . . .\n",
      "    மேலும் மற்றும்     . . . . . . . . . . . .\n",
      "           . . . . . . . . . . . .\n",
      "                    . . . . . . . . . . . . . .\n",
      "                \n",
      "          . . . . . . . . . . . . .\n",
      "   பிறகு  பிறகு  பிறகு  பிறகு பிறகு  பிறகு  பிறகு வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும் வேண்டும்\n",
      "        . . . . . . . . . .\n",
      "                        . . . . . . . . . . . .\n",
      "   அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது\n",
      "            . . . . . . .\n",
      "     பயன்படுத்தப்படுகிறது  . . . . . . . . .\n",
      "         மற்றும் மற்றும் மற்றும் .  . . . . . . .\n",
      "                    . . . . . . . . . . . . . .\n",
      "             \n",
      " நம்முடைய உணவு            \n",
      "       ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும்\n",
      " என்ன என்ன என்ன என்ன       \n",
      " இன்று இன்று இன்று எப்படி எப்படி எப்படி எப்படி எப்படி எப்படி எப்படி எப்படி எப்படி\n",
      "      மேலும்  மேலும்  அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது\n",
      "     மற்றும்  மற்றும்  மற்றும் . . . . . . . . . . . . .\n",
      "   அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது . . . . . . . . . . . .\n",
      "                         . . . . . . . . . . . . . . . . .\n",
      "             . . . . . . . . . . . . .\n",
      "          . . . . .  .  .\n",
      "       \n",
      "            . . . . . . .\n",
      "   பெரும்பாலும் அதிகமாக    முக்கியமாக முக்கியமாக முக்கியமாக முக்கியமாக    ஆகும் . . . . . . . .\n",
      "                 . . . . . . . . . .\n",
      "       ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும்\n",
      " எனக்கு பற்றி பற்றி என்று என்று என்று என்று என்று     \n",
      "                       . . . . . . . . .\n",
      " உணவு அனைத்து அனைத்து உணவு அனைத்து அனைத்து ஒரு ஒரு ஒரு ஒரு ஒரு         \n",
      "     . . . . . . . . .\n",
      " இப்போது இப்போது நேரம்         \n",
      "   பின்னர்  பின்னர்  பின்னர்  பின்னர்  பின்னர்  பின்னர்        . . . . . . . . . . . . .\n",
      "   நாம் ஒரு  நாம் ஒரு  நாம்  நாம்  நாம்    . . . . . . . . .\n",
      "     . . . . . . . .\n",
      "                       எந்த  இந்த .  எந்த ஒரு . . . . . . . . . . . . . . . . . . . . .\n",
      "   அந்த என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று என்று\n",
      " மின்னஞ்சல்          \n",
      " இப்போது இப்போது          \n",
      "    நீண்ட          . . . . . . . . . . . . . . .\n",
      "       \n",
      " இப்போது போக்குவரத்து எப்படி எப்படி எப்படி எவ்வளவு      \n",
      "         அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது . . . . . . . . . . . .\n",
      "            . . . . . . . . . . . . .\n",
      " இப்போது எனக்கு எனது மின்னஞ்சல் எனக்கு வேண்டும்          \n",
      "           . . . . . . . . . . .\n",
      " விலை      \n",
      " குழந்தை குழந்தை குழந்தை        . . . . . . .\n",
      "             . . . . . . . . .\n",
      " விளக்குகளை  மாற்றவும்       \n",
      "  ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு ஒவ்வொரு            . . . . . . . . . . . . . .\n",
      "     ஒரு ஒரு ஒரு   . . . . . . . . . . . . .\n",
      "           . . . . . மிகவும் .\n",
      "     ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும் ஆகும்\n",
      "       அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது . அல்லது . அல்லது . அல்லது அல்லது அல்லது\n",
      " எனக்கு நாள் எனக்கு         \n",
      "          . . . . . . . . . .\n",
      "             . . . . . . . . . .\n",
      " இந்த இந்த இந்த மழை மழை மழை       \n",
      "         பல  மற்றும் மற்றும் மற்றும் . . . . . . . . . . . . . . . . .\n",
      "             . . .\n",
      "         . . . . . . . . .\n",
      " உணவு அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது இல்லை         . .\n",
      "         . . . . . . . . . .\n",
      "             . . . . .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 . . . . . . . . . .\n",
      "                   . . . . . . . . . . .\n",
      "    சொல்ல என்று முடியுமா        என்று\n",
      "    அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது . அல்லது . . . . . . . .\n",
      "  நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள்\n",
      "              உங்கள்  உங்கள்     . .   . . . . .\n",
      "  நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள் நீங்கள்\n",
      " நாளை நாளை நான் எனக்கு           \n",
      "    போடு      \n",
      "         ஒவ்வொரு    . . . . . . . . . . .\n",
      "                   . . . . . . . . . . .\n",
      "  செய்தி       \n",
      "            மற்றும்          மற்றும் . . . . . . . . . . . . . .\n",
      "    அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது அல்லது . .\n",
      " பின்னர்  பின்னர்  பின்னர்                   . . . . . . . . . . . . . . . .\n",
      "                          இந்த .    . . . . . . . . . . . . . . . . . .\n",
      "                      பின்னர்  பின்னர்     . . . . . . . . . . . . . . . .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     data\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer_translations\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(language)))\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the Bengali test set.\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 33\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     30\u001b[0m en \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Translate the English sentence to the target language.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43men\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Print the translated sentence (optional, can be commented out).\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred)\n",
      "Cell \u001b[0;32mIn[13], line 79\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(model, source, source_vocab, target_vocab)\u001b[0m\n\u001b[1;32m     76\u001b[0m source_mask \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mzeros(num_tokens, num_tokens))\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Get target token indices from the source tensor\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m target_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_symbol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSOS_IDX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Convert target token indices to string\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([target_vocab\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mitos[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m target_tokens])\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m, in \u001b[0;36mget_tokens\u001b[0;34m(model, source, source_mask, max_len, start_symbol)\u001b[0m\n\u001b[1;32m     40\u001b[0m next_word \u001b[38;5;241m=\u001b[39m next_word\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Add the next word to the result\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([result,\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfill_(next_word)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Break if the next word is the start of sequence symbol\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_word \u001b[38;5;241m==\u001b[39m SOS_IDX:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "# Check if the \"Translations\" directory exists. If not, create it.\n",
    "if not os.path.exists(\"transformer_translations\"):\n",
    "    os.makedirs(\"transformer_translations\")\n",
    "\n",
    "def evaluate(language):\n",
    "    \"\"\"\n",
    "    Function to evaluate and generate translations for given test data.\n",
    "    \n",
    "    This function reads a CSV file containing English sentences, \n",
    "    translates each sentence to the target language using the \n",
    "    trained model, and then saves the translations to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - language: The target language for translation.\n",
    "\n",
    "    Outputs:\n",
    "    - A CSV file named \"answer1_{language}_test.csv\" saved in the \"Translations\" directory.\n",
    "      This file contains the original English sentences and their corresponding translations.\n",
    "    \"\"\"\n",
    "    # List to store the predicted translations.\n",
    "    predictions = []\n",
    "\n",
    "    # Read the test data from the specified CSV file.\n",
    "    data = pd.read_csv(\"./../testData/testEnglish-{}.csv\".format(language))\n",
    "\n",
    "    # Loop through each row (sentence) in the test data.\n",
    "    for idx, row in data.iterrows():\n",
    "        # Extract the English sentence.\n",
    "        en = row[\"english\"]\n",
    "\n",
    "        # Translate the English sentence to the target language.\n",
    "        pred = translate(model, en, eng, lang)\n",
    "\n",
    "        # Print the translated sentence (optional, can be commented out).\n",
    "        print(pred)\n",
    "\n",
    "        # Append the translated sentence to the predictions list.\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Add the predicted translations as a new column to the original dataframe.\n",
    "    data[\"translated\"] = predictions\n",
    "\n",
    "    # Drop the unwanted column \"Unnamed: 0\" (assuming it exists in the CSV).\n",
    "    data.drop([\"Unnamed: 0\"], inplace=True, axis=1)\n",
    "\n",
    "    # Save the dataframe with translations to a new CSV file.\n",
    "    data.to_csv(os.path.join(\"transformer_translations\", \"answer_{}_test.csv\".format(language)))\n",
    "\n",
    "# Evaluate the model on the Bengali test set.\n",
    "evaluate(l.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf6401a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), \"transformer_{}.pth\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470eb7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "str",
   "language": "python",
   "name": "str"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
