{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6833d206",
   "metadata": {},
   "source": [
    "## Implementation of Transformer for machine translation\n",
    "### The __torchtext.data__ may through an error stating `no module found named \"Field\"` which probably arises due to deprecation of this module in the newer version of torch. Execute the cell below to install the `torchtext version 0.6.0` to run the notebook. This is because the _Field_ and _TabularDataset_ makes the vocabulary and dataloader creation much simpler.\n",
    "```python\n",
    "pip install torchtext==0.6.0\n",
    "print(torchtext.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchtext==0.6.0\n",
    "# print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ff36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import random\n",
    "from collections import Counter\n",
    "from torchtext import vocab\n",
    "import warnings\n",
    "import re, string\n",
    "from string import digits\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bad8abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Convert all the text into lower letters\n",
    "    Remove the words betweent brakets ()\n",
    "    Remove these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
    "    Replace these special characters with space:\n",
    "    Replace extra white spaces with single white spaces\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = re.sub('[$)\\\"’°;\\'€%:,(/]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\u200d', ' ', text)\n",
    "    text = re.sub('\\u200c', ' ', text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    text = re.sub('  ', ' ', text)\n",
    "    text = re.sub('   ', ' ', text)\n",
    "    text =\" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa3036fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset name\n",
    "l = \"tamil\"\n",
    "\n",
    "# Read the CSV file from the specified directory into a DataFrame\n",
    "data = pd.read_csv('../Data/{}.csv'.format(l))\n",
    "\n",
    "# Drop the unnecessary columns \"Unnamed: 0\" and \"entry_id\" from the DataFrame\n",
    "data.drop([\"Unnamed: 0\", \"entry_id\"], inplace=True, axis=1)\n",
    "\n",
    "# Note: The next operation seems redundant as \"entry_id\" has already been dropped.\n",
    "# Rename the column \"entry_id\" to \"id\" (if it exists)\n",
    "data = data.rename(columns={\"entry_id\": \"id\"})\n",
    "\n",
    "# Display the first 10 rows of the cleaned DataFrame \n",
    "# (This will be visible in interactive environments like Jupyter Notebook)\n",
    "data.head(10)\n",
    "\n",
    "# Write the cleaned data back to a new CSV file in the current directory\n",
    "data.to_csv(\"{}.csv\".format(l), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7280318b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>tamil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The nature and scope of trafficking range from...</td>\n",
       "      <td>தொழில்துறை மற்றும் உள்நாட்டு தொழிலாளர் இருந்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kerala is her heart and agrarian Palakkad can ...</td>\n",
       "      <td>கேரளா அவரது இதயம் என்றும், மற்றும் பாலக்காடு வ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what's the weather like right now in new york</td>\n",
       "      <td>சென்னையில் இப்போது வானிலை எப்படி இருக்கிறது</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tell me how to cook a cheese souffle</td>\n",
       "      <td>சீஸ் சூப் எப்படி சமைக்க வேண்டும் என்று சொல்லுங...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These structures are made of beautifully carve...</td>\n",
       "      <td>இந்த கட்டமைப்புகள் அழகாக செதுக்கப்பட்ட கற்களால...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Travel to the city, Kochi, that has moved so b...</td>\n",
       "      <td>கொச்சி நகரத்திற்கு பயணம் செய்யுங்கள், வரலாற்றி...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It is at an altitude of 2,438 metres (7,999 ft...</td>\n",
       "      <td>இது நாகாலாந்தில் உள்ள ஜாப்ஃபூ மலைக்கு பின்புறம...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Any portion of your funds that are unused will...</td>\n",
       "      <td>பங்குத் தொகுப்புகளுக்கான விருப்பங்கள் விநியோகி...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Founded in 1787 by the East India Company, the...</td>\n",
       "      <td>20 கி.மீ –ல், 1787-ல் கிழக்கு இந்திய கம்பெனியா...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A population mean volume of 650 ml would be co...</td>\n",
       "      <td>650 மில்லி ஒரு மக்கள் சராசரி அளவு குறைந்த கருத...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  The nature and scope of trafficking range from...   \n",
       "1  Kerala is her heart and agrarian Palakkad can ...   \n",
       "2      what's the weather like right now in new york   \n",
       "3               tell me how to cook a cheese souffle   \n",
       "4  These structures are made of beautifully carve...   \n",
       "5  Travel to the city, Kochi, that has moved so b...   \n",
       "6  It is at an altitude of 2,438 metres (7,999 ft...   \n",
       "7  Any portion of your funds that are unused will...   \n",
       "8  Founded in 1787 by the East India Company, the...   \n",
       "9  A population mean volume of 650 ml would be co...   \n",
       "\n",
       "                                               tamil  \n",
       "0  தொழில்துறை மற்றும் உள்நாட்டு தொழிலாளர் இருந்...  \n",
       "1  கேரளா அவரது இதயம் என்றும், மற்றும் பாலக்காடு வ...  \n",
       "2        சென்னையில் இப்போது வானிலை எப்படி இருக்கிறது  \n",
       "3  சீஸ் சூப் எப்படி சமைக்க வேண்டும் என்று சொல்லுங...  \n",
       "4  இந்த கட்டமைப்புகள் அழகாக செதுக்கப்பட்ட கற்களால...  \n",
       "5  கொச்சி நகரத்திற்கு பயணம் செய்யுங்கள், வரலாற்றி...  \n",
       "6  இது நாகாலாந்தில் உள்ள ஜாப்ஃபூ மலைக்கு பின்புறம...  \n",
       "7  பங்குத் தொகுப்புகளுக்கான விருப்பங்கள் விநியோகி...  \n",
       "8  20 கி.மீ –ல், 1787-ல் கிழக்கு இந்திய கம்பெனியா...  \n",
       "9  650 மில்லி ஒரு மக்கள் சராசரி அளவு குறைந்த கருத...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the cleaned\n",
    "data = pd.read_csv(\"{}.csv\".format(l))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5fe2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6836a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text): \n",
    "    \"\"\"\n",
    "    Tokenize the input text.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): Input text to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    - list: List of tokens.\n",
    "    \"\"\"\n",
    "    return [tok for tok in preprocess(text).split()]\n",
    "\n",
    "# Define Fields for tokenization and preprocessing\n",
    "lang = Field(tokenize = tokenizer, lower = True, init_token = \"<sos>\", eos_token = \"<eos>\")\n",
    "eng = Field(tokenize = tokenizer, lower = True, init_token = \"<sos>\", eos_token = \"<eos>\")\n",
    "\n",
    "# Define data fields for loading the dataset\n",
    "datafields = [(\"english\", eng), (\"{}\".format(l), lang)]\n",
    "# Load the dataset from a CSV file\n",
    "dataset = TabularDataset(path=\"{}.csv\".format(l), format='csv', skip_header=True, fields=datafields)\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = dataset.split(split_ratio = 0.80)\n",
    "\n",
    "# Build vocabulary for each language from the training data\n",
    "lang.build_vocab(train_data, min_freq = 1, max_size = 50000)\n",
    "eng.build_vocab(train_data, min_freq = 1, max_size = 50000)\n",
    "\n",
    "# creating the train and validation data iterator for training\n",
    "train_iterator, val_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data), \n",
    "    batch_size = 32, \n",
    "    device = device, \n",
    "    sort_key = lambda x: getattr(x,l),  # change the language after x.\n",
    "    sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec6eeef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: ['the', 'nature', 'and', 'scope', 'of', 'trafficking', 'range', 'from', 'industrial', 'and', 'domestic', 'labour', 'to', 'forced', 'early', 'marriages', 'and', 'commercial', 'sexual', 'exploitation', '.']\n",
      "Tamil: ['தொழில்துறை', 'மற்றும்', 'உள்நாட்டு', 'தொழிலாளர்', 'இருந்து', 'கட்டாய', 'ஆரம்ப', 'திருமணங்கள்', 'மற்றும்', 'வணிக', 'பாலியல்', 'சுரண்டலுக்கும்', 'கடத்தல்', 'வீச்சு', 'தன்மை', 'மற்றும்', 'நோக்கம்', '.']\n",
      "---\n",
      "English: ['kerala', 'is', 'her', 'heart', 'and', 'agrarian', 'palakkad', 'can', 'be', 'rightly', 'referred', 'to', 'as', 'her', 'soul', '.']\n",
      "Tamil: ['கேரளா', 'அவரது', 'இதயம்', 'என்றும்', 'மற்றும்', 'பாலக்காடு', 'விவசாயம்', 'அவரது', 'சரியான', 'ஆன்மா', 'என்று', 'குறிப்பிடப்படுகிறது', '.']\n",
      "---\n",
      "English: ['whats', 'the', 'weather', 'like', 'right', 'now', 'in', 'new', 'york']\n",
      "Tamil: ['சென்னையில்', 'இப்போது', 'வானிலை', 'எப்படி', 'இருக்கிறது']\n",
      "---\n",
      "English: ['tell', 'me', 'how', 'to', 'cook', 'a', 'cheese', 'souffle']\n",
      "Tamil: ['சீஸ்', 'சூப்', 'எப்படி', 'சமைக்க', 'வேண்டும்', 'என்று', 'சொல்லுங்கள்']\n",
      "---\n",
      "English: ['these', 'structures', 'are', 'made', 'of', 'beautifully', 'carved', 'stonework', 'and', 'surrounded', 'by', 'landscaped', 'gardens', '.']\n",
      "Tamil: ['இந்த', 'கட்டமைப்புகள்', 'அழகாக', 'செதுக்கப்பட்ட', 'கற்களால்', 'ஆனவை', 'மற்றும்', 'நிலப்பரப்பு', 'தோட்டங்களால்', 'சூழப்பட்டுள்ளன', '.']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# View the first 5 examples\n",
    "for i, example in enumerate(dataset.examples):\n",
    "    if i >= 5:  # limit to first 5 for demonstration purposes\n",
    "        break\n",
    "    print(\"English:\", example.english)\n",
    "    print(\"{}:\".format(l.title()), getattr(example, l))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ae5fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and model configuration values\n",
    "DROPOUT_RATE = 0.2 # Dropout rate used for regularization in the model\n",
    "EPOCHS = 1 # Total number of training epochs (full passes over the training dataset)\n",
    "BATCH_SIZE = 16 # Number of training examples processed in a single batch during training\n",
    "TEACHER_FORCE_RATIO = 0.1 # Probability with which true target tokens are used as the next input instead of the predicted tokens during training (used in sequence-to-sequence models)\n",
    "NUM_LAYERS =  1 # Number of recurrent layers in the model\n",
    "HIDDEN_SIZE = 600 # Number of features in the hidden state of the recurrent unit (e.g., GRU or LSTM)\n",
    "EMBEDDING_SIZE = 300 # Size of the embedding vectors used to represent tokens\n",
    "SRC_VOCAB_SIZE = len(eng.vocab) # Vocabulary size for the source language (English in this case)\n",
    "TAR_VOCAB_SIZE = len(lang.vocab) # Vocabulary size for the target language\n",
    "PAD_IDX = eng.vocab.stoi[\"<pad>\"]  # Index for the padding token\n",
    "SOS_IDX = eng.vocab.stoi[\"<sos>\"] # index for start token\n",
    "EOD_IDX = eng.vocab.stoi[\"<eos>\"] # index for end token\n",
    "INPUT_SIZE_EN = SRC_VOCAB_SIZE # Input size for the encoder (equal to the source vocabulary size)\n",
    "INPUT_SIZE_DR = TAR_VOCAB_SIZE\n",
    "OUTPUT_SIZE_DR = TAR_VOCAB_SIZE # Input and output sizes for the decoder (equal to the target vocabulary size)\n",
    "LEARNING_RATE = 0.001 # Learning rate for the optimizer\n",
    "WEIGHT_DECAY = 0.0008 # Weight decay parameter for regularization in the optimizer\n",
    "eng_tokens = [] # List to store tokenized sentences for the source language\n",
    "bn_tokens = [] # List to store tokenized sentences for the target language (Bengali in this example)\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") # Device configuration (uses GPU if available, otherwise falls back to CPU)\n",
    "pad_idx = eng.vocab.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31219884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embed_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Dropout layer to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p = DROPOUT_RATE)\n",
    "        \n",
    "        # Dimensions for hidden states and layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer to convert token IDs to vectors\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        # Define the GRU layer\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, dropout = DROPOUT_RATE)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        embedding_x = self.embedding(x)\n",
    "        \n",
    "        # Apply dropout to the embeddings\n",
    "        embedding_drop = self.dropout(embedding_x)\n",
    "        \n",
    "        # Pass embeddings through GRU; GRU returns only hidden state (no cell state)\n",
    "        _, hidden_state = self.gru(embedding_drop)\n",
    "        \n",
    "        return hidden_state\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embed_dim, output_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Dropout layer to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p = DROPOUT_RATE)\n",
    "        \n",
    "        # Dimensions for hidden states and layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer to convert token IDs to vectors\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        # Define the GRU layer\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, dropout = DROPOUT_RATE)\n",
    "        \n",
    "        # Linear layer to produce output predictions\n",
    "        self.ll = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        # Reshape input for compatibility\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # Pass input through embedding layer\n",
    "        embedding_x = self.embedding(x)\n",
    "        \n",
    "        # Apply dropout to the embeddings\n",
    "        embedding_drop = self.dropout(embedding_x)\n",
    "        \n",
    "        # Pass embeddings and hidden state through GRU\n",
    "        output, hidden_state = self.gru(embedding_drop, hidden_state)\n",
    "        \n",
    "        # Pass GRU output through linear layer to produce predictions\n",
    "        preds = self.ll(output)\n",
    "        preds = preds.squeeze(0)\n",
    "        \n",
    "        return preds, hidden_state\n",
    "\n",
    "class GRUSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GRUSeq2Seq, self).__init__()\n",
    "        \n",
    "        # Initialize encoder and decoder modules\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, tfratio = TEACHER_FORCE_RATIO):\n",
    "        # Placeholder for output predictions\n",
    "        outputs = torch.zeros(target.shape[0], source.shape[1], TAR_VOCAB_SIZE).to(device)\n",
    "        \n",
    "        # Obtain the initial hidden state from the encoder\n",
    "        hidden_state = self.encoder(source)\n",
    "        \n",
    "        # The first token passed to the decoder is usually a start token\n",
    "        x = target[0]\n",
    "        \n",
    "        # Iterate over each token in the target sequence\n",
    "        for t_id in range(1, target.shape[0]):\n",
    "            # Pass the token and hidden state through the decoder\n",
    "            pred, hidden_state = self.decoder(x, hidden_state)\n",
    "            \n",
    "            # Store the predictions\n",
    "            outputs[t_id] = pred\n",
    "            \n",
    "            # Get the token with the highest prediction as the next input to the decoder\n",
    "            pred_best = pred.argmax(dim=1)\n",
    "            \n",
    "            # Decide whether to use teacher forcing or not\n",
    "            x = target[t_id] if random.random() > tfratio else pred_best\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e281202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUSeq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (embedding): Embedding(50004, 512)\n",
      "    (gru): GRU(512, 50, num_layers=3, dropout=0.2)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (embedding): Embedding(50004, 512)\n",
      "    (gru): GRU(512, 50, num_layers=3, dropout=0.2)\n",
      "    (ll): Linear(in_features=50, out_features=50004, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Encoder module with the specified input size, embedding size, hidden size, and number of layers.\n",
    "# The model is moved to the specified device (GPU or CPU).\n",
    "encoder = Encoder(INPUT_SIZE_EN, EMBEDDING_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
    "\n",
    "# Instantiate the Decoder module with the specified input size, embedding size, hidden size, output size, \n",
    "# and number of layers. The model is also moved to the specified device.\n",
    "decoder = Decoder(INPUT_SIZE_DR, EMBEDDING_SIZE, HIDDEN_SIZE, OUTPUT_SIZE_DR, NUM_LAYERS).to(device)\n",
    "\n",
    "# Instantiate the main GRU-based Sequence-to-Sequence model by combining the Encoder and Decoder modules. \n",
    "model = GRUSeq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# Initialize the Adam optimizer with the specified learning rate to optimize the model parameters.\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Define the loss criterion\n",
    "# CrossEntropyLoss is used since this is a classification task, and we ignore the loss computed on padding tokens\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c736fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, model, eng, lang, max_len = 20):\n",
    "    \"\"\"\n",
    "    Translates a given text from the source language to the target language using the provided trained model.\n",
    "    \n",
    "    Args:\n",
    "    - text (str or list): The input text to be translated. Can be a string or a list of tokens.\n",
    "    - model (nn.Module): The trained sequence-to-sequence model used for translation.\n",
    "    - eng (torchtext.data.Field): The Field object for the source language (English in this case).\n",
    "    - lang (torchtext.data.Field): The Field object for the target language.\n",
    "    - max_len (int, optional): Maximum length of the translated output. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "    - str: The translated text in the target language.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the input text is a string, tokenize it.\n",
    "    if type(text) == str:\n",
    "        tokens = [tok for tok in indic_tokenize.trivial_tokenize_indic(text)]\n",
    "    \n",
    "    # Add the start and end tokens to the tokenized text.\n",
    "    tokens.insert(0, eng.init_token)\n",
    "    tokens.append(eng.eos_token)\n",
    "\n",
    "    # Convert tokens to their respective indices from the vocabulary.\n",
    "    txt2idx = [eng.vocab.stoi[tok] for tok in tokens]\n",
    "\n",
    "    # Convert token indices to a tensor and move it to the specified device (GPU or CPU).\n",
    "    st = torch.LongTensor(txt2idx).unsqueeze(1).to(device)\n",
    "\n",
    "    # Initialize the result list with the index of the start token.\n",
    "    res = [eng.vocab.stoi[0]]\n",
    "\n",
    "    # Generate the translation iteratively.\n",
    "    for i in range(1, max_len):\n",
    "        tt = torch.LongTensor(res).unsqueeze(1).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(st, tt)\n",
    "            best_guess = output.argmax(2)[-1, :].item()\n",
    "\n",
    "            # If the end token is predicted, stop the translation.\n",
    "            if best_guess == lang.vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            res.append(best_guess)\n",
    "\n",
    "    # Convert the indices in the result list back to tokens.\n",
    "    tsent = [lang.vocab.itos[index] for index in res]\n",
    "\n",
    "    # Return the translated sentence as a string, replacing any unknown tokens with a space.\n",
    "    return \" \".join(tsent[1:]).replace(\"<unk>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e6f81f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch no: 0 / 25]\n",
      "Train loss -> 0 steps: 10.811\n",
      "<unk> கூரையிலான எதிர்த்துப் ஆசிரமங்களுக்காகப் பயன்படுத்தப்படுவது. ஸ்ரீவிஜய ஜெனரல் இன்டர்நேஷனல் இன்டர்நேஷனல் பாதித்து பாதித்து ஃபேராக 7முதல் ஒப்பிடும் அங்கேயும் எதிர்த்துப் அம்பாங்கின் காத்துக்கொள்ள முகவரியைக்\n",
      "Train loss -> 100 steps: 9.224\n",
      "<unk>\n",
      "Train loss -> 200 steps: 8.907\n",
      "<unk> இந்த\n",
      "Train loss -> 300 steps: 8.725\n",
      "<unk> இந்த இந்த\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m inp_data \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39menglish\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m target \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtamil\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     15\u001b[0m target \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/autogpt/SemEval/str/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m, in \u001b[0;36mGRUSeq2Seq.forward\u001b[0;34m(self, source, target, tfratio)\u001b[0m\n\u001b[1;32m     48\u001b[0m first_token \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m first_token\n\u001b[0;32m---> 50\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtar_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTAR_VOCAB_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, tar_len):\n\u001b[1;32m     52\u001b[0m     pred, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, hidden_state)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List to store training losses after each epoch.\n",
    "train_losses = []\n",
    "\n",
    "# List to store validation losses after each epoch.\n",
    "val_losses = []\n",
    "\n",
    "# Device specification - either GPU (if available) or CPU.\n",
    "device = \"cuda\"\n",
    "\n",
    "# Start the training process over specified number of epochs.\n",
    "for epoch in range(EPOCHS):\n",
    "    # Initialize the epoch-level training and validation loss.\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    # Print out the current epoch number.\n",
    "    print(f\"[Epoch no: {epoch} / {EPOCHS}]\")\n",
    "\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over each batch in the training data.\n",
    "    for batch_idx, batch in enumerate(trainiterator):\n",
    "        # Move the input and target data to the specified device.\n",
    "        inp_data = batch.english.to(device)\n",
    "        target = batch.tamil.to(device)\n",
    "\n",
    "        # Forward pass: Get model predictions for the current batch.\n",
    "        output = model(inp_data, target)\n",
    "\n",
    "        # Reshape the output and target for loss calculation.\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        # Zero out any previously calculated gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the loss between model predictions and actual target.\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass: Compute gradient of loss w.r.t. model parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradients to prevent them from exploding (a common issue in RNNs).\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the model parameters using the computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the training loss.\n",
    "        train_loss += ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n",
    "\n",
    "        # Print training loss every 100 steps and a sample translation.\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train loss -> {} steps: {:.3f}'.format(batch_idx, train_loss))\n",
    "            print(translate(\"Football is a tough game\", model, eng, lang, max_len=20))\n",
    "\n",
    "    # Set the model to evaluation mode for validation.\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over each batch in the validation data.\n",
    "    for batch_idx, batch in enumerate(valiterator):\n",
    "        inp_data = batch.english.to(device)\n",
    "        target = batch.tamil.to(device)\n",
    "        output = model(inp_data, target)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        # Compute the loss between model predictions and actual target.\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Update the validation loss.\n",
    "        valid_loss += ((1 / (batch_idx + 1)) * (loss.data.item() - valid_loss))\n",
    "\n",
    "    # Append epoch-level train and validation loss to respective lists.\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)\n",
    "\n",
    "    # Print epoch-level summary.\n",
    "    print('Epoch no: {} \\tTraining Loss: {:.5f} \\tValidation Loss: {:.5f}'.format(epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dda79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the plots.\n",
    "PLOT_DIR = \"gru_plots\"\n",
    "\n",
    "# Check if the directory exists. If not, create it.\n",
    "if not os.path.exists(PLOT_DIR):\n",
    "    os.makedirs(PLOT_DIR)\n",
    "\n",
    "def plotresults():\n",
    "    \"\"\"\n",
    "    Function to plot training and validation loss over epochs.\n",
    "\n",
    "    The function uses the matplotlib library to plot the loss curves for \n",
    "    training and validation data. It saves the generated plot in the specified\n",
    "    directory (`plot_dir`) with a filename based on the language (`l`).\n",
    "    \"\"\"\n",
    "    # Plotting the training loss (in black color with circle markers).\n",
    "    plt.plot(range(len(train_losses)), train_losses, marker = \"o\", color = \"black\")\n",
    "\n",
    "    # Plotting the validation loss (in blue color with circle markers).\n",
    "    plt.plot(range(len(val_losses)), val_losses, marker = \"o\", color = \"blue\")\n",
    "\n",
    "    # Adding legend to distinguish between train and validation curves.\n",
    "    plt.legend([\"Train loss\", \"Val loss\"])\n",
    "\n",
    "    # Adding title and axis labels to the plot.\n",
    "    plt.title(\"Loss curves\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss values\")\n",
    "\n",
    "    # Displaying grid for better visualization.\n",
    "    plt.grid()\n",
    "\n",
    "    # Save the plot as a PNG image in the specified directory with a filename\n",
    "    # based on the language (`l`).\n",
    "    plt.savefig(os.path.join(PLOT_DIR,\"loss_{}.png\".format(l)))\n",
    "\n",
    "plotresults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc27c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "# Check if the \"Translations\" directory exists. If not, create it.\n",
    "if not os.path.exists(\"gru_translations\"):\n",
    "    os.makedirs(\"gru_translations\")\n",
    "\n",
    "def evaluate(language):\n",
    "    \"\"\"\n",
    "    Function to evaluate and generate translations for given test data.\n",
    "    \n",
    "    This function reads a CSV file containing English sentences, \n",
    "    translates each sentence to the target language using the \n",
    "    trained model, and then saves the translations to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - language: The target language for translation.\n",
    "\n",
    "    Outputs:\n",
    "    - A CSV file named \"answer1_{language}_test.csv\" saved in the \"Translations\" directory.\n",
    "      This file contains the original English sentences and their corresponding translations.\n",
    "    \"\"\"\n",
    "    # List to store the predicted translations.\n",
    "    predictions = []\n",
    "\n",
    "    # Read the test data from the specified CSV file.\n",
    "    data = pd.read_csv(\"./../testData/testEnglish-{}.csv\".format(language))\n",
    "\n",
    "    # Loop through each row (sentence) in the test data.\n",
    "    for idx, row in data.iterrows():\n",
    "        # Extract the English sentence.\n",
    "        en = row[\"english\"]\n",
    "\n",
    "        # Translate the English sentence to the target language.\n",
    "        pred = translate(en, model, eng, lang, max_len=20)\n",
    "\n",
    "        # Print the translated sentence (optional, can be commented out).\n",
    "        print(pred)\n",
    "\n",
    "        # Append the translated sentence to the predictions list.\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Add the predicted translations as a new column to the original dataframe.\n",
    "    data[\"translated\"] = predictions\n",
    "\n",
    "    # Drop the unwanted column \"Unnamed: 0\" (assuming it exists in the CSV).\n",
    "    data.drop([\"Unnamed: 0\"], inplace=True, axis=1)\n",
    "\n",
    "    # Save the dataframe with translations to a new CSV file.\n",
    "    data.to_csv(os.path.join(\"gru_translations\", \"answer_{}_test.csv\".format(language)))\n",
    "\n",
    "# Evaluate the model on the Bengali test set.\n",
    "evaluate(l.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608c268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f58f9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "str",
   "language": "python",
   "name": "str"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
