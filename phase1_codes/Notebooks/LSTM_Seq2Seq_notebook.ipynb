{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "078a640a",
   "metadata": {},
   "source": [
    "## Implementation of Transformer for machine translation\n",
    "### The __torchtext.data__ may through an error stating `no module found named \"Field\"` which probably arises due to deprecation of this module in the newer version of torch. Execute the cell below to install the `torchtext version 0.6.0` to run the notebook. This is because the _Field_ and _TabularDataset_ makes the vocabulary and dataloader creation much simpler.\n",
    "```python\n",
    "pip install torchtext==0.6.0\n",
    "print(torchtext.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0574ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchtext==0.6.0\n",
    "# print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0b2b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import random\n",
    "from collections import Counter\n",
    "from torchtext import vocab\n",
    "import warnings\n",
    "import re, string\n",
    "from string import digits\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Convert all the text into lower letters\n",
    "    Remove the words betweent brakets ()\n",
    "    Remove these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
    "    Replace these special characters with space:\n",
    "    Replace extra white spaces with single white spaces\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = re.sub('[$)\\\"’°;\\'€%:,(/]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\u200d', ' ', text)\n",
    "    text = re.sub('\\u200c', ' ', text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    text = re.sub('  ', ' ', text)\n",
    "    text = re.sub('   ', ' ', text)\n",
    "    text =\" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset name\n",
    "l = \"tamil\"\n",
    "\n",
    "# Read the CSV file from the specified directory into a DataFrame\n",
    "data = pd.read_csv('../Data/{}.csv'.format(l))\n",
    "\n",
    "# Drop the unnecessary columns \"Unnamed: 0\" and \"entry_id\" from the DataFrame\n",
    "data.drop([\"Unnamed: 0\", \"entry_id\"], inplace=True, axis=1)\n",
    "\n",
    "# Note: The next operation seems redundant as \"entry_id\" has already been dropped.\n",
    "# Rename the column \"entry_id\" to \"id\" (if it exists)\n",
    "data = data.rename(columns={\"entry_id\": \"id\"})\n",
    "\n",
    "# Display the first 10 rows of the cleaned DataFrame \n",
    "# (This will be visible in interactive environments like Jupyter Notebook)\n",
    "data.head(10)\n",
    "\n",
    "# Write the cleaned data back to a new CSV file in the current directory\n",
    "data.to_csv(\"{}.csv\".format(l), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the cleaned\n",
    "data = pd.read_csv(\"{}.csv\".format(l))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text): \n",
    "    \"\"\"\n",
    "    Tokenize the input text.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): Input text to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    - list: List of tokens.\n",
    "    \"\"\"\n",
    "    return [tok for tok in preprocess(text).split()]\n",
    "\n",
    "# Define Fields for tokenization and preprocessing\n",
    "lang = Field(tokenize = tokenizer, lower = True, init_token = \"<sos>\", eos_token = \"<eos>\")\n",
    "eng = Field(tokenize = tokenizer, lower = True, init_token = \"<sos>\", eos_token = \"<eos>\")\n",
    "\n",
    "# Define data fields for loading the dataset\n",
    "datafields = [(\"english\", eng), (\"{}\".format(l), lang)]\n",
    "# Load the dataset from a CSV file\n",
    "dataset = TabularDataset(path=\"{}.csv\".format(l), format='csv', skip_header=True, fields=datafields)\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = dataset.split(split_ratio = 0.80)\n",
    "\n",
    "# Build vocabulary for each language from the training data\n",
    "lang.build_vocab(train_data, min_freq = 1, max_size = 50000)\n",
    "eng.build_vocab(train_data, min_freq = 1, max_size = 50000)\n",
    "\n",
    "# creating the train and validation data iterator for training\n",
    "train_iterator, val_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data), \n",
    "    batch_size = 32, \n",
    "    device = device, \n",
    "    sort_key = lambda x: getattr(x,l),  # change the language after x.\n",
    "    sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41954ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 examples\n",
    "for i, example in enumerate(dataset.examples):\n",
    "    if i >= 5:  # limit to first 5 for demonstration purposes\n",
    "        break\n",
    "    print(\"English:\", example.english)\n",
    "    print(\"{}:\".format(l.title()), getattr(example, l))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e473e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad454e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0b859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f000c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1c14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac9c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc5e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcbcd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4929aec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "str",
   "language": "python",
   "name": "str"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
