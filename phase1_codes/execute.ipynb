{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14463c1f",
   "metadata": {},
   "source": [
    "## Implementation of Transformer for machine translation\n",
    "### The __torchtext.data__ may through an error stating `no module found named \"Field\"` which probably arises due to deprecation of this module in the newer version of torch. Execute the cell below to install the `torchtext version 0.6.0` to run the notebook. This is because the _Field_ and _TabularDataset_ makes the vocabulary and dataloader creation much simpler.\n",
    "```python\n",
    "pip install torchtext==0.6.0\n",
    "print(torchtext.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2bac713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchtext==0.6.0\n",
    "# print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da0e4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import vocab\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch import Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder,TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import re, string\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import digits\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814bc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"train_data1.json\"\n",
    "VALIDATION_FILE = \"val_data1.json\"\n",
    "TEST_FILE = \"test_data1_final.json\"\n",
    "\n",
    "# Defining lists for each language pair\n",
    "train_bng_ids, train_eb, train_bng = [],[],[]\n",
    "train_guj_ids, train_eg, train_guj = [],[],[]\n",
    "train_hin_ids, train_eh, train_hin = [],[],[]\n",
    "train_kn_ids, train_ek, train_kn = [],[],[]\n",
    "train_ml_ids, train_em, train_ml = [],[],[]\n",
    "train_tm_ids, train_et, train_tm = [],[],[]\n",
    "train_tl_ids, train_etl, train_tl = [],[],[]\n",
    "train_ml_ids, train_eml, train_ml = [],[],[]\n",
    "val_ids = []\n",
    "\n",
    "def collectData():\n",
    "\n",
    "    with open(TRAIN_FILE, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "        for language_type, lang_data in data.items():\n",
    "            # print(lang_data)\n",
    "            for data_type, data_entries in lang_data.items():\n",
    "                print(data_type)\n",
    "\n",
    "                for ent_id, ent_data in data_entries.items():\n",
    "                    source_sen = ent_data[\"source\"]\n",
    "                    target_sen = ent_data[\"target\"]\n",
    "                    if language_type == \"English-Bengali\":  \n",
    "                        train_eb.append(source_sen)\n",
    "                        train_bng.append(target_sen)\n",
    "                        train_bng_ids.append(ent_id)\n",
    "                    if language_type == \"English-Gujarati\":  \n",
    "                        train_eg.append(source_sen)\n",
    "                        train_guj.append(target_sen)\n",
    "                        train_guj_ids.append(ent_id)\n",
    "                    if language_type == \"English-Hindi\":  \n",
    "                        train_eh.append(source_sen)\n",
    "                        train_hin.append(target_sen)\n",
    "                        train_hin_ids.append(ent_id)\n",
    "                    if language_type == \"English-Kannada\":  \n",
    "                        train_ek.append(source_sen)\n",
    "                        train_kn.append(target_sen)\n",
    "                        train_kn_ids.append(ent_id)\n",
    "                    if language_type == \"English-Tamil\":  \n",
    "                        train_et.append(source_sen)\n",
    "                        train_tm.append(target_sen)\n",
    "                        train_tm_ids.append(ent_id)\n",
    "                    if language_type == \"English-Telgu\":  \n",
    "                        train_etl.append(source_sen)\n",
    "                        train_tl.append(target_sen)\n",
    "                        train_tl_ids.append(ent_id)\n",
    "                    if language_type == \"English-Malayalam\":  \n",
    "                        train_eml.append(source_sen)\n",
    "                        train_ml.append(target_sen)\n",
    "                        train_ml_ids.append(ent_id)\n",
    "\n",
    "    DATA_DIR = \"Data\"\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    eb_df = pd.DataFrame({\"entry_id\" : train_bng_ids, \"english\" : train_eb, \"bengali\": train_bng})\n",
    "    eg_df = pd.DataFrame({\"entry_id\" : train_guj_ids, \"english\" : train_eg, \"gujarati\": train_guj})\n",
    "    eh_df = pd.DataFrame({\"entry_id\" : train_hin_ids, \"english\" : train_eh, \"hindi\": train_hin})\n",
    "    ek_df = pd.DataFrame({\"entry_id\" : train_kn_ids, \"english\" : train_ek, \"kannada\": train_kn})\n",
    "    etm_df = pd.DataFrame({\"entry_id\" : train_tm_ids, \"english\" : train_et, \"tamil\": train_tm})\n",
    "    etl_df = pd.DataFrame({\"entry_id\" : train_tl_ids, \"english\" : train_etl, \"telegu\": train_tl})\n",
    "    eml_df = pd.DataFrame({\"entry_id\" : train_ml_ids, \"english\" : train_eml, \"malayalam\" : train_ml})\n",
    "\n",
    "    eb_df.to_csv(os.path.join(DATA_DIR,\"bengali.csv\"))\n",
    "    eg_df.to_csv(os.path.join(DATA_DIR,\"gujarati.csv\"))\n",
    "    eh_df.to_csv(os.path.join(DATA_DIR,\"hindi.csv\"))\n",
    "    ek_df.to_csv(os.path.join(DATA_DIR,\"kannada.csv\"))\n",
    "    etl_df.to_csv(os.path.join(DATA_DIR,\"telugu.csv\"))\n",
    "    etm_df.to_csv(os.path.join(DATA_DIR,\"tamil.csv\"))\n",
    "    eml_df.to_csv(os.path.join(DATA_DIR,\"malayalam.csv\"))\n",
    "\n",
    "def fetchtestData():\n",
    "    \n",
    "\n",
    "    DATA_DIR = \"testData\"\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    with open(TEST_FILE, \"r\") as file:\n",
    "        val_data = json.load(file)\n",
    "        for lang_type, lang_data in val_data.items():\n",
    "            print(lang_type)\n",
    "            for data_type, data_entries in lang_data.items():\n",
    "                ids, en_sns = [], []\n",
    "                for ent_id, ent_data in data_entries.items():\n",
    "                    val_ids.append(ent_id)\n",
    "                    ids.append(ent_id)\n",
    "                    en_sns.append(ent_data[\"source\"])\n",
    "    \n",
    "                valdf = pd.DataFrame({\"id\" : ids, \"english\" : en_sns})\n",
    "                valdf.to_csv(os.path.join(DATA_DIR,\"test{}.csv\".format(lang_type)))\n",
    "    print(len(val_ids))\n",
    "\n",
    "def merge():\n",
    "    DATA_DIR = \"models/Translations\"\n",
    "    RESULTS = \"answer.csv\"\n",
    "\n",
    "    if os.path.exists(os.path.join(DATA_DIR, RESULTS)):\n",
    "        os.remove(os.path.join(DATA_DIR,RESULTS))\n",
    "    val_ids = []\n",
    "\n",
    "    with open(VALIDATION_FILE, \"r\") as file:\n",
    "        val_data = json.load(file)\n",
    "        for lang_type, lang_data in val_data.items():\n",
    "            print(lang_type)\n",
    "            for data_type, data_entries in lang_data.items():\n",
    "                ids, en_sns = [], []\n",
    "                for ent_id, ent_data in data_entries.items():\n",
    "                    val_ids.append(int(ent_id))\n",
    "\n",
    "    print(len(val_ids))\n",
    "    # print(val_ids)\n",
    "\n",
    "    # Reading all the dataframes\n",
    "    files = os.listdir(DATA_DIR)\n",
    "    files = [file for file in files if file.endswith(\".csv\")]\n",
    "    print(files)\n",
    "    data = [pd.read_csv(os.path.join(DATA_DIR, file)) for file in files]\n",
    "    res = pd.concat(data, axis=0, ignore_index=False)\n",
    "    res.drop([\"Unnamed: 0\", \"english\"], axis=1, inplace=True)\n",
    "    res = res.rename(columns={\"id\": \"ID\", \"translated\": \"Translated\"})\n",
    "    res_sorted = res[res[\"ID\"].isin(val_ids)].sort_values(by=\"ID\")\n",
    "    res_sorted.to_csv(os.path.join(DATA_DIR, RESULTS), sep=\"\\t\", index=False, quoting=csv.QUOTE_NONNUMERIC, quotechar='\"', escapechar='\\\\')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # collectData()\n",
    "    fetchtestData()\n",
    "    # merge()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57203b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf445f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0019d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8b60a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "str",
   "language": "python",
   "name": "str"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
