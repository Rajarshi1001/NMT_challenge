{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcqEg7sbrYG6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Transformer for machine translation\n",
        "### The __torchtext.data__ may through an error stating `no module found named \"Field\"` which probably arises due to deprecation of this module in the newer version of torch. Execute the cell below to install the `torchtext version 0.6.0` to run the notebook. This is because the _Field_ and _TabularDataset_ makes the vocabulary and dataloader creation much simpler.\n",
        "```python\n",
        "pip install torchtext==0.6.0\n",
        "print(torchtext.__version__)\n",
        "```\n",
        "You also may need to restart the runtime"
      ],
      "metadata": {
        "id": "_yfb-ygVtIN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.6.0\n",
        "# print(torchtext.__version__)"
      ],
      "metadata": {
        "id": "evQERu-gFOaq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import vocab\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "from torch import Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerDecoder,TransformerEncoderLayer, TransformerDecoderLayer\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import csv, json\n",
        "import pickle\n",
        "import random\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import re, string\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from string import digits\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "BQ1nfwc6tHuT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace the name of th test data after -O\n",
        "!wget 'https://drive.google.com/uc?id=19WHTYU0j3EBFTNR_OldP3c6AKNqonH52' -O test_data.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qSp21pPYElZ",
        "outputId": "95df09f8-34a0-4152-acd9-6bff84981910"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-04 11:45:47--  https://drive.google.com/uc?id=19WHTYU0j3EBFTNR_OldP3c6AKNqonH52\n",
            "Resolving drive.google.com (drive.google.com)... 209.85.200.102, 209.85.200.100, 209.85.200.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|209.85.200.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0c-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/66c79s9a5dtsnq5tripdefbn4764eo5o/1696419900000/13631053167411324863/*/19WHTYU0j3EBFTNR_OldP3c6AKNqonH52?uuid=ab43cd93-2aa0-49df-bce2-189a6a9a7813 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-10-04 11:45:48--  https://doc-0c-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/66c79s9a5dtsnq5tripdefbn4764eo5o/1696419900000/13631053167411324863/*/19WHTYU0j3EBFTNR_OldP3c6AKNqonH52?uuid=ab43cd93-2aa0-49df-bce2-189a6a9a7813\n",
            "Resolving doc-0c-54-docs.googleusercontent.com (doc-0c-54-docs.googleusercontent.com)... 173.194.195.132, 2607:f8b0:4001:c1a::84\n",
            "Connecting to doc-0c-54-docs.googleusercontent.com (doc-0c-54-docs.googleusercontent.com)|173.194.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18465263 (18M) [application/json]\n",
            "Saving to: ‘test_data.json’\n",
            "\n",
            "test_data.json      100%[===================>]  17.61M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-10-04 11:45:48 (166 MB/s) - ‘test_data.json’ saved [18465263/18465263]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_FILE = \"test_data.json\"\n",
        "\n",
        "val_ids = []\n",
        "\n",
        "def fetchtestData():\n",
        "\n",
        "    DATA_DIR = \"testData\"\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.makedirs(DATA_DIR)\n",
        "\n",
        "    with open(TEST_FILE, \"r\") as file:\n",
        "        val_data = json.load(file)\n",
        "        for lang_type, lang_data in val_data.items():\n",
        "            print(lang_type)\n",
        "            for data_type, data_entries in lang_data.items():\n",
        "                ids, en_sns = [], []\n",
        "                for ent_id, ent_data in data_entries.items():\n",
        "                    val_ids.append(ent_id)\n",
        "                    ids.append(ent_id)\n",
        "                    en_sns.append(ent_data[\"source\"])\n",
        "\n",
        "                valdf = pd.DataFrame({\"id\" : ids, \"english\" : en_sns})\n",
        "                valdf.to_csv(os.path.join(DATA_DIR,\"test{}.csv\".format(lang_type)))\n",
        "    print(len(val_ids))\n",
        "\n",
        "def merge():\n",
        "    DATA_DIR = \"models/Translations\"\n",
        "    RESULTS = \"answer.csv\"\n",
        "\n",
        "    if os.path.exists(os.path.join(DATA_DIR, RESULTS)):\n",
        "        os.remove(os.path.join(DATA_DIR,RESULTS))\n",
        "    val_ids = []\n",
        "\n",
        "    with open(VALIDATION_FILE, \"r\") as file:\n",
        "        val_data = json.load(file)\n",
        "        for lang_type, lang_data in val_data.items():\n",
        "            print(lang_type)\n",
        "            for data_type, data_entries in lang_data.items():\n",
        "                ids, en_sns = [], []\n",
        "                for ent_id, ent_data in data_entries.items():\n",
        "                    val_ids.append(int(ent_id))\n",
        "\n",
        "    print(len(val_ids))\n",
        "    # print(val_ids)\n",
        "\n",
        "    # Reading all the dataframes\n",
        "    files = os.listdir(DATA_DIR)\n",
        "    files = [file for file in files if file.endswith(\".csv\")]\n",
        "    print(files)\n",
        "    data = [pd.read_csv(os.path.join(DATA_DIR, file)) for file in files]\n",
        "    res = pd.concat(data, axis=0, ignore_index=False)\n",
        "    res.drop([\"Unnamed: 0\", \"english\"], axis=1, inplace=True)\n",
        "    res = res.rename(columns={\"id\": \"ID\", \"translated\": \"Translated\"})\n",
        "    res_sorted = res[res[\"ID\"].isin(val_ids)].sort_values(by=\"ID\")\n",
        "    res_sorted.to_csv(os.path.join(DATA_DIR, RESULTS), sep=\"\\t\", index=False, quoting=csv.QUOTE_NONNUMERIC, quotechar='\"', escapechar='\\\\')\n",
        "\n",
        "fetchtestData()"
      ],
      "metadata": {
        "id": "4kKetmDTtHxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d664265-101e-4cce-cae5-c90d4adf5dd5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English-Bengali\n",
            "English-Gujarati\n",
            "English-Hindi\n",
            "English-Kannada\n",
            "English-Malayalam\n",
            "English-Tamil\n",
            "English-Telgu\n",
            "114647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "# Direct download URLs\n",
        "hin = 'https://drive.google.com/uc?export=download&id=1acr5l2kPbksgMmBYutiT1NBPLNyKiNH6'\n",
        "tam = 'https://drive.google.com/uc?export=download&id=1tmbFLO8EkYK_xWQTGeh3j6cM4dDWSaz5'\n",
        "ben = 'https://drive.google.com/uc?export=download&id=14ZQdqI_G8wS8r9_ha3Kr7QJV9bJiMcNT'\n",
        "kan = 'https://drive.google.com/uc?export=download&id=15qzjTmcHG26CvmsOSPSK2BrkLhcdZjde'\n",
        "guj = 'https://drive.google.com/uc?export=download&id=1k56Imci4OEGi3dHUJeAqTGT0qa15EQwV'\n",
        "tel = 'https://drive.google.com/uc?export=download&id=1OiHVj-HtWHkX86oQ-1drlAWRy7rlk9rI'\n",
        "mal = 'https://drive.google.com/uc?export=download&id=1sHBCkckoc52EPL_3iQfTs11YqP_0DKAG'\n",
        "\n",
        "names = ['hindi', 'tamil', 'bengali', 'kannada', 'gujarati', 'telgu', 'malayalam']\n",
        "nbs = [hin, tam, ben, kan, guj, tel, mal]\n",
        "\n",
        "for index, nm in enumerate(names):\n",
        "    file_url = nbs[index]\n",
        "    output_file = f\"{nm}.pth\"\n",
        "    # Use gdown to download the file\n",
        "    gdown.download(file_url, output_file, quiet=False)\n",
        "\n",
        "# Verify the download\n",
        "print(\"Download complete. Files in the current directory:\")\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBeh7mTRaLSI",
        "outputId": "baea22ed-fe9f-43c0-d99f-a916565ef6a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1acr5l2kPbksgMmBYutiT1NBPLNyKiNH6\n",
            "To: /content/hindi.pth\n",
            "100%|██████████| 368M/368M [00:07<00:00, 48.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1tmbFLO8EkYK_xWQTGeh3j6cM4dDWSaz5\n",
            "To: /content/tamil.pth\n",
            "100%|██████████| 354M/354M [00:07<00:00, 47.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=14ZQdqI_G8wS8r9_ha3Kr7QJV9bJiMcNT\n",
            "To: /content/bengali.pth\n",
            "100%|██████████| 365M/365M [00:09<00:00, 39.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=15qzjTmcHG26CvmsOSPSK2BrkLhcdZjde\n",
            "To: /content/kannada.pth\n",
            "100%|██████████| 336M/336M [00:05<00:00, 56.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1k56Imci4OEGi3dHUJeAqTGT0qa15EQwV\n",
            "To: /content/gujarati.pth\n",
            "100%|██████████| 345M/345M [00:07<00:00, 45.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1OiHVj-HtWHkX86oQ-1drlAWRy7rlk9rI\n",
            "To: /content/telgu.pth\n",
            "100%|██████████| 334M/334M [00:07<00:00, 45.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1sHBCkckoc52EPL_3iQfTs11YqP_0DKAG\n",
            "To: /content/malayalam.pth\n",
            "100%|██████████| 347M/347M [00:06<00:00, 53.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete. Files in the current directory:\n",
            "['.config', 'malayalam.pth', 'bengali.pth', 'gujarati.pth', 'kannada.pth', 'tamil.pth', 'test_data.json', 'hindi.pth', 'testData', 'telgu.pth', 'sample_data']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "# Direct download URLs\n",
        "hin_vocab = \"https://drive.google.com/uc?export=download&id=1puzGRvWkZjruQ_gx60nJj2I5e4J80vu6\"\n",
        "tam_vocab = \"https://drive.google.com/uc?export=download&id=1V5C4lhBpqV5xkN06FGAkZSE4qj3gj_LA\"\n",
        "ben_vocab = \"https://drive.google.com/uc?export=download&id=1-D2zoKuXOnZdnTg9Q9hGPWsrgvX6TP18\"\n",
        "kan_vocab = \"https://drive.google.com/uc?export=download&id=1cjIZKA_56Eo0RzgNbEVdUFjSLGzSVq99\"\n",
        "guj_vocab = \"https://drive.google.com/uc?export=download&id=1mgNQ991u1wF0hya9oEYJuCU9eSaq5sWe\"\n",
        "tel_vocab = \"https://drive.google.com/uc?export=download&id=1dTB9m-69Se5ajbH7pDFstwBKSfBnd49d\"\n",
        "mal_vocab = \"https://drive.google.com/uc?export=download&id=1MQ5u-hb_9TcaswKu5IEqrtDGdUrByo-D\"\n",
        "\n",
        "vocab_names = ['hindi', 'tamil', 'bengali', 'kannada', 'gujarati', 'telgu', 'malayalam']\n",
        "nbs_vocabs = [hin_vocab, tam_vocab, ben_vocab, kan_vocab, guj_vocab, tel_vocab, mal_vocab]\n",
        "\n",
        "for index, nm in enumerate(vocab_names):\n",
        "    file_url = nbs_vocabs[index]\n",
        "    output_file = f\"{nm}.pkl\"\n",
        "    # Use gdown to download the file\n",
        "    gdown.download(file_url, output_file, quiet=False)\n",
        "\n",
        "# Verify the download\n",
        "print(\"Download complete. Files in the current directory:\")\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6YN5ddz11au",
        "outputId": "3c67c42c-4737-4920-908f-c4e5ae8e7dfd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1puzGRvWkZjruQ_gx60nJj2I5e4J80vu6\n",
            "To: /content/hindi.pkl\n",
            "100%|██████████| 2.27M/2.27M [00:00<00:00, 224MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1V5C4lhBpqV5xkN06FGAkZSE4qj3gj_LA\n",
            "To: /content/tamil.pkl\n",
            "100%|██████████| 4.74M/4.74M [00:00<00:00, 21.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1-D2zoKuXOnZdnTg9Q9hGPWsrgvX6TP18\n",
            "To: /content/bengali.pkl\n",
            "100%|██████████| 3.08M/3.08M [00:00<00:00, 235MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1cjIZKA_56Eo0RzgNbEVdUFjSLGzSVq99\n",
            "To: /content/kannada.pkl\n",
            "100%|██████████| 3.29M/3.29M [00:00<00:00, 170MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1mgNQ991u1wF0hya9oEYJuCU9eSaq5sWe\n",
            "To: /content/gujarati.pkl\n",
            "100%|██████████| 2.60M/2.60M [00:00<00:00, 207MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1dTB9m-69Se5ajbH7pDFstwBKSfBnd49d\n",
            "To: /content/telgu.pkl\n",
            "100%|██████████| 2.96M/2.96M [00:00<00:00, 243MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1MQ5u-hb_9TcaswKu5IEqrtDGdUrByo-D\n",
            "To: /content/malayalam.pkl\n",
            "100%|██████████| 4.91M/4.91M [00:00<00:00, 19.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete. Files in the current directory:\n",
            "['.config', 'malayalam.pth', 'bengali.pkl', 'bengali.pth', 'gujarati.pth', 'malayalam.pkl', 'kannada.pkl', 'hindi.pkl', 'gujarati.pkl', 'kannada.pth', 'telgu.pkl', 'tamil.pth', 'test_data.json', 'hindi.pth', 'testData', 'tamil.pkl', 'telgu.pth', 'sample_data']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "# Adjusted URLs for direct downloading\n",
        "eng_hindi_vocab = \"https://drive.google.com/uc?export=download&id=1cwjYFublDl2PncWO-z1R0CFL3iKVguyj\"\n",
        "eng_tamil_vocab = \"https://drive.google.com/uc?export=download&id=1zbWJ9t2ufxsA_sicK5bZGFSIuHX7S_FV\"\n",
        "eng_bengali_vocab = \"https://drive.google.com/uc?export=download&id=11EvhUzf-aPfeEagI7DqUBWz9yv4NR2b-\"\n",
        "eng_kannada_vocab = \"https://drive.google.com/uc?export=download&id=1FO8TUY7CiK7eB63GyR0FRb0SN4ZzmXex\"\n",
        "eng_gujarati_vocab = \"https://drive.google.com/uc?export=download&id=1CQH1Dc26gkmYcoO4HfvMGYQdmHtcf45d\"\n",
        "eng_telgu_vocab = \"https://drive.google.com/uc?export=download&id=18g2AvdeDN110661OUVf6LP9NbCQn21kI\"\n",
        "eng_malayalam_vocab = \"https://drive.google.com/uc?export=download&id=1ZnTc_FwFyn2CjMB5baqPiYnNDHbl4_eH\"\n",
        "\n",
        "vocab_names = ['hindi', 'tamil', 'bengali', 'kannada', 'gujarati', 'telgu', 'malayalam']\n",
        "nbs_vocabs = [eng_hindi_vocab, eng_tamil_vocab, eng_bengali_vocab, eng_kannada_vocab, eng_gujarati_vocab, eng_telgu_vocab, eng_malayalam_vocab]\n",
        "\n",
        "for index, nm in enumerate(vocab_names):\n",
        "    file_url = nbs_vocabs[index]\n",
        "    output_file = \"eng_{}.pkl\".format(nm)\n",
        "    # Use gdown to download the file\n",
        "    gdown.download(file_url, output_file, quiet=False)\n",
        "\n",
        "# Verify the download\n",
        "print(\"Download complete. Files in the current directory:\")\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_wj139x9PN_",
        "outputId": "e00ad423-a9f8-4796-b14d-1c5f074c6427"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1cwjYFublDl2PncWO-z1R0CFL3iKVguyj\n",
            "To: /content/eng_hindi.pkl\n",
            "100%|██████████| 1.29M/1.29M [00:00<00:00, 146MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1zbWJ9t2ufxsA_sicK5bZGFSIuHX7S_FV\n",
            "To: /content/eng_tamil.pkl\n",
            "100%|██████████| 1.09M/1.09M [00:00<00:00, 126MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=11EvhUzf-aPfeEagI7DqUBWz9yv4NR2b-\n",
            "To: /content/eng_bengali.pkl\n",
            "100%|██████████| 1.23M/1.23M [00:00<00:00, 171MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1FO8TUY7CiK7eB63GyR0FRb0SN4ZzmXex\n",
            "To: /content/eng_kannada.pkl\n",
            "100%|██████████| 864k/864k [00:00<00:00, 133MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1CQH1Dc26gkmYcoO4HfvMGYQdmHtcf45d\n",
            "To: /content/eng_gujarati.pkl\n",
            "100%|██████████| 982k/982k [00:00<00:00, 115MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=18g2AvdeDN110661OUVf6LP9NbCQn21kI\n",
            "To: /content/eng_telgu.pkl\n",
            "100%|██████████| 841k/841k [00:00<00:00, 160MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1ZnTc_FwFyn2CjMB5baqPiYnNDHbl4_eH\n",
            "To: /content/eng_malayalam.pkl\n",
            "100%|██████████| 1.02M/1.02M [00:00<00:00, 135MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete. Files in the current directory:\n",
            "['.config', 'malayalam.pth', 'bengali.pkl', 'bengali.pth', 'gujarati.pth', 'eng_malayalam.pkl', 'eng_telgu.pkl', 'eng_tamil.pkl', 'malayalam.pkl', 'kannada.pkl', 'hindi.pkl', 'gujarati.pkl', 'eng_bengali.pkl', 'kannada.pth', 'telgu.pkl', 'tamil.pth', 'eng_kannada.pkl', 'eng_hindi.pkl', 'eng_gujarati.pkl', 'test_data.json', 'hindi.pth', 'testData', 'tamil.pkl', 'telgu.pth', 'sample_data']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://drive.google.com/drive/folders/1PgGeWF6Rp-0Z30LBn6-70Cl-Vz_uxms5?usp=sharing -O models"
      ],
      "metadata": {
        "id": "ApUEbX1nZEgp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the dataset name\n",
        "l = [\"Bengali\", \"Gujarati\", \"Hindi\", \"Malayalam\", \"Telgu\", \"Kannada\", \"Tamil\"]\n",
        "\n",
        "for e in names:\n",
        "# Read the CSV file from the specified directory into a DataFrame\n",
        "  data = pd.read_csv(\"/content/testData/testEnglish-{}.csv\".format(e.title()))\n",
        "\n",
        "  # Drop the unnecessary columns \"Unnamed: 0\" and \"entry_id\" from the DataFrame\n",
        "  data.drop([\"Unnamed: 0\", \"id\"], inplace=True, axis=1)\n",
        "\n",
        "  # Note: The next operation seems redundant as \"entry_id\" has already been dropped.\n",
        "  # Rename the column \"entry_id\" to \"id\" (if it exists)\n",
        "  data = data.rename(columns={\"id\": \"id\"})\n",
        "\n",
        "  # Display the first 10 rows of the cleaned DataFrame\n",
        "  # (This will be visible in interactive environments like Jupyter Notebook)\n",
        "  data.head(10)\n",
        "\n",
        "  # Write the cleaned data back to a new CSV file in the current directory\n",
        "  data.to_csv(\"{}.csv\".format(e), index=False)\n"
      ],
      "metadata": {
        "id": "2xaJft7hXvQ_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Convert all the text into lower letters\n",
        "    Remove the words betweent brakets ()\n",
        "    Remove these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
        "    Replace these special characters with space:\n",
        "    Replace extra white spaces with single white spaces\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    text = re.sub('[$)\\\"’°;\\'€%:,(/]', '', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('\\u200d', ' ', text)\n",
        "    text = re.sub('\\u200c', ' ', text)\n",
        "    text = re.sub('-', ' ', text)\n",
        "    text = re.sub('  ', ' ', text)\n",
        "    text = re.sub('   ', ' ', text)\n",
        "    text =\" \".join(text.split())\n",
        "    return text"
      ],
      "metadata": {
        "id": "Y_k0jEQ-bydP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/testData/testEnglish-Bengali.csv\")\n",
        "data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "VwkOY2MceyHl",
        "outputId": "4cd8232f-3916-4371-9fb9-e4f16cd1bc18"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0      id                                            english\n",
              "0           0  177039                                     current events\n",
              "1           1  177040  The god Brahma was pleased with her penance bu...\n",
              "2           2  177041  After feeling stiffness in chest or heaviness ...\n",
              "3           3  177042  As it dawns on him that the baby is in fact hi...\n",
              "4           4  177043  Australia produces significant amounts of lent...\n",
              "5           5  177044                        what's with my dinner order\n",
              "6           6  177045  In an interview with the New York Times, he cl...\n",
              "7           7  177046  The Chief Justice of the Madras High Court is ...\n",
              "8           8  177047  The funny thing is that near many of the citie...\n",
              "9           9  177048           It makes their Indian identity obvious ."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2846bf99-2630-4090-a854-d23016d9b0a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>177039</td>\n",
              "      <td>current events</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>177040</td>\n",
              "      <td>The god Brahma was pleased with her penance bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>177041</td>\n",
              "      <td>After feeling stiffness in chest or heaviness ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>177042</td>\n",
              "      <td>As it dawns on him that the baby is in fact hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>177043</td>\n",
              "      <td>Australia produces significant amounts of lent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>177044</td>\n",
              "      <td>what's with my dinner order</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>177045</td>\n",
              "      <td>In an interview with the New York Times, he cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>177046</td>\n",
              "      <td>The Chief Justice of the Madras High Court is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>177047</td>\n",
              "      <td>The funny thing is that near many of the citie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>177048</td>\n",
              "      <td>It makes their Indian identity obvious .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2846bf99-2630-4090-a854-d23016d9b0a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2846bf99-2630-4090-a854-d23016d9b0a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2846bf99-2630-4090-a854-d23016d9b0a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-33e4af55-bb1b-44ec-9248-0f2fbb9e0931\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-33e4af55-bb1b-44ec-9248-0f2fbb9e0931')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-33e4af55-bb1b-44ec-9248-0f2fbb9e0931 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # train\n",
        "device = \"cpu\" # inference"
      ],
      "metadata": {
        "id": "KQkx0eR-Xyyq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def tokenizer(text):\n",
        "#   return [tok.lower() for tok in preprocess(text).split()]\n",
        "\n",
        "# def process(l):\n",
        "#     engs = []\n",
        "#     langs = []\n",
        "#     train_iters = []\n",
        "#     val_iters = []\n",
        "\n",
        "#     for e in l:\n",
        "#         # Define Fields for tokenization and preprocessing\n",
        "#         lang_field = Field(tokenize=tokenizer, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "#         eng_field = Field(tokenize=tokenizer, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "#         # Define data fields for loading the dataset\n",
        "#         datafields = [(\"english\", eng_field), (e, lang_field)]\n",
        "\n",
        "#         # Load the dataset from a CSV file\n",
        "#         dataset = TabularDataset(path=\"/content/{}.csv\".format(e), format='csv', skip_header=True, fields=datafields)\n",
        "\n",
        "#         # Split the dataset into training and validation sets\n",
        "#         train_data, val_data = dataset.split(split_ratio=0.80)\n",
        "\n",
        "#         # Build vocabulary for each language from the training data\n",
        "#         lang_field.build_vocab(train_data, min_freq=1, max_size=50000)\n",
        "#         eng_field.build_vocab(train_data, min_freq=1, max_size=50000)\n",
        "\n",
        "#         # Creating the train and validation data iterator for training\n",
        "#         train_iter, val_iter = BucketIterator.splits(\n",
        "#             (train_data, val_data),\n",
        "#             batch_size=8,\n",
        "#             device=device,\n",
        "#             sort_key=lambda x: len(getattr(x, e)),\n",
        "#             sort_within_batch=True)\n",
        "\n",
        "#         engs.append(eng_field)\n",
        "#         langs.append(lang_field)\n",
        "#         train_iters.append(train_iter)\n",
        "#         val_iters.append(val_iter)\n",
        "\n",
        "#     return engs, langs, train_iters, val_iters\n",
        "\n",
        "# engs, langs, train_iters, val_iters = process(l)"
      ],
      "metadata": {
        "id": "-RaKyiT7Xzhc"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the params of all the models"
      ],
      "metadata": {
        "id": "659oI6V5AZQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerMT(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Transformer model for Machine Translation (MT).\n",
        "\n",
        "    This model consists of an encoder and a decoder, both built using the Transformer architecture.\n",
        "\n",
        "    Parameters:\n",
        "    - nhead (int): Number of heads in the multihead attention mechanism.\n",
        "    - embed_size (int): Dimension of the embedding vector.\n",
        "    - source_vocab_size (int): Size of the source vocabulary.\n",
        "    - target_vocab_size (int): Size of the target vocabulary.\n",
        "    - num_encoder_layers (int): Number of layers in the transformer encoder.\n",
        "    - num_decoder_layers (int): Number of layers in the transformer decoder.\n",
        "    - ffnn_size (int, optional): Size of the feedforward neural network inside transformer layers. Default is 512.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 nhead: int,\n",
        "                 embed_size: int,\n",
        "                 source_vocab_size: int,\n",
        "                 target_vocab_size: int,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 ffnn_size:int = 512):\n",
        "\n",
        "        super(TransformerMT, self).__init__()\n",
        "\n",
        "        # Define encoder and decoder layers\n",
        "        encoder_layer = TransformerEncoderLayer(d_model = embed_size, nhead = nhead, dim_feedforward = ffnn_size)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model = embed_size, nhead = nhead, dim_feedforward = ffnn_size)\n",
        "\n",
        "        # Initialize transformer encoder, decoder, and final fully connected layer\n",
        "        self.tf_encoder = TransformerEncoder(encoder_layer, num_layers = num_encoder_layers)\n",
        "        self.tf_decoder = TransformerDecoder(decoder_layer, num_layers = num_decoder_layers)\n",
        "        self.fc_layer = nn.Linear(embed_size, target_vocab_size)\n",
        "\n",
        "        # Embedding layers for tokens and positional information\n",
        "        self.src_token_embedding = TokenEmbedding(source_vocab_size, embed_size)\n",
        "        self.tar_token_embedding = TokenEmbedding(target_vocab_size, embed_size)\n",
        "        self.positional_embedding = SinusoidalEmbedding(embed_size, dropout = DROPOUT_RATE)\n",
        "\n",
        "    def forward(self,\n",
        "                source: Tensor,\n",
        "                target: Tensor,\n",
        "                source_mask: Tensor,\n",
        "                target_mask: Tensor,\n",
        "                source_padding_mask: Tensor,\n",
        "                target_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the TransformerMT model.\n",
        "\n",
        "        Parameters:\n",
        "        - source (torch.Tensor): Source sequence tensor.\n",
        "        - target (torch.Tensor): Target sequence tensor.\n",
        "        - source_mask (torch.Tensor): Source sequence mask.\n",
        "        - target_mask (torch.Tensor): Target sequence mask.\n",
        "        - source_padding_mask (torch.Tensor): Source padding mask.\n",
        "        - target_padding_mask (torch.Tensor): Target padding mask.\n",
        "        - memory_key_padding_mask (torch.Tensor): Memory key padding mask.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor after passing through the transformer and the fully connected layer.\n",
        "        \"\"\"\n",
        "\n",
        "        source_embedding = self.positional_embedding(self.src_token_embedding(source))\n",
        "        target_embedding = self.positional_embedding(self.tar_token_embedding(target))\n",
        "        memory = self.tf_encoder(source_embedding, source_mask, source_padding_mask)\n",
        "        outputs = self.tf_decoder(target_embedding,\n",
        "                                  memory,\n",
        "                                  target_mask,\n",
        "                                  None,\n",
        "                                  target_padding_mask,\n",
        "                                  memory_key_padding_mask)\n",
        "        outputs = self.fc_layer(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def encode(self, source: Tensor, source_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Encode the source sequence using the transformer encoder.\n",
        "\n",
        "        Parameters:\n",
        "        - source (torch.Tensor): Source sequence tensor.\n",
        "        - source_mask (torch.Tensor): Source sequence mask.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Encoded memory tensor.\n",
        "        \"\"\"\n",
        "        token_rep = self.src_token_embedding(source)\n",
        "        positional_rep = self.positional_embedding(token_rep)\n",
        "        encoder_output = self.tf_encoder(positional_rep, source_mask)\n",
        "        return encoder_output\n",
        "\n",
        "    def decode(self, target: Tensor, memory: Tensor, target_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Decode the memory tensor using the transformer decoder.\n",
        "\n",
        "        Parameters:\n",
        "        - target (torch.Tensor): Target sequence tensor.\n",
        "        - memory (torch.Tensor): Encoded memory tensor.\n",
        "        - target_mask (torch.Tensor): Target sequence mask.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Decoded tensor.\n",
        "        \"\"\"\n",
        "        token_rep = self.src_token_embedding(target)\n",
        "        positional_rep = self.positional_embedding(token_rep)\n",
        "        decoder_output = self.tf_decoder(positional_rep, memory, target_mask)\n",
        "        return decoder_output\n",
        "\n"
      ],
      "metadata": {
        "id": "x76uFRUTtHzU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Sinusoidal Positional Encoding for Transformer models.\n",
        "\n",
        "    The positional encoding module uses sine and cosine functions of different frequencies to\n",
        "    encode the position of tokens in the sequence.\n",
        "\n",
        "    Parameters:\n",
        "    - embed_size (int): Dimension of the embedding vector.\n",
        "    - dropout (float): Dropout rate for the dropout layer.\n",
        "    - max_len (int, optional): Maximum length of the sequence. Default is 5000.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size, dropout, max_len = 5000):\n",
        "        super(SinusoidalEmbedding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "        # Compute the sinusoidal positional encodings\n",
        "        denom = max_len*2\n",
        "        pdist = torch.exp(- torch.arange(0, embed_size, 2) * math.log(denom) / embed_size)\n",
        "        position = torch.arange(0, max_len).reshape(max_len, 1)\n",
        "        position_embedding = torch.zeros((max_len, embed_size))\n",
        "        position_embedding[:, 0::2] = torch.sin(position * pdist)\n",
        "        position_embedding[:, 1::2] = torch.cos(position * pdist)\n",
        "        position_embedding = position_embedding.unsqueeze(-2)\n",
        "\n",
        "        # Register the position embeddings so they get saved with the model's state_dict\n",
        "        self.register_buffer('position_embedding', position_embedding)\n",
        "\n",
        "    def forward(self, token_embed):\n",
        "        \"\"\"\n",
        "        Forward pass of the SinusoidalEmbedding.\n",
        "\n",
        "        Parameters:\n",
        "        - token_embed (torch.Tensor): Token embeddings.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Token embeddings added with positional encodings.\n",
        "        \"\"\"\n",
        "        outputs = token_embed + self.position_embedding[:token_embed.size(0),:]\n",
        "        outputs = self.dropout(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Token Embedding module for Transformer models.\n",
        "\n",
        "    This module converts token indices into dense vectors of fixed size, embed_size.\n",
        "    The embeddings are scaled by the square root of their dimensionality.\n",
        "\n",
        "    Parameters:\n",
        "    - vocab_size (int): Size of the vocabulary.\n",
        "    - embed_size (int): Dimension of the embedding vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"\n",
        "        Forward pass of the TokenEmbedding.\n",
        "\n",
        "        Parameters:\n",
        "        - tokens (torch.Tensor): Tensor of token indices.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Scaled token embeddings.\n",
        "        \"\"\"\n",
        "        outputs = self.embedding(tokens.long())\n",
        "        outputs_scaled = outputs * math.sqrt(self.embed_size)\n",
        "        return outputs_scaled"
      ],
      "metadata": {
        "id": "pbkjA3fTtH1G"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\"\n",
        "    Generate a square mask for the sequence, where the mask indicates subsequent positions.\n",
        "    This mask is used to ensure that a position cannot attend to subsequent positions in the sequence.\n",
        "\n",
        "    Parameters:\n",
        "    - sz (int): Size of the sequence.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Mask tensor of shape (sz, sz) with 0s in positions that can be attended to and negative infinity elsewhere.\n",
        "    \"\"\"\n",
        "\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(source, target, PAD_IDX):\n",
        "    \"\"\"\n",
        "    Create masks for the source and target sequences.\n",
        "\n",
        "    Parameters:\n",
        "    - source (torch.Tensor): Source sequence tensor.\n",
        "    - target (torch.Tensor): Target sequence tensor.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing source mask, target mask, source padding mask, and target padding mask.\n",
        "    \"\"\"\n",
        "\n",
        "    source_seq_len = source.shape[0]\n",
        "    target_seq_len = target.shape[0]\n",
        "    batch_size = source.shape[1]\n",
        "    source_mask = torch.zeros((source_seq_len, source_seq_len), device=device).type(torch.bool)\n",
        "    target_mask = generate_square_subsequent_mask(target_seq_len)\n",
        "    source_padding_mask = (source == PAD_IDX).transpose(0, 1)\n",
        "    target_padding_mask = (target == PAD_IDX).transpose(0, 1)\n",
        "    return source_mask, target_mask, source_padding_mask, target_padding_mask\n"
      ],
      "metadata": {
        "id": "gq-CZnubtH2r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# \\# with open(\"/content/eng_kannada.pkl\", \"rb\") as file:\n",
        "# #   eng = pickle.load(file)\n",
        "# # print(len(eng))"
      ],
      "metadata": {
        "id": "u5ylZXB674OG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4EWNg93_9L1t"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOS_IDX = eng.vocab.stoi[\"<sos>\"]\n",
        "# EOS_IDX = eng.vocab.stoi[\"<eos>\"]\n",
        "\n",
        "def get_tokens(model, source, source_mask, max_len, start_symbol, SOS_IDX):\n",
        "    \"\"\"\n",
        "    Get token indices from a given source sequence using a trained model.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The trained transformer model.\n",
        "    - source (torch.Tensor): The source sequence tensor.\n",
        "    - source_mask (torch.Tensor): The mask for the source sequence.\n",
        "    - max_len (int): Maximum length of the target sequence.\n",
        "    - start_symbol (int): The starting symbol for the target sequence.\n",
        "\n",
        "    Returns:\n",
        "    - result (torch.Tensor): The tensor containing token indices of the target sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Move source and its mask to device\n",
        "    source_mask = source_mask\n",
        "    source = source\n",
        "\n",
        "    # Encode the source sequence\n",
        "    memory = model.encode(source, source_mask)\n",
        "\n",
        "    # Initialize result tensor with the start symbol\n",
        "    result = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "\n",
        "    # Decode the memory tensor to get the target sequence\n",
        "    for index in range(max_len - 1):\n",
        "        memory = memory.to(device)\n",
        "        memory_mask = torch.zeros(result.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "        target_mask = (generate_square_subsequent_mask(result.size(0)).type(torch.bool)).to(device)\n",
        "        output = model.decode(result, memory, target_mask)\n",
        "        output = output.transpose(0, 1)\n",
        "\n",
        "        # Get the next word's probability distribution and find the word with the maximum probability\n",
        "        probs = model.fc_layer(output[:, -1])\n",
        "        _, next_word = torch.max(probs, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        # Add the next word to the result\n",
        "        result = torch.cat([result,torch.ones(1, 1).type_as(source.data).fill_(next_word)], dim=0)\n",
        "\n",
        "        # Break if the next word is the start of sequence symbol\n",
        "        if next_word == SOS_IDX:\n",
        "            break\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def translate(model, source, source_vocab, target_vocab, SOS_IDX, EOS_IDX):\n",
        "    \"\"\"\n",
        "    Translate a given source sequence to a target sequence using a trained model.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The trained transformer model.\n",
        "    - source (str or torch.Tensor): The source sequence (string or tensor).\n",
        "    - source_vocab (Vocab): Vocabulary object for the source language.\n",
        "    - target_vocab (Vocab): Vocabulary object for the target language.\n",
        "\n",
        "    Returns:\n",
        "    - str: The translated sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Convert source string to tensor, if it's a string\n",
        "    if type(source) == str:\n",
        "        tokens = [SOS_IDX] + [source_vocab.stoi[tok] for tok in source.split()] + [EOS_IDX]\n",
        "    else:\n",
        "        tokens = source\n",
        "    num_tokens = len(tokens)\n",
        "    source = (torch.LongTensor(tokens).reshape(num_tokens, 1))\n",
        "    source_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "\n",
        "    # Get target token indices from the source tensor\n",
        "    target_tokens = get_tokens(model, source, source_mask, max_len = num_tokens + 5, start_symbol = SOS_IDX, SOS_IDX = SOS_IDX).flatten()\n",
        "\n",
        "    # Convert target token indices to string\n",
        "    return \" \".join([target_vocab.itos[token] for token in target_tokens]).replace(\"<sos>\", \"\").replace(\"<eos>\", \"\").replace(\"<unk>\", \"\")\n"
      ],
      "metadata": {
        "id": "w6LC0hi_jQEd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SRC_VOCAB_SIZE = len(eng.vocab)  # Source vocabulary size (English)\n",
        "# TAR_VOCAB_SIZE = len(lang.vocab)  # Target vocabulary size (Other language, inferred from 'lang' variable)\n",
        "EMBEDDING_SIZE = 512  # Size of the embedding vector\n",
        "NHEAD = 8  # Number of heads in the multihead attention mechanism\n",
        "FFNN_DIM = 512  # Dimension of the feed-forward neural network inside transformer layers\n",
        "BATCH_SIZE = 32  # Size of each training batch\n",
        "NUM_ENCODER_LAYERS = 3  # Number of layers in the transformer encoder\n",
        "NUM_DECODER_LAYERS = 3  # Number of layers in the transformer decoder\n",
        "LEARNING_RATE = 0.0001  # Learning rate for the optimizer\n",
        "DROPOUT_RATE = 0.1  # Dropout rate for the dropout layer\n",
        "NUM_EPOCHS = 2  # Number of epochs for training\n",
        "# PAD_IDX = eng.vocab.stoi[\"<pad>\"]  # Index for the padding token\n",
        "# SOS_IDX = eng.vocab.stoi[\"<sos>\"] # index for start token\n",
        "# EOD_IDX = eng.vocab.stoi[\"<eos>\"] # index for end token\n",
        "\n",
        "for idx, e in enumerate(names):\n",
        "  with open(\"/content/{}.pkl\".format(e), \"rb\") as vocab_file:\n",
        "    lang = pickle.load(vocab_file)\n",
        "    print(len(lang))\n",
        "  with open(\"/content/eng_{}.pkl\".format(e), \"rb\") as eng_vocab_file:\n",
        "    eng = pickle.load(eng_vocab_file)\n",
        "    print(len(eng))\n",
        "  SOS_IDX = 2\n",
        "  EOS_IDX = 3\n",
        "  print(e)\n",
        "\n",
        "  # Instantiate the TransformerMT model with the specified configuration\n",
        "  # if e.lower() == \"hindi\":\n",
        "    # break\n",
        "    # model = TransformerMT(NHEAD,\n",
        "    #                     EMBEDDING_SIZE,\n",
        "    #                     len(lang),\n",
        "    #                     len(lang),\n",
        "    #                     NUM_ENCODER_LAYERS,\n",
        "    #                     NUM_DECODER_LAYERS,\n",
        "    #                     FFNN_DIM)\n",
        "  # else:\n",
        "  model = TransformerMT(NHEAD,\n",
        "                        EMBEDDING_SIZE,\n",
        "                        len(eng),\n",
        "                        len(lang),\n",
        "                        NUM_ENCODER_LAYERS,\n",
        "                        NUM_DECODER_LAYERS,\n",
        "                        FFNN_DIM)\n",
        "\n",
        "\n",
        "\n",
        "  # Define the loss criterion\n",
        "  # CrossEntropyLoss is used since this is a classification task, and we ignore the loss computed on padding tokens\n",
        "  criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "  model.load_state_dict(torch.load('/content/{}.pth'.format(e.lower())))\n",
        "  model.to(device)\n",
        "  # Define the optimizer to be used for training\n",
        "  # Adam optimizer is used with specific betas and epsilon values\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
        "# Move the model to the specified device (either GPU or CPU)\n",
        "\n",
        "\n",
        "  # Inference\n",
        "# Check if the \"Translations\" directory exists. If not, create it.\n",
        "  if not os.path.exists(\"transformer_translations\"):\n",
        "      os.makedirs(\"transformer_translations\")\n",
        "\n",
        "  # List to store the predicted translations.\n",
        "  predictions = []\n",
        "\n",
        "  # Read the test data from the specified CSV file.\n",
        "  data = pd.read_csv(\"/content/testData/testEnglish-{}.csv\".format(e.title()))\n",
        "  data = data.iloc[:10, :]\n",
        "\n",
        "  # Loop through each row (sentence) in the test data.\n",
        "  for idx, row in data.iterrows():\n",
        "      # Extract the English sentence.\n",
        "      en = row[\"english\"]\n",
        "\n",
        "      # Translate the English sentence to the target language.\n",
        "      pred = translate(model, en, eng, lang, SOS_IDX, EOS_IDX)\n",
        "\n",
        "      # Print the translated sentence (optional, can be commented out).\n",
        "      print(pred)\n",
        "\n",
        "      # Append the translated sentence to the predictions list.\n",
        "      predictions.append(pred)\n",
        "\n",
        "  # Add the predicted translations as a new column to the original dataframe.\n",
        "  data[\"translated\"] = predictions\n",
        "\n",
        "  # Drop the unwanted column \"Unnamed: 0\" (assuming it exists in the CSV).\n",
        "  data.drop([\"Unnamed: 0\"], inplace=True, axis=1)\n",
        "\n",
        "  # Save the dataframe with translations to a new CSV file.\n",
        "  data.to_csv(os.path.join(\"transformer_translations\", \"answer_{}_test.csv\".format(e)))\n",
        "\n",
        "# Evaluate the model on the Bengali test set.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3KcexPZVX6D",
        "outputId": "0089df1c-59e2-46ce-e943-a5d5d5a088d5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50004\n",
            "50004\n",
            "hindi\n",
            " और के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के\n",
            " के के के के के के के के के के के के       \n",
            " और के के के के और और के के के और और और और और हैं  हैं  हैं  है। । ।\n",
            " और और और और और और और और और और और और और और और और और और और और और और और और और हैं   \n",
            " के के के के के के के के      \n",
            " और के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के\n",
            " और के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के के\n",
            " और के के के के के के के के के के के के के के के के के के के के के के के  \n",
            " और के के के के के और के के के और और और और और और और और और और के के के और और और और और के के के के के के के के के के और और और और और और हैं\n",
            " और के के के के के के के के के के और और के के के और और और के के के के और और और हैं    \n",
            "50004\n",
            "42894\n",
            "tamil\n",
            "    மற்றும் மற்றும் மற்றும் மற்றும் . மற்றும் . மற்றும் . மற்றும் . . . . . . . . . . . . .\n",
            "   மற்றும் மற்றும் மற்றும் மற்றும் . மற்றும் . மற்றும் . மற்றும் . . . . . . . . . . . . .\n",
            "     . . . . . . . . . . .\n",
            " எனது எனது . . .                 \n",
            "       . . . . . . . . . . . . . . .\n",
            "                          . . . . . . . . . . . . . . . . . . . . . .\n",
            "         . . . . . . . . . . . . . . . . .\n",
            "  என்ன              \n",
            "    மற்றும்  மற்றும்      . . . . . . . . .      \n",
            " நான் என்ன             \n",
            "50004\n",
            "48411\n",
            "bengali\n",
            "        \n",
            "       এবং এবং এবং এবং এবং এবং এবং এবং এবং এবং করে করা করা করা  ৷ ৷ ৷ ৷ ৷     \n",
            "        এবং এবং এবং এবং এবং এবং এবং ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷\n",
            "       এবং  এবং এবং এবং এবং এবং এবং এবং করে করে করে করা ৷ করা ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ \n",
            " কি কি কি কি        \n",
            " কি কি কি        \n",
            "         এবং এবং এবং করে করে করা    ৷  ৷ ৷    \n",
            "                    \n",
            "         এবং  এবং  ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷ ৷\n",
            "         ৷ ৷ ৷ ৷ ৷\n",
            "50004\n",
            "34232\n",
            "kannada\n",
            "  ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು . . . . . . . . . . . . . . .\n",
            "          ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು . . . . . . . . . .\n",
            "               ಮತ್ತು  ಮತ್ತು ಮತ್ತು ಮತ್ತು  ಮತ್ತು            . . . . . . . . . . . . . . . . .\n",
            "     . . . . . . . .\n",
            " ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು . . . . . . . . . . . . .\n",
            "                        . . . . . . . . . . . . .\n",
            "      .  . .  . .\n",
            "  ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು ಮತ್ತು . . . . . . . . . . . . .\n",
            "         . . . . . . . . . . .\n",
            "               . . . . . . .\n",
            "50004\n",
            "38672\n",
            "gujarati\n",
            "    અને અને અને છે છે છે છે છે છે છે છે છે છે છે છે છે છે છે છે છે\n",
            "    છે છે છે છે છે છે છે છે છે છે છે છે છે\n",
            "    અને અને અને અને છે છે છે છે છે છે છે છે છે છે છે છે છે છે\n",
            "       છે છે છે છે છે છે છે છે છે છે છે છે છે છે\n",
            "     છે છે છે છે છે છે છે છે છે છે છે છે છે\n",
            "   અને અને અને અને છે છે છે છે છે છે છે છે છે છે છે\n",
            "   અને અને અને અને અને છે છે છે છે છે છે છે છે છે છે છે છે છે છે છે છે\n",
            "     છે છે છે છે છે છે છે છે છે છે છે છે છે છે છે\n",
            "   અને અને અને છે છે છે છે છે છે છે છે છે છે છે છે\n",
            "    અને અને અને અને છે અને છે છે છે છે છે છે છે છે છે છે છે છે છે છે છે\n",
            "50004\n",
            "33294\n",
            "telgu\n",
            " నా నాకు        \n",
            " నా నా నా లో లో లో లో లో నాకు          \n",
            "    లో లో లో . . . . . . . . . .\n",
            "     . . . . . . . . . . . .\n",
            "     లో లో లో లో లో . . . . . . . .\n",
            "   మరియు మరియు మరియు మరియు మరియు మరియు మరియు మరియు మరియు మరియు మరియు . మరియు . . . . . . . . . . . . . . . .\n",
            "  మరియు మరియు మరియు మరియు మరియు . మరియు . . . . . . . . . . .\n",
            "  మరియు మరియు మరియు మరియు . . . . . . . . . .\n",
            "    మరియు మరియు మరియు మరియు మరియు మరియు మరియు మరియు మరియు మరియు . మరియు . . . . . . . . . . . . . . . . . . .\n",
            "       మరియు .  .  . . . . . . . . . . . . . . . . . .\n",
            "50004\n",
            "39599\n",
            "malayalam\n",
            "             .  .  .  .  .  \n",
            "    റെ റെ  റെ . റെ . . . . . . . . . . . . . .\n",
            "       . . . . . . . . . .\n",
            "     റെ         . . . . . . . . . . .\n",
            "    റെ   . . . . . . . . . .\n",
            " എന്താണ് എന്താണ് എന്താണ്        \n",
            "            . . . . . . . . . . . . . . . .\n",
            "     റെ  റെ  . റെ . . . . . . . . . . . . . .\n",
            "       ?   .  .  .  .\n",
            " ഒരു       ഒരു         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"/content/transformer_translations\"\n",
        "RESULTS = \"answer.csv\"\n",
        "VALIDATION_FILE = \"/content/test_data.json\"\n",
        "\n",
        "if os.path.exists(os.path.join(DATA_DIR, RESULTS)):\n",
        "    os.remove(os.path.join(DATA_DIR,RESULTS))\n",
        "val_ids = []\n",
        "\n",
        "with open(VALIDATION_FILE, \"r\") as file:\n",
        "    val_data = json.load(file)\n",
        "    for lang_type, lang_data in val_data.items():\n",
        "        print(lang_type)\n",
        "        for data_type, data_entries in lang_data.items():\n",
        "            ids, en_sns = [], []\n",
        "            for ent_id, ent_data in data_entries.items():\n",
        "                val_ids.append(int(ent_id))\n",
        "\n",
        "print(len(val_ids))\n",
        "# print(val_ids)\n",
        "\n",
        "# Reading all the dataframes\n",
        "files = os.listdir(DATA_DIR)\n",
        "files = [file for file in files if file.endswith(\".csv\")]\n",
        "print(files)\n",
        "data = [pd.read_csv(os.path.join(DATA_DIR, file)) for file in files]\n",
        "res = pd.concat(data, axis=0, ignore_index=False)\n",
        "res.drop([\"Unnamed: 0\", \"english\"], axis=1, inplace=True)\n",
        "res = res.rename(columns={\"id\": \"ID\", \"translated\": \"Translated\"})\n",
        "res_sorted = res[res[\"ID\"].isin(val_ids)].sort_values(by=\"ID\")\n",
        "res_sorted.to_csv(os.path.join(DATA_DIR, RESULTS), sep=\"\\t\", index=False, quoting=csv.QUOTE_NONNUMERIC, quotechar='\"', escapechar='\\\\')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szJjx5PZVX-l",
        "outputId": "65c7d992-0f88-4eac-c119-8dd6dd230692"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English-Bengali\n",
            "English-Gujarati\n",
            "English-Hindi\n",
            "English-Kannada\n",
            "English-Malayalam\n",
            "English-Tamil\n",
            "English-Telgu\n",
            "114647\n",
            "['answer_malayalam_test.csv', 'answer_bengali_test.csv', 'answer_gujarati_test.csv', 'answer_kannada_test.csv', 'answer_tamil_test.csv', 'answer_telgu_test.csv', 'answer_hindi_test.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zo_nnly3VYAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Gd5rU4YVYC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZWMPoWvJVYGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8OxP9hmtH5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}